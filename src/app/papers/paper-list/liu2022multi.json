{
  "title": "Multi-Job Intelligent Scheduling With Cross-Device Federated Learning",
  "authors": [
    "Ji Liu",
    "Juncheng Jia",
    "Beichen Ma",
    "Chendi Zhou",
    "Jingbo Zhou",
    "Yang Zhou",
    "Huaiyu Dai",
    "Dejing Dou"
  ],
  "year": "2022-11",
  "labels": [
    "federated learning",
    "multi-job",
    "fairness",
    "client selection",
    "reinforcement learning"
  ],
  "summaries": [
    "propose __a multi-job FL framework__ which enables the training process of multiple jobs in parallel, achieving (1) faster training time, and (2) higher accuracy, comparing with (1) single-job sequential FL with various client selection methods, and (2) multiple-job FL various client selection methods",
    "the framework assumes 1) each device can only be scheduled to one job at a given time, 2) each job is of equal importance",
    "in the device scheduling stage, devices are scheduled by a contextual-based optimization problem which minimized execution time and unfairness",
    "the device scheduling optimization problem is solved by the proposed 1) Bayesian Optimization-Based method, 2) reinforcement learning based method"
  ],
  "systemModel": [
    "a set of $M$ job $\\mathcal{M}$, a set of $K$ devices $\\mathcal{K}$",
    "step 1): server sends requests to available devices",
    "step 2): server schedules devices $\\mathcal{V}_m^r$ for the current job $m$ based on the scheduling plan generated by the scheduling method [see 1. device scheduling]",
    "step 3): erver distributes the latest global model for the current job to the scheduled devices",
    "step 4): the model is updated in each device based on the local data",
    "step 5): each device sends the updated model to the server after its local training",
    "step 6): server aggregates the models of scheduled devices to generate a new global model with FedAvg",
    "The combination of steps 1)-6) is denoted by a round, which is repeated for each job until the corresponding global model reaches the expected performance (accuracy, loss value, or convergence)."
  ],
  "motivation": [
    "While current FL solutions  focus on a singletask job or a multi-task job, FL with multiple jobs remains an open problem.",
    "The major difference between the multi-task job and multiple jobs is that the tasks of the multitask job share some common parts of the model, while the multiple jobs do not interact with each other in terms of the model.",
    "The multi-job FL deals with the simultaneous training process of multiple independent jobs.",
    "Each job corresponds to multiple updates during the training process of a global model with the corresponding decentralized data.",
    "While the FL with a single job generally selects a portion of devices to update the model, the other devices remain idle, and the efficiency thus is low.",
    "The multi-job FL can well exploit diverse devices for multiple jobs simultaneously, which brings high efficiency.",
    "The available devices are generally heterogeneous",
    "During the training process of multiple jobs, the devices need to be scheduled for each job.",
    "At a given time, a device can be scheduled to one job.",
    "However, only a portion of the available devices are scheduled to one job to reduce the influence of stragglers",
    "Powerful devices should be scheduled to jobs to accelerate the training process, while other eligible devices should also participate in the training process to increase the fairness of data to improve the accuracy of the final global models.",
    "The fairness of data refers to the fair participation of the data in the training process of FL, which can be indicated by the standard deviation of the times to be scheduled to a job",
    "While the scheduling problem of devices is typical NPhard, some solutions have already been proposed for the training process of FL or distributed systems, which generally only focus on a single job with FL.",
    "In addition, these methods either cannot address the heterogeneity of devices, or do not consider the data fairness during the training process, which may lead to low accuracy."
  ],
  "techniques": [
    "1. device scheduling",
    [
      "$\\begin{array}{ll} \\underset{\\mathcal{V}_m^r}{\\operatorname{min}} & \\sum\\limits_{m=1}^M \\alpha\\cdot\\mathcal{T}_m^r(\\mathcal{V_m^r}) + \\beta\\cdot\\mathcal{F}_m^r(\\mathcal{V_m^r}) \\\\  \\text { s.t. } & \\mathcal{T}_m^r(\\mathcal{V_m^r}) = \\max\\limits_{k\\in\\mathcal{V}_m^r}\\{t_m^k\\}, \\\\  & \\mathcal{F}_m^r(\\mathcal{V_m^r})=\\frac{1}{|\\mathcal{V}_m^r|}\\sum\\limits_{k \\in \\mathcal{V}_m^r}\\left(s_{k,m}^r-\\frac{1}{|\\mathcal{K}|}\\sum\\limits_{k\\in\\mathcal{K}}s_{k,m}^r\\right)^2, \\\\  & \\mathcal{V}_m^r\\subset\\mathcal{K}. \\end{array}$",
      [
        "$t_m^k$: execution time (communication time + computation time), assume it follows the shift exponential distribution",
        "$s_{k,m}^{r+1}=\\begin{cases}s_{k,m}^{r}+1 & k\\in\\mathcal{V}_m^r \\\\ s_{k,m}^r & k\\notin\\mathcal{V}_m^r \\end{cases}$"
      ],
      "a Bayesian optimization-based device scheduling (BODS) algorithm is proposed to solve simple jobs and exploit Gaussian Process to find a near-optimal solution",
      "In order to learn more information about the near-optimal scheduling patterns for complex jobs, we further propose a Reinforcement Learning-based Device Scheduling (RLDS) method",
      "Above all the scheduling methods, we proposed a meta-greedy scheduling approach which exploit diverse scheduling methods, e.g., BODS, RLDS, Random, FedCS, Genetic, and Greedy, to generate scheduling plan candidates which outperforms all other methods"
    ]
  ],
  "doi": "arXiv:2211.13430",
  "id": "liu2022multi",
  "bibtex": "@article{liu2022multi, title={Multi-Job Intelligent Scheduling With Cross-Device Federated Learning}, author={Liu, Ji and Jia, Juncheng and Ma, Beichen and Zhou, Chendi and Zhou, Jingbo and Zhou, Yang and Dai, Huaiyu and Dou, Dejing}, journal={IEEE Transactions on Parallel and Distributed Systems}, volume={34}, number={2}, pages={535--551}, year={2022}, publisher={IEEE}}"
}