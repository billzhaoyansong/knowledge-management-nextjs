{
  "title": "Hierarchical federated learning with local model embedding",
  "authors": [
    "Yunlong He",
    "Dandan Yan",
    "Fei Chen"
  ],
  "type": "technical",
  "year": "2022-10",
  "labels": [
    "federated learning",
    "grouping",
    "hierarchical",
    "personalization"
  ],
  "summaries": [
    "in the hierarchical FL framework, propose __a 3-layer FL architecture to group members with similar data sample distributions__, because single client might not have sufficient samples while the global model is not tailored for personalization",
    "in this architecture (user layer, edge layer, cloud layer), the devices are grouped based on feature representation vectors using K-Means",
    "a GNN-based predictor trained by the cloud server is used to predict the group of a new client"
  ],
  "systemModel": [
    "Players",
    [
      "$1$ cloud server, many edge nodes, many clients, a public dataset"
    ],
    "steps",
    [
      "group generation step (1): clients perform local training and generate data samples by employing Dirichlet sampling from a public dataset [see 1. Dirichlet Sampling]",
      "group generation step (2): edge nodes perform model aggregation and combine data samples from clients to obtain the shared data set [see 2. Shared Dataset]",
      "group generation step (3): edge nodes upload aggregated models and feature representation vectors, which is generated by model embedding [see 3. Model Embedding], to the cloud server",
      "group generation step (4): cloud server adjusts clients based on the feature representation vectors [see 4. Client Grouping], trains GNN-based group predictor with those vectors, and aggregates models to obtain the global model",
      "group prediction step (1): the new client gets the global model from the cloud server",
      "group prediction step (2): the new client trains with local data and uploads to the cloud server directly",
      "group prediction step (3): the cloud server embeds the model to get the feature representation vector",
      "group prediction step (4): the cloud server takes the feature representation vector as the input of GNN-based predictor [see 5. GNN-Based Predictor] and finally arranges a training group"
    ]
  ],
  "motivation": [
    "Although federated learning can leverage local computation capacities and preserve privacy for end users, a unified model training over heterogeneous devices still faces inherent problems, such as long training period, model redundancy, and lower communication efficiency",
    "On one hand, only part of clients can participate in federated learning each round as the distributed devices with heterogeneous capacity can hardly be synchronized",
    "On the other hand, as the data samples from each client are usually non-IID, it is difficult to learn an efficient global model through model aggregation, especially when the clients differ in feature space",
    "Meanwhile, model redundancy of geodistributed clients could inevitably leads to communication overhead and low learning efficiency",
    "In order to address the limitations of conventional federated learning, personalized federated learning (PFL) is proposed to offer personalized solutions for clients with different data sample distributions",
    "Nevertheless, these schemes still have their limitations.",
    "In traditional federated learning, the uniform global model can hardly be tailored for a specific user as its local data distribution is unknown.",
    "Oppositely, in PFL the user can know the local data distribution itself, but the local data samples are insufficient to learn a powerful local model.",
    "Therefore, a group-based approach becomes a more reasonable trade off solution in which federated learning is performed for group members with similar data sample distributions."
  ],
  "questions": [
    "On the one hand, the data samples are collected from users with diverse preferences, and the data distribution can be non-independent and identically distributed (non-IID). (data heterogeneity)",
    "On the other hand, the consequent model trained locally needs to communicate with a remote parameter server periodically for parameter synchronization, which leads to overwhelming communication and synchronization overhead given heterogeneous device capacities and network conditions of end-users. (system heterogeneity)"
  ],
  "techniques": [
    "1. Dirichlet Sampling: sample a public dataset",
    [
      "$\\hat{a} \\gets softmax(|a-\\bar{a}|)$",
      [
        "$a$: accuracy vector of the local model on the public dataset",
        "$\\bar{a}$: mean value of the accuracy vector"
      ],
      "$S_i=\\{ x | x ~ Dir_i(\\hat{a})\\}$",
      [
        "$S_i$: Dirichlet sampling from the client $i$"
      ]
    ],
    "2. Shared Dataset",
    [
      "the shared dataset is obtained by merging all the clients' Dirichlet sampling: $\\mathcal{D}_{share} = \\{S_1,...,S_N\\}$"
    ],
    "3. Model Embedding",
    [
      "By using the shared dataset, a feature latent representatoin vector is generated for each client: $latent_i=[f(d_1;w_i), ... ,f(d_m;w_i)]$",
      [
        "$d_k$: the $k$th sample in the shared dataset $\\mathcal{D}_{share}$",
        "$w_i$: model parameters of client $i$",
        "$f(\\cdot)$: loss of the local model"
      ]
    ],
    "4. Client Grouping",
    [
      "K-Means is used to group clients and the distance is calculated by $DM_{i,j}=\\Vert latent_i-latent_j \\Vert$"
    ],
    "5. GNN-Based Predictor",
    [
      "$P=softmax(D^{-\\frac{1}{2}} S D^{\\frac{1}{2}} X W)$",
      [
        "$P \\in \\mathbb{R}^{N \\times G}$: the probability matrix of client $i$ in group $g$",
        "$D$: degree matrix",
        "$S=\\max(ReLU(\\tanh(L^T L)), \\tilde{A})$: similarity of user data distributions, $S_{i,j}$ means the similarity between client $i$ and client $j$",
        [
          "$L=concat(\\{latent_i | \\forall i \\in N \\})$",
          "$\\tilde{A} = A + I$: adjacency matrix where $A_{i,j}=1$ indicates $i$ and $j$ are neighbor nodes"
        ],
        "$X$: feature latent representation vector of the new participant",
        "$W$: ???"
      ]
    ]
  ],
  "doi": "10.1016/j.engappai.2023.106148",
  "id": "he2023hierarchical",
  "bibtex": "@article{he2023hierarchical, title={Hierarchical federated learning with local model embedding}, author={He, Yunlong and Yan, Dandan and Chen, Fei}, journal={Engineering Applications of Artificial Intelligence}, volume={123}, pages={106148}, year={2023}, publisher={Elsevier}}"
}