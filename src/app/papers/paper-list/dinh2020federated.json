{
  "title": "Federated Learning over Wireless Networks: Convergence Analysis and Resource Allocation (FEDL)",
  "type": "technical",
  "authors": [
    "Canh T. Dinh",
    "Nguyen H. Tran",
    "Minh N. H. Nguyen",
    "Choong Seon Hong",
    "Wei Bao",
    "Albert Y. Zomaya",
    "Vincent Gramoli"
  ],
  "year": "2020-10",
  "labels": [
    "federated learning",
    "data heterogeneity",
    "wireless",
    "resource allocation",
    "convergence rate"
  ],
  "summaries": [
    "in the conventional FL framework, propose __a resource allocation algorithm to tackle the convergence difficulties brought by the large number of equipments and the non-iid data__ (minimize energy consumption and wall clock time in one global round), achieving [1] faster convergence rate [2] higher test accuracy comparing the valina FedAvg",
    "the resource allocation algorithm aims to minimize energy consumption and wall clock time in one global round",
    "this resource allocation algorithm optimizes (1) CPU frequencies (2) server communication times allocated to UEs (3) local accuracy (4) one global round communication time (5) one global round computation time"
  ],
  "systemModel": [
    "one edge server and a set $\\mathcal{N}$ of $N$ user equipments (UEs), each $n$ UE has a local dataset $\\mathcal{D}_n$ of size $D_n$",
    "propose a FL algorithm named FEDL, which considers synchronous FL and requires $\\nabla F_n(w_n^t)$ in addition to $w_n^t$ with convergence guarantee, comparing with FedAvg",
    "step 1: solve a resource allocation problem considering CPU frequencies, server communication times, local training accuracies, one global round communication and computation time to minimize energy consumption and wall clock time of one global round [see 1. FEDL Optimization]",
    "step 2: selected UEs download global gradient $\\nabla \\bar{F}^{t-1}$ and model $w^{t-1}$ from the server, train with local model provided with given CPU frequency [see 2. Local Training]",
    "step 3: selected UEs transmit $w^{t}_n$ and $\\nabla F_n (w^{t}_n)$ to the edge server",
    "step 4: the server aggregates model and gradients [see 3. Global Aggregation]"
  ],
  "motivation": [
    "Despite its promising benefits, FL comes with new challenges to tackle",
    "On one hand, the number of UEs in FL can be large and the data generated by UEs have diverse distributions.",
    "Designing efficient algorithms to handle statistical heterogeneity with convergence guarantee is thus a priority question.",
    "While all of the above FL algorithms’ complexities are measured in terms of the number of local and global update rounds (or iterations), the wall clock time of FL when deployed in a wireless environment mainly depends on the number of UEs and their diverse characteristics, since UEs may have different hardware, energy budget, and wireless connection status.",
    "Specifically, the total wall-clock training time of FL includes not only the UE computation time (which depend on UEs’ CPU types and local data sizes) but also the communication time of all UEs (which depends on UEs’ channel gains, transmission power, and local data sizes).",
    "to minimize the wall-clock training time of FL, a careful resource allocation problem for FL over wireless networks needs to consider not only the FL parameters such as accuracy level for computation-communication trade-off, but also allocating the UEs’ resources such as power and CPU cycles with respect to wireless conditions."
  ],
  "techniques": [
    "1. FEDL Optimization",
    [
      "$\\begin{array}{ll} \\underset{f, \\tau, \\theta, \\eta, T_{c o}, T_{c p}}{\\operatorname{minimize}} & K_g\\left(E_g+\\kappa (T_{co}+ K_l T_{cp})\\right) \\\\  \\text { subject to } & \\sum_{n=1}^N \\tau_n \\leq T_{c o}, \\\\ & \\max _n \\frac{c_n D_n}{f_n}=T_{c p}, \\\\ & f_n^{\\min } \\leq f_n \\leq f_n^{\\max }, \\forall n \\in \\mathcal{N}, \\\\ & p_n^{\\min } \\leq p_n\\left(s_n / \\tau_n\\right) \\leq p_n^{\\max }, \\forall n \\in \\mathcal{N}, \\\\ & 0 \\leq \\theta \\leq 1 . \\end{array}$",
      [
        "$f=\\{f_1,...,f_N\\}, \\tau=\\{\\tau_1,...,\\tau_N\\}$: specified CPU cycle frequencies and fraction of communication time allocated by the edge server",
        "$\\theta, \\eta$: specified local accuracy and a controllable parameter",
        "$T_{co}, T_{cp}$: communication time in one global round and computation time in one local iteration",
        "$E_g=\\sum_{n=1}^N E_{n,co} + K_l E_{n,cp}$",
        [
          "$E_{n,co}=\\tau_n \\frac{N_0}{\\bar{h}_n}(e^{\\frac{s_n/r_n}{B}}-1)$",
          [
            "$N_0, \\bar{h}_n$: background noise, the average channel gain",
            "$s_n, B$: data size of local model and gradient, bandwidth"
          ],
          "$K_l=\\frac{2}{\\gamma}\\log\\frac{C}{\\theta}$: local iterations",
          "$E_{n,cp}=\\frac{\\alpha}{2} c_n D_n f_n^2$: "
        ],
        "$\\frac{c_n D_n}{f_n}$: computation time per local iteration"
      ]
    ],
    "2. Local Training",
    [
      "$\\min\\limits_{w\\in \\mathbb{R}^d} J_n^t(w) = F_n(w)+\\lang \\eta \\nabla \\bar{F}^{t-1} - \\nabla F_n(w^{t-1}), w \\rang$",
      [
        "$\\lang \\cdot, \\cdot \\rang$: inner product"
      ]
    ],
    "3. Global Aggregation",
    [
      "$w^t=\\sum_{n=1}^N p_n w_n^t$",
      "$\\nabla \\bar{F}^{t} = \\sum_{n=1}^N p_n \\nabla F_n(w_n^t)$",
      [
        "$p_n=\\frac{D_n}{D}$"
      ]
    ]
  ],
  "doi": "10.1109/TNET.2020.3035770",
  "id": "dinh2020federated",
  "bibtex": "@article{dinh2020federated, title={Federated learning over wireless networks: Convergence analysis and resource allocation}, author={Dinh, Canh T and Tran, Nguyen H and Nguyen, Minh NH and Hong, Choong Seon and Bao, Wei and Zomaya,Albert Y and Gramoli, Vincent}, journal={IEEE/ACM Transactions on Networking},  volume={29}, number={1}, pages={398--409}, year={2020}, publisher={IEEE}}"
}