# Temporal Difference (TD) Learning

## Overview

- ðŸ“– Idea
  - combines ideas from _**Monte Carlo**_ methods and _**dynamic programming**_ and is used to estimate _**value functions**_
- ðŸ’¡ Key Concepts
  - _**Bootstrapping**_: updates estimates using other intermediate estimates.
  - _**TD-error**_: $\delta_t = r_{t+1} + Î³V (s_{t+1}) âˆ’ V(s_t)$

## Algorithms

- ðŸ§  **TD(0) Learning**
  - Intuition: Adjust my current estimate towards the immediate reward plus my estimate of the next state.
  - Update Rule: 
    - $V (s_t) \leftarrow V (s_t) + Î±[r_t + Î³V (s_{t+1}) âˆ’ V (s_t)]$
    - <details><summary>ðŸ‘£ Steps</summary>
  
        ```
        - Initialize V(s) arbitrarily for all states.
        - Repeat for each episode:
            - Initialize state S
            - Repeat until terminal state:
                - Take action A using policy Ï€, observe reward $R$ and next state $S'$
                - Update V according to rule            
        ```
    </details>
- ðŸ§  **SARSA (On-Policy TD Control)**
  - Update Rule: 
    - $Q (s_t, a_t) \leftarrow Q (s_t, a_t) + Î±[r_t + Î³ Q(s_{t+1}) âˆ’ Q (s_{t+1}, a)]$
    - <details><summary>ðŸ‘£ Steps</summary>

        ```
        - Initialize $Q(s, a)$ arbitrarily.
        - Repeat for each episode:
            - Initialize state $S$
            - Repeat until terminal state:
                - Take action A using policy Ï€, observe reward R and next state S'
                - Update Q according to rule     
        ```
    </details>
- ðŸ§  **Tabular Q-Learning (Off-Policy TD Control)**
  - Update Rule:
    - $Q(S_t, A_t) \leftarrow Q(S_t, A_t)+Î± [R_{t+1} + Î³ \max_a Q(S_{t+1}, a) âˆ’ Q(S_t, A_t)]$