# Approximate Q-learning

- ‚ùî Why: 
  - Tabular learning approaches have trouble learning in _**large environments**_ since
    - (1) there is no generalization between similar situations, and
    - (2) storing $Q$ or $V$ can even be an issue
  - Approximate approaches  allow working with large environments (finite and continuous $S$) by considering
    - $Q(s,a)\approx Q_{\theta}(\phi(s), a)$
      - $\phi(s) \in \mathbb{R}^d$: the vector of features
- üß† **Deep Q-Network (DQN)**
  - uses _**2D convolutions**_ with an MLP to extract features
  - overcomes _**stability issues**_ thanks to:
    - Experience-replay.
    - Target network.