# Monte Carlo

## Overview

- üìñ Monte Carlo (MC) methods are a class of _value-based_ (a type of model-free) algorithms
- üîç Assumptions
  - terminal states
  - Finite Returns
- üí°Idea
  - The value function is repeatedly altered to more closely _approximate the value function for the current policy_,
  - and the _policy is repeatedly improved with respect to the current value function_

## Policy Evaluation

- üß† (On-Policy) **First-Visit MC**
  - _**only the first time**_ a state (or state-action pair) is visited in an episode.
  - Lower variance but slower convergence (fewer updates per episode).
    - <details><summary>Show Code</summary>

        ```python
        def first_visit_mc(env, policy, num_episodes, gamma=0.9):
            # Initialize
            V = defaultdict(float)          # State values
            returns = defaultdict(list)     # List of returns for each state
            
            for _ in range(num_episodes):
                episode = []
                state = env.reset()
                
                # Generate an episode
                while True:
                    action = policy(state)
                    next_state, reward, done, _ = env.step(action)
                    episode.append((state, action, reward))
                    if done:
                        break
                    state = next_state
                
                # Calculate returns and update V(s)
                G = 0
                visited_states = set()
                
                for t in reversed(range(len(episode))):
                    state, _, reward = episode[t]
                    G = gamma * G + reward
                    
                    # First-visit: Only update the first occurrence of a state
                    if state not in visited_states:
                        visited_states.add(state)
                        returns[state].append(G)
                        V[state] = np.mean(returns[state])
            
            return V
            ```
    </details>
- üß† (On-Policy) **Every-Visit MC**
  - _**Every occurrence**_ of a state in an episode is included in the average.
  - Higher variance but faster convergence (more updates per episode).
    - <details><summary>Show Code</summary>

        ```python
        def every_visit_mc(env, policy, num_episodes, gamma=0.9):
            # Initialize
            V = defaultdict(float)          # State values
            returns = defaultdict(list)     # List of returns for each state
            
            for _ in range(num_episodes):
                episode = []
                state = env.reset()
                
                # Generate an episode
                while True:
                    action = policy(state)
                    next_state, reward, done, _ = env.step(action)
                    episode.append((state, action, reward))
                    if done:
                        break
                    state = next_state
                
                # Calculate returns and update V(s)
                G = 0
                
                for t in reversed(range(len(episode))):
                    state, _, reward = episode[t]
                    G = gamma * G + reward
                    
                    # Every-visit: Update every occurrence of the state
                    returns[state].append(G)
                    V[state] = np.mean(returns[state])
            
            return V
        ```
    </details>

## MC Control (Policy Improvement)

- üß† **On-Policy MC Control (Œµ-Greedy)**
  - Uses Œµ-greedy policy for exploration.
  - <details><summary>Show Code</summary>

    ```python
    def on_policy_mc_control(env, num_episodes, epsilon=0.1, gamma=0.99):
    # Initialize
    Q = defaultdict(lambda: np.zeros(env.action_space.n))  # Action values
    returns = defaultdict(list)                           # Stores returns for (s,a) pairs
    policy = defaultdict(lambda: np.ones(env.action_space.n)/env.action_space.n)  # Random policy
    
    for _ in range(num_episodes):
        # Generate episode using current Œµ-greedy policy
        episode = []
        state = env.reset()
        while True:
            action = np.random.choice(env.action_space.n, p=policy[state])
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            if done: break
            state = next_state
        
        # Update Q and policy (every-visit)
        G = 0
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:  # First-visit alternative
                returns[(state, action)].append(G)
                Q[state][action] = np.mean(returns[(state, action)])
                # Update Œµ-greedy policy
                best_action = np.argmax(Q[state])
                policy[state] = epsilon/len(Q[state])  # Exploration
                policy[state][best_action] += 1 - epsilon  # Exploitation
    
    return Q, policy
    ```
  </details>
- üß† **Off-Policy MC Control (Importance Sampling)**
  - <details><summary>Show Code</summary>

    ```python
    def off_policy_mc_control(env, num_episodes, gamma=0.99):
    # Initialize
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    C = defaultdict(float)  # Cumulative importance ratios
    target_policy = defaultdict(int)  # Greedy policy (deterministic)
    behavior_policy = defaultdict(lambda: np.ones(env.action_space.n)/env.action_space.n)  # Random
    
    for _ in range(num_episodes):
        # Generate episode using behavior policy
        episode = []
        state = env.reset()
        while True:
            action = np.random.choice(env.action_space.n, p=behavior_policy[state])
            next_state, reward, done, _ = env.step(action)
            episode.append((state, action, reward))
            if done: break
            state = next_state
        
        # Update Q with importance sampling (every-visit)
        G = 0
        W = 1  # Importance sampling ratio
        for t in reversed(range(len(episode))):
            state, action, reward = episode[t]
            G = gamma * G + reward
            C[(state, action)] += W
            Q[state][action] += (W / C[(state, action)]) * (G - Q[state][action])
            # Update target policy (greedy)
            target_policy[state] = np.argmax(Q[state])
            # Early termination if œÄ(a|s)=0
            if action != target_policy[state]:
                break
            W *= 1. / behavior_policy[state][action]  # Update ratio
    
    return Q, target_policy
    ```
  </details>
