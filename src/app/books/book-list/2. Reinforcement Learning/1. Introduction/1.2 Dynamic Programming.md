# Dynamic Programming

## Key Assumptions

- **Known model**: You have full knowledge of the environment's dynamics:
  - Transition probabilities $P(s' \mid s,a)$
  - Reward function $𝑅(𝑠,𝑎,𝑠')$
- **Finite MDPs**: Typically assumes discrete and finite state and action spaces.
- 📝 DP is a type of _model-based_ algorithms in RL.

## Algorithms

- 🧠 **General Policy Iteration (GPI)**
  - Steps:
    - 1️⃣ Initialization
      - assign arbitrary policy $\pi(a \mid s)$ for all $s$
    - 2️⃣ Policy Evaluation
      - update state-value $V(s)$ for all $s$ under current policy
        - if the policy is _**indeterministic**_
          - $V^\pi(s) = \sum\limits_a \underbrace{\pi(a|s)}_{\text{e.g. 40\% left, 60\% right}} \sum\limits_{s'} \overbrace{P(s'|s,a)}^{\text{chances of }s'}[R + \gamma V^\pi(s')]$
            - $V^\pi(s')$: use state-value in the same step
        - if the policy is _**deterministic**_
          - $V^\pi(s) = Q^\pi(s, a)= \sum_{s'} P(s'|s,a) [R + \gamma V^\pi(s')]$
    - 3️⃣ Policy Improvement
      - improve policy _**greedily**_
        - $\pi'(s) = \argmax_a Q^\pi(s,a)$
    - 4️⃣ Termination
      - if policy stable, terminate; otherwise, go step (2)
- 🧠 **Value Iteration Algorithm** 
  - Steps:
    - 1️⃣ Initialization
      - assign arbitrary state-value $V(s)$ for all $s$
    - 2️⃣ Policy Improvement by Greedily Select the one Maximize New State-Value
      - $V_{i+1}(s) = \max\limits_{a \in A}\sum\limits_{s'\in S} \pi(a \mid s) P(s' \mid s, a) [R(s, a, s') + \gamma V_i(s')]$
        - $V_i(s')$: use state-value from last step
    - 3️⃣ Termination
      - if the new state-values are stable, then terminate
