# Overview

## Markov Decision Process (MDP)

- A **Markov Decision Process** is a tuple $(S, A, R, P, \rho_0)$ where
  - _**state space $S$**_: the set of environment and agent states;
  - _**action space $A$**_: the set of actions of the agent;
  - _**reward function $R : S \times A \times S \rightarrow  \mathbb{R}$**_:
    - $r_t = R(s_t, a_t, s_{t+1})$. 
    - In the case the reward is stochastic and $r_t$ is a random variable, we have $R(s, a, s') = \mathbb{E}[r_t \vert s_t = s; a_t = a; s_{t+1} = s']$
  - _**transition probability function $P : S×A×S \rightarrow [0, 1]$**_ 
    - $P(s' | s,a)$ is the probability of transitioning into state $s_0$ if you are in state $s$ and take action $a$
  - _**starting state distribution $\rho_0 : S \rightarrow [0, 1]$**_
- **Markov property**:
  - Transitions _**only depend on the most recent state and action**_, and no prior history: $P(s_{t+1} | s_t, a_t, ..., s_1, a_0, s_0) = P(s_{t+1} | s_t, a_t)$
  - _**when MDP is known**_, optimal policies can be found offline without interacting with the environment, using _**Dynamic Programming (DP)**_ algorithms.
  - _**otherwise**_, _**RL algorithms**_ come to a rescue to deal with delayed rewards by trial-and-error search
- **Extensions of MDP**:
  - This markov property assumption does not always hold, for instance
  - **_Partially Observable Markov Decision Process_**
    - when the observed state does not contain all necessary information 
  - **_Non-Stationary Markov Decision Process_**
    - when $P$ and $R$ actually depend on $t$.
  - _**Decentralized Control (Multi-Agent RL)**_
    - Multiple agents interact without centralized coordination (e.g., swarm robotics, competitive games).

## Definitions

- **Policy $\pi$**: determines the behavior of our agent, who will take actions $a_t ∼ π(· | s_t)$. Policies can 
  - (1) be derived from an action-value function, or
  - (2) be explicitly parameterized and denoted by $\pi_{\theta}$, or 
  - (3) be deterministic, in which case they are sometimes denoted by $\mu_{\theta}$, with $a_t = \mu_{\theta}(s_t)$
- **Trajectory/Episodes $\tau = (s_0, a_0, s_1, ...)$**: a sequence of states and actions in the world, with $s_0 ∼ \rho_0$ and $s_{t+1} ∼
P(· | s_t, a_t)$. It is sampled from $\pi$ if $a_t ∼ \pi(· | s_t)$ for each $t$.
- **Cumulative reward over a trajectory** $R(\tau)$: the quantity to be maximized by our agent.
  - _**finite-horizon undiscounted return**_ $R(\tau) = \sum_{t=0}^T r_t$
  - _**infinite-horizon discounted return**_ $R(\tau) = \sum_{t=0}^\infty \gamma^t r_t$
    - $\gamma$: discount factor
- _**State-Value function $V^{\pi }(s)=\mathbb{E}_{\tau ~ \pi}[R(\tau) \vert s_0 = s]$**_: expected return starting with state $s$ and successively following policy $\pi$, estimating "how good" the policy is to be in a given state
- _**Action-Value function (Q-Function)**_