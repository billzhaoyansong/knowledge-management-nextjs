# Overview

## Basics

- <img style="width:75%;max-width:500px;" src="/books/Reinforcement Learning/MDP.png" />
- **1. Markov Decision Process (MDP)**: a tuple $(S, A, R, P, \rho_0)$ where
  - (1) _**state space $S$**_: the set of environment and agent states;
  - (2) _**action space $A$**_: the set of actions of the agent;
  - (3) _**reward function $R : S \times A \times S \rightarrow  \mathbb{R}$**_:
    - _**immediate reward: $r_t = R(s_t, a_t, s_{t+1})$**_. 
    - In the case the reward is stochastic and $r_t$ is a random variable, we have $R(s, a, s') = \mathbb{E}[r_t \mid s_t = s, a_t = a, s_{t+1} = s']$
  - (4) _**transition probability function $P : S×A×S \rightarrow [0, 1]$**_ 
    - $P(s' \mid s,a)$ is the probability of transitioning into state $s_0$ if you are in state $s$ and take action $a$
  - (5) _**starting state distribution $\rho_0 : S \rightarrow [0, 1]$**_
  - (6) _**policy $\pi$**_:
    - determines the behavior of our agent, who will take actions $a_t ∼ π(· \mid s_t)$. Policies can
      - (1) be derived from an action-value function, or
      - (2) be explicitly parameterized and denoted by $\pi_{\theta}$, or 
      - (3) be deterministic, in which case they are sometimes denoted by $\mu_{\theta}$, with $a_t = \mu_{\theta}(s_t)$
- **2. Return**
  - _**Trajectory/Episodes $\tau = (s_0, a_0, s_1, ...)$**_: 
    - a sequence of states and actions in the world, with $s_0 ∼ \rho_0$ and $s_{t+1} ∼
P(· \mid s_t, a_t)$.
    - It is sampled from $\pi$ if $a_t ∼ \pi(· \mid s_t)$ for each $t$.
  - _**Return** $R(\tau)$_: sum of the total rewards over an agent’s trajectory
    - (1) _**finite-horizon undiscounted return**_ $R(\tau) = \sum_{t=0}^T r_t$
    - (2) _**infinite-horizon discounted return**_ $R(\tau) = \sum_{t=0}^\infty \gamma^t r_t$
      - $\gamma$: discount factor
- **3. Value and Advantage Functions**
  - _**(1) State-Value (On-Policy Value) function $V^{\pi }(s)=\mathbb{E}_{\tau \sim \pi}\left[R(\tau) \mid s_0 = s \right]$**_
    - expected return starting with state $s$ and successively following policy $\pi$
    - $V^{\pi}(s)=\mathbb{E}_{\tau \sim \pi}\left[R(s,a,s') + \gamma V^\pi(s') \mid s_0 = s \right]$ (_**Bellman Equation for $V^\pi$**_)
  - _**(2) Action-Value (On-Policy Action-Value) function (Q-Function) ${Q^{\pi }(s,a)=\operatorname {\mathbb {E}_{\tau \sim \pi} } [R(\tau)\mid s_0=s,a_0=a]}$**_,
    - expected return starting from $s$, taking action $a$ and thereafter following $\pi$
    - $Q^{\pi }(s)=\mathbb{E}_{\tau \sim \pi}\left[R(s,a,s') + \gamma Q^{\pi}(s',a')\mid s_0 = s,a_0=a \right]$ (_**Bellman Equation for $Q^\pi$**_)
  - _**(3) Advantage $A^\pi(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$**_
  - (4) If **$\pi^*$ is an optimal policy**:
    - Optimal action-value function $Q^{*}(s,a)=\max_{\pi} Q^{\pi}(s,a)$
    - Optimal state-value function $V^{*}(s)=\max_{\pi} V^{\pi}(s)=\max_{a} Q^{*}(s,a)$
- **4. Markov property**:
  - Transitions _**only depend on the most recent state and action**_, and no prior history: $P(s_{t+1} \mid s_t, a_t, ..., s_1, a_0, s_0) = P(s_{t+1} \mid s_t, a_t)$
  - _**when MDP is known**_, optimal policies can be found offline without interacting with the environment, using _**Dynamic Programming (DP)**_ algorithms.
  - _**otherwise**_, _**RL algorithms**_ come to a rescue to deal with delayed rewards by trial-and-error search
- **5. Extensions of MDP**:
  - This markov property assumption does not always hold, for instance
    - **_Partially Observable Markov Decision Process_**
      - when the observed state does not contain all necessary information 
    - **_Non-Stationary Markov Decision Process_**
      - when $P$ and $R$ actually depend on $t$.
    - _**Decentralized Control (Multi-Agent RL)**_
      - Multiple agents interact without centralized coordination (e.g., swarm robotics, competitive games).
