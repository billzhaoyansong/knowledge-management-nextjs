# Introduction

- https://arxiv.org/abs/2005.01643

## Online RL vs Off-policy RL vs Offline RL

- <img src="/books/Reinforcement Learning/online_rl_offline_rl.png"/>
- Comparison
  - In **online reinforcement learning (a)**, the policy $\pi_k$ is updated with streaming data collected by $\pi_k$ itself. 
  - In the classic **off-policy** setting (b), the agent’s **experience is appended to a data buffer** (also called a replay buffer) $\mathcal{D}$, and each new policy $\pi_k$ collects additional data, such that $\mathcal{D}$ is composed of samples from $π_0, π_1, ... , π_k$, and all of this data is used to train an updated new policy $π_{k+1}$.
  - In contrast, **offline reinforcement learning employs a dataset $\mathcal{D}$** collected by some (potentially unknown) behavior policy $\pi_{\beta}$. 
    - The dataset is collected once, and is not altered during training, which makes it feasible to use large previous collected datasets. 
    - The training process does not interact with the MDP at all, and the policy is only deployed after being fully trained.

## Offline RL Problem

- Given a static dataset $\mathcal{D} = \{(\mathbf{s}_t^i, \mathbf{a}_t^i, \mathbf{s}_{t+1}^i, r_t^i)\}$ collected by a behavior policy $\pi_\beta(\mathbf{a}|\mathbf{s})$, the goal is to learn a policy $\pi(\mathbf{a}|\mathbf{s})$ that maximizes the expected cumulative reward:l
$$
J(\pi) = \mathbb{E}_{\tau \sim p_\pi(\tau)}\left[\sum_{t=0}^H \gamma^t r(\mathbf{s}_t, \mathbf{a}_t)\right],
$$
where $\tau$ is a trajectory sampled from the policy $\pi$.

## Scenarios for Offline RL

- **Scenario**: When online exploration is unsafe or costly 
- **Examples**:
  - _**Healthcare Decision-Making**_
    - **Problem**: Optimize treatments (e.g., sepsis management, ICU interventions) using historical patient records.
    - **Challenge**: Online exploration is unsafe; policies must generalize from limited, biased datasets (e.g., severe cases are overrepresented).
    - **Example**: Offline RL trains policies from ICU logs (e.g., MIMIC-III dataset) to recommend treatments without risking patient harm.
  - _**Goal-Directed Dialogue Systems**_
    - **Problem**: Train chatbots (e.g., customer service, e-commerce) to generate responses that maximize user satisfaction.
    - **Challenge**: Collecting online human feedback is expensive; offline RL learns from logged human conversations.
    - **Example**: A restaurant-booking agent trained on past dialogues to improve response quality (Jaques et al., 2019).
  - _**Advertising & Recommender Systems**_
    - **Problem**: Optimize ad placements or recommendations using logged user interactions.
    - **Challenge**: Avoid costly A/B tests; offline RL evaluates policies via counterfactual estimators.
    - **Example**: Doubly robust estimators for click-through-rate optimization (Dudik et al., 2014).

## Challenges for Offline RL

- **Distributional Shift:** The learned policy $\pi$ may visit states/actions outside the support of $\mathcal{D}$, leading to erroneous value estimates.
- **Counterfactual Queries:** Offline RL requires estimating the outcomes of actions not present in $\mathcal{D}$.

## Offline RL via Importance Sampling

