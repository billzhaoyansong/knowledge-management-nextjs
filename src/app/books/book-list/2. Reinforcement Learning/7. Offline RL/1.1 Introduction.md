# Introduction

- https://arxiv.org/abs/2005.01643

## Online RL vs Off-policy RL vs Offline RL

- <img src="/books/Reinforcement Learning/online_rl_offline_rl.png"/>
- Comparison
  - In **online reinforcement learning (a)**, the policy $\pi_k$ is updated with streaming data collected by $\pi_k$ itself. 
  - In the classic off-policy setting (b), the agent’s experience is appended to a data buffer (also called a replay buffer) $D$, and each new policy $\pi_k$ collects additional data, such that $D$ is composed of samples from $π_0, π_1, ... , π_k$, and all of this data is used to train an updated new policy $π_{k+1}$.
  - In contrast, offline reinforcement learning employs a dataset $D$ collected by some (potentially unknown) behavior policy $\pi_{\beta}$. 
    - The dataset is collected once, and is not altered during training, which makes it feasible to use large previous collected datasets. 
    - The training process does not interact with the MDP at all, and the policy is only deployed after being fully trained.