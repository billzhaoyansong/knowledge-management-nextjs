# Tokenization

- TLDR: Tokenization techniques determines vocabulary and tokens

## Notations

- Token
  - token is an arbitrarily defined _**unit of text.**_
    - <img style="width:75%;max-width:100px;" src="/books/Transformers_LLMs/token_example.png"/>
  - <details><summary>granularities of tokens</summary>
      <img style="width:75%;max-width:800px;" src="/books/Transformers_LLMs/token_examples.png"/>
    </details>
- Vocabulary ($V$)
  - vocabulary is a fixed set of predefined tokens with size $|V|$.
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/vocabulary_example.png"/>
  - <details><summary>Special tokens</summary>
      <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/token_special.png"/>
    </details>

## Tokenizer

- Intro
  - A tokenizer $T$ is a process that converts an input text into tokens
- Process
  - _**Encode**_: 
    - Convert _text into tokens_, which will be able to be represented and processed by the model.
      - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/encoder.png"/>
  - _**Decode**_:
    - Convert _tokens back to text_ format, which is typically used when the model makes token-level predictions which need to be translated into natural language
      - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/decoder.png"/>
- Types of tokenizers:
  - _**Rule-based**_: The tokenizer $T$ does not need to be trained, Word-level and character-level tokenizations are prime examples of rule-based tokenizers. 
  - _**Learned**_: The tokenizer $T$ needs to be trained and typically leads to better results since it directly learns relevant patterns from the data. (Examples of learned tokenizers include BPE, WordPiece and Unigram.)
  - <details><summary><i>Why subword tokenizer over word-level and character-level tokenizers?</i></summary>

      - **_subword tokenizer_** is superior over _word-level_ and _character-level_ tokenization methods in that:
        - _Leverage root meanings_: account for these similarities in tokens (e.g. run, runner, and running) by dividing input text in a root-sensitive manner.
        - _Reduce out-of-vocabulary occurrences_: recognizing small variations in words, such as singular $bear$ and plural $bears$.

    </details>
- Normalization: 
  - used to deal with inconsistencies in the input text, e.g.:
    - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/tokenizer_normalization.png"/>

## Byte-Pair Encoding (BPE)

- Intro
  - BPE is a subword tokenization algorithm that constructs the vocabulary by learning from the most common pairs of entities that are present in the training corpus
- Process
  - _**1.1 Tokenizer training**_
    - _(1) Initialize vocabulary_: Divide the corpus into characters and count the occurrence of each character
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step1.png"/>
          - The initial$$ vocabulary is composed of the unique characters that appeared in the corpus and has a size $v_i$.
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step1-1.png"/>

        </details>
    - _(2) Add new elements to the vocabulary_: Merge characters to form new tokens.
      - <details><summary>details</summary>

        - First, count the frequency of token pairs in the initial corpus.
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2.png"/>
        - Next, select the pair that occurred the most often.
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2-1.png"/>
        - Then, merge the elements of that pair and add it to the vocabulary.
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2-2.png"/>
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2-3.png"/>

        </details>
    - _(3) Finalize vocabulary_: Add new elements until the vocabulary reaches the desired size of $|V| = v_f > v_i$. Also, add any predefined tokens that handle special cases, such as the unknown token `[UNK]`
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step3.png"/>

        </details>
  - _**1.2 Encoding**_
    - _(1) Divide into characters_: The input text is divided into characters.
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step4.png"/>

        </details>
    - _(2) Apply merge rules_: We start by applying the first merge rule of the list; then successively apply the remaining merge rules in the order they were inserted in the vocabulary.
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step5-1.png"/>
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step5-2.png"/>
          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step5-3.png"/>

        </details>
- _**Applications**_: GPT, LLaMA, etc
- _**Variations**_: The _WordPiece algorithm_ merges based on the most likely pair. (used in BERT).

## Unigram algorithm

- Intro
  - Unigram algorithm is a subword tokenization algorithm that assumes that the probability of a tokenâ€™s appearance is independent of any tokens that appeared previously.
- Process
  - _**1.1 Tokenizer training**_
    - _(1) Initialize vocabulary_: Start from an arbitrarily large vocabulary composed of all possible subsets of characters in the corpus
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/Unigram_step1.png"/>

        </details>
    - _(2) Refine vocabulary_: Reduce the number of elements in the vocabulary until reaching the desired vocabulary size
      - <details><summary>details</summary>

          - _Compute probability_: Compute the loss of each subword of the vocabulary using the _Expectation-Maximization algorithm_
            - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/Unigram_step2.png"/>
          - _Prune vocabulary_: Keep the top 80% elements among subwords that increase the loss the most.
            - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/Unigram_step3.png"/>

        </details>
  - _**2.2 Encoding**_
    - _(1) Consider possible segmentations_: List all possible segmentations of the word using elements of the vocabulary.
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/Unigram_step4.png"/>

        </details>
    - _(2) Compute associated probabilities_: Use the probability of each element of the vocabulary to compute the probability of the word with a given segmentation.
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/Unigram_step5.png"/>

        </details>
    - _(3) Find the best segmentation_: Select the segmentation that corresponds to the highest score.
      - <details><summary>details</summary>

          - <img style="width:75%;max-width:200px;" src="/books/Transformers_LLMs/Unigram_step6.png"/>

        </details>
- _**Applications**_: T5
