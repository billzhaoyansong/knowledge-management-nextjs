# Embeddings

## Encodings

- Encoding vs Embedding
  - Encoding is the process of converting data from one format to another.
    - e.g. _**one-hot encoding**_, label encoding, etc.
  - Embedding, as a type of encoding, is a _**learn-from-data**_, _**low-dimensional**_, _**dense vector**_ representation of a discrete variable.
    - e.g. word embeddings (_**word2vec**_, _**GloVe**_, etc.)

- One-Hot Encoding (OHE)
  - OHE is a method that represents each token $i$ of a vocabulary $V$ with a vector of size $|V|$ where the $i$th element is $1$ and all other elements are $0$
    - <img style="width:240px;" src="/books/Transformers_LLMs/one-hot.png"/>
- *Limitations of OHE*
  - <details><summary><i>(1) Loss of Token Similarity</i></summary>

      - The resulting token vectors are orthogonal to one another, leading to the loss of the ability to compare the similarity between two tokens (synonyms should be closer together and antonyms farther apart)
        - <img style="width:400px;" src="/books/Transformers_LLMs/one-hot-table.png"/>
    </details>

  - <details><summary><i>(2) High Dimensionality</i></summary>

    - The dimension of the resulting token vector is the same as the number of distinct words in the corpus of interest ($\approx 10^4 - 10^5$),which significantly increases computational requirements and memory usage.
    - <img style="width:150px;" src="/books/Transformers_LLMs/One-hot-dimension.png"/>
    </details>


## Word2vec

- Intro
  - Word2vec is a family of methods that aims at generating token embeddings that are _**continuous**_ (float number in each dimension) and _**fixed dimension**_ (regardless of the size of the vocabulary)
      - <img style="width:200px;" src="/books/Transformers_LLMs/Word2vec.png"/>
  - The resulting _**embeddings capture semantic relationships**_ between words and often reflect intuitive patterns (e.g., similar words are close together in the embedding space).
    - <details><summary><i>example</i></summary>

        - <img style="width:200px;" src="/books/Transformers_LLMs/word2vec-meaningful.png"/>

      </details>
- Architecture
  - Word2vec uses a shallow neural network with a single hidden layer.
    - <img style="width:200px;" src="/books/Transformers_LLMs/word2vec-arch.png"/>
  - The network learns token embeddings $x_{\text{embed}}$ through the weights of the projection (hidden) layer during training.
    - <img style="width:200px;" src="/books/Transformers_LLMs/word2vec-pipeline.png"/>
- Comparison of Word2vec Algorithms
  - CBOW is several times faster to train than the skip-gram, slightly better accuracy for the frequent words.
  - _**Skip-Gram**_ works well with small amount of the training data and _represents well even rare words or phrases_, making it _**a better choice most of the time**_, but with the price of increased computational cost. [(reference)](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling)
  - <details><summary><i>architectures of CBOW and Skip-Gram</i></summary>

      - <img style="width:500px;" src="/books/Transformers_LLMs/word2vec.webp"/>

    </details>

## Continuous Bag Of Words (CBOW)

- Intro
  - CBOW predicts a _target token_ based on the _**average of its context token embeddings**_.
- **Training**
  - **_(1) Initialize network_**: initialize a neural network with one hidden layer and fix the context size $C$ on each side of the target token. 
    - <details><summary>details</summary>

      - <img style="width:250px;" src="/books/Transformers_LLMs/CBOW-init.png"/>

      </details>
  - **_(2) Create training data_**: Consider _context tokens OHEs_ within the context window as input, and _target token OHE_ as output.
    - <details><summary>details</summary>

      - input: $(\text{OHE}_{\text{context-word-1}}, \cdots, \text{OHE}_{\text{context-word-2C}})$
      - output: $\text{OHE}_{\text{target-word}}$

      </details>
  - **_(3) Forward pass_**: Context token OHEs are projected and then averaged. The resulting embedding is passed through the output layer to predict the target token.
    - <details><summary>details</summary>

        - <img style="width:250px;" src="/books/Transformers_LLMs/CBOW-arch.png"/>

      </details>

## Skip-Gram

- Intro
  - Skip-Gram predicts _context tokens_ within a fixed window _given the target token_.
- **Training**
  - **_(1) Initialize network_**: initialize a neural network with one hidden layer fix the context size $C$ on each side of the target token.
    - <details><summary>details</summary>

      - <img style="width:250px;" src="/books/Transformers_LLMs/Skip_gram_init.png"/>

      </details>
  - **_(2) Creating training data_**: Consider the target token OHE as input and the OHEs of the context tokens as output. [(for numerical example)](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling)
    - <details><summary>details</summary>

        - $(\text{OHE}_{\text{target-word}}, \text{OHE}_{\text{context-word-1}})$
        - $(\text{OHE}_{\text{target-word}}, \text{OHE}_{\text{context-word-2}})$
        - $\cdots$
        - $(\text{OHE}_{\text{target-word}}, \text{OHE}_{\text{context-word-2C}})$

      </details>
  - **_(3) Forward pass_**: After projecting the target token OHE, predict each of its context tokens.
    - <details><summary>details</summary>

        - <img style="width:250px;" src="/books/Transformers_LLMs/Skip_gram_arch.png"/>

      </details>

## Skip-Gram with Negative Sampling (SGNS)

- Motivation
  - the softmax operation at the output layer of the skip-gram model requires summing over all classes $V$ and becomes too expensive if $|V|$ is large
  - <details><summary><i>example on skip-gram</i></summary>

      - assume: number of input token: 1, number of output token: $|V|$
      - The softmax probability for a specific context word $w_o$ given a center word $w_c$ is:
        - $P(w_o | w_c) = \frac{\exp(u_o^T v_c)}{\sum_{i=1}^{V} \exp(u_i^T v_c)}$
          - $v_c$: The embedding vector for the center word $w_c$.
          - $u_o$: The "output" embedding vector for the context word $w_o$.
      - the denominator requires $|V|$ times expensive dot-product and exponential calculations.

    </details>

- *Negative sampling*: reframes the multi-class classification into binary classficiation and simplifies by **_sampling a small number_** of negative examples instead of considering all of them
  - <details><summary>details</summary>

      - 

    </details>

## Continuous encodings - GloVe

- *Global Vectors for word representation (GloVe)* is technique that leverages co-occurrences to derive word embeddings.
- **Algorithm**
  - (1) Construct co-occurrence matrix
    - 1.1 initialize a symmetric context window size
      - <img style="width:250px;" src="/books/Transformers_LLMs/glove-init1.png"/>
    - 1.2 build a co-occurence matrix $X$ where each $X_{i,j}$ denotes the number of times that a target word $i$ occurred with a context word $j$ ($X$ will be symmetric)
      - <img style="width:150px;" src="/books/Transformers_LLMs/glove-init2.png"/>
  - (2) Model co-occurrence
    - model the logarithmic co-occurrence $\log(X_{i,j})$ of a target word $i$ and context word $j$ using the target embedding $w_{t,i}$ and context embedding $w_{c,j}$ along with their respective bias terms $b_{t,i}$ and $b_{c,j}$
      - <img style="width:150px;" src="/books/Transformers_LLMs/glove-model.png"/>
  - (3) Learn weights
    - Use gradient descent to minimize the weighted squared loss $\mathcal{L}$ to learn the embeddings $w_t$ and $w_c$:
      - <img style="width:250px;" src="/books/Transformers_LLMs/glove-learn.png"/>
  - (4) Deduce embeddings
    - Given the symmetry that $w_t$ and $w_c$ play in this model, the final word embedding $w$ is given by
      - $\boxed{w=\frac{w_t+w_c}{2}}$
