# Similarity

## Vector Norm

- **Vector Norm**
  - The _norm_ of a vector is a quantified number of its measure and
is used in various settings.
- **Types of Norms**
  - <img style="width:350px;" src="/books/Transformers_LLMs/similarity-norm-table1.png"/>
  - <img style="width:350px;" src="/books/Transformers_LLMs/similarity-norm-table2.png"/>

## t-distributed Stochastic Neighbor Embedding (t-SNE)

- **Definition**
  - **$t$-SNE** is used to reduce high-dimensional embeddings $\{x_1,...,x_n\} \in \mathbb{R}^D$ into a lower dimensional space $d < D$ for better visualization
- **Idea**
  - Find _**a low-dimensional representation**_ that _**preserves similarities**_ between data points as seen in the original high-dimensional space _**using**_ a similarity score that relies on the _**t-distribution**_. (typically $d=2$)
    - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-idea.png"/>
- **Algorithm**
  - _**(1) Quantify high-dimensional similarity**_: Quantitatively characterize the relative position of each observation in the original high-dimensional space
    - (1.1) Compute pairwise distances $\Vert x_i −x_j\Vert^2$ between each pair of observations $x_i$ and $x_j$.
    - (1.2) Compute similarity scores between observations in the high-dimensional space using a Gaussian kernel $K_D$ of parameter $σ_i$ (cosine-similarity also applicable here):
      - $K_D(x_i, x_j) = \exp \left( - \frac{\Vert x_i - x_j \Vert ^2}{2 \sigma_i^2} \right)$
        - $\sigma_i$ determines the proximity of observations $j$ to observation $i$
          - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-similarity.png"/>
    - (1.3) For each observation $x_i$, use the similarity score to compute the conditional probability $p(j|i)$ that another observation $x_j$ would be neighbor to $x_i$.
      - $p(j|i) = \frac{K_D(x_i, x_j)}{\sum_{k\neq i} K_D(x_i, x_j)}$
    - (1.4) Normalize the socre
      - $\boxed{p_{i,j} = \frac{p(j|i) + p(i|j)}{2n}}$
  - _**(2) Initialize low-dimensional observations**_: Initialize $n$ observations $y_1^0, ..., y_n^0$ in a lower-dimensional space of desired dimension $d$
    - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-init.png"/>
  - _**(3) Align low-dimensional representation with the initial high-dimensional one**_
    - (3.1) Compute similarity score between observations in the low-dimensional space using a kernel $K_d$ defined as:
      - $K_d(y_i, y_j) = \frac{1}{1+\Vert y_i - y_j \Vert^2}$
    - (3.2) Compute pairwise similarities $q_{i,j}$ between each pair of observations $y_i$ and $y_j$ using $K_d$:
      - $\boxed{q_{i,j} = \frac{K_d(y_i, y_j)}{\sum_k\sum_{l \neq k}(y_k, y_l)}}$
    - (3.3) Compute the KL-divergence loss between probability distributions of observations:
      - $\boxed{\text{KL}(P \Vert Q) = \sum_{i \neq j} p_{i,j} \log \left(\frac{p_{i,j}}{q_{i,j}} \right)}$
      - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-out.png"/>
    - (3.4) Use gradient descent to decrease the loss until convergence
      - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-sgd.png"/>
- **Hyperparameters**
  - _**Number of iterations**_ in the gradient descent algorithm
  - _**Perplexity**_ to balances the focus between local and global aspects of the data
    - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-perplexity.png"/>
- **Application**
  - use t-SNE on word embeddings to visualize them in a 2D space
    - <img style="width:350px;" src="/books/Transformers_LLMs/t-dist-application-word.png"/>