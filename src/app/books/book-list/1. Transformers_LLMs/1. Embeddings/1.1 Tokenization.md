# Tokenization

## Notations

- **Token**: is an arbitrarily defined unit of text.
  - <img style="width:75%;max-width:100px;" src="/books/Transformers_LLMs/token_example.png"/>
  - granularities of tokens:
    - <img style="width:75%;max-width:800px;" src="/books/Transformers_LLMs/token_examples.png"/>
- **Vocabulary**: , $V$, is a fixed set of predefined tokens.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/vocabulary_example.png"/>
  - _**Special tokens**_: The vocabulary also contains _special tokens_ that convey a specific meaning. The common ones are:
    - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/token_special.png"/>
  - _**Size**_: The order of magnitude of the size $|V|$ of the vocabulary V depends on:
    - _Granularity of tokens_
    - _Target text_
- **Tokenizer**: A tokenizer $T$ is a process that converts an input text into tokens
  - _**Encode**_: Convert _text into tokens_, which will be able to be represented and processed by the model.
    - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/encoder.png"/>
  - _**Decode**_: Convert _tokens back to text_ format, which is typically used when the model makes token-level predictions which need to be translated into natural language
    - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/decoder.png"/>
  - _**Types of tokenizers**_:
    - _**Rule-based**_: The tokenizer $T$ does not need to be trained, Word-level and character-level tokenizations are prime examples of rule-based tokenizers. 
    - _**Learned**_: The tokenizer $T$ needs to be trained and typically leads to better results since it directly learns relevant patterns from the data. (Examples of learned tokenizers include BPE, WordPiece and Unigram.)
    - _**Discussion**_: **_subword tokenizer_** is superior over _word-level_ and _character-level_ tokenization methods in that:
      - _Leverage root meanings_: account for these similarities in tokens (e.g. run, runner, and running) by dividing input text in a root-sensitive manner.
      - _Reduce out-of-vocabulary occurrences_: recognizing small variations in words, such as singular $bear$ and plural $bears$.
  - _**Normalization**_: used to deal with inconsistencies in the input text, e.g.:
    - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/tokenizer_normalization.png"/>

## Subword Tokenization Algorithms

- **BPE**: ,Byte-Pair Encoding (BPE), constructs the vocabulary by learning from the most common pairs of entities that are present in the training corpus
  - _**1. Tokenizer training**_
    - _(1) Initialize vocabulary_: Divide the corpus into characters and count the occurrence of each character
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step1.png"/>
      - The initial vocabulary is composed of the unique characters that appeared in the corpus and has a size $v_i$.
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step1-1.png"/>
    - _(2) Add new elements to the vocabulary_: 
      - First, count the frequency of token pairs in the initial corpus.
        - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2.png"/>
      - Next, select the pair that occurred the most often.
        - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2-1.png"/>
      - Then, merge the elements of that pair and add it to the vocabulary.
        - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2-2.png"/>
        - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step2-3.png"/>
    - _(3) Finalize vocabulary_: Add new elements until the vocabulary reaches the desired size of $|V| = v_f > v_i$. Also, add any predefined tokens that handle special cases, such as the unknown token `[UNK]`
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step3.png"/>
  - _**2. Encoding**_
    - _(1) Divide into characters_: The input text is divided into characters.
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step4.png"/>
    - _(2) Apply merge rules_: We start by applying the first merge rule of the list; then successively apply the remaining merge rules in the order they were inserted in the vocabulary.
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step5-1.png"/>
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step5-2.png"/>
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/BPE_step5-3.png"/>
  - _**Applications**_: GPT, LLaMA, etc
  - _**Variations**_: The _WordPiece algorithm_ merges based on the most likely pair. (used in BERT).

- **Unigram algorithm**: assumes that the probability of a tokenâ€™s appearance is independent of any tokens that appeared previously.
  - _**1. Tokenizer training**_
    - _(1) Initialize vocabulary_: Start from an arbitrarily large vocabulary composed of all possible subsets of characters in the corpus
      - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/Unigram_step1.png"/>
    - _(2) Refine vocabulary_: Reduce the number of elements in the vocabulary until reaching the desired vocabulary size
      - _Compute probability_: Compute the loss of each subword of the vocabulary using the _Expectation-Maximization algorithm_
        - <img style="width:75%;max-width:200px;" src="/books/Transformers_LLMs/Unigram_step2.png"/>
      - _Prune vocabulary_: Keep the top 80% elements among subwords that increase the loss the most.
        - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/Unigram_step3.png"/>
  - _**2. Encoding**_
    - _(1) Consider possible segmentations_: List all possible segmentations of the word using elements of the vocabulary.
        - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/Unigram_step4.png"/>
    - _(2) Compute associated probabilities_: Use the probability of each element of the vocabulary to compute the probability of the word with a given segmentation.
      - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/Unigram_step5.png"/>
    - _(3) Find the best segmentation_: Select the segmentation that corresponds to the highest score.
      - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/Unigram_step6.png"/>
  - _**Applications**_: T5
