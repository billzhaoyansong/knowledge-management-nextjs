# Token Embeddings

- TLDR: Token embedding techniques turn tokens into vectors

## One-hot encodings

- Definition:
  - One-Hot Encoding (OHE) is a method that represents each token $i$ of a vocabulary $V$ with a vector of size $|V|$ where the $i$th element is $1$ and all other elements are $0$
    - <img style="width:240px;" src="/books/Transformers_LLMs/one-hot.png"/>
- *Limitations*:
  - <details><summary><i>(1) Loss of Token Similarity</i></summary>

      - The resulting token vectors are orthogonal to one another, leading to the loss of the ability to compare the similarity between two tokens (synonyms should be closer together and antonyms farther apart)
        - <img style="width:400px;" src="/books/Transformers_LLMs/one-hot-table.png"/>
    </details>

  - <details><summary><i>(2) High Dimensionality</i></summary>

    - The dimension of the resulting token vector is the same as the number of distinct words in the corpus of interest ($\approx 10^4 - 10^5$),which significantly increases computational requirements and memory usage.
    - <img style="width:150px;" src="/books/Transformers_LLMs/One-hot-dimension.png"/>
    </details>


## Continuous encodings - Word2vec

- **Definition**
  - Word2vec is a family of methods that aims at generating token embeddings that are _**continuous**_ (float number in each dimension) and _**fixed dimension**_ (regardless of the size of the vocabulary)
    - <img style="width:300px;" src="/books/Transformers_LLMs/Word2vec.png"/>
- **Architecture**
  - a shallow neural network with one hidden layer
    - <img style="width:200px;" src="/books/Transformers_LLMs/word2vec-arch.png"/>
- **Results**
  - through projection layer, the token embeddings $x_{\text{embed}}$ are empirically found.
    - <img style="width:250px;" src="/books/Transformers_LLMs/word2vec-pipeline.png"/>
  - embeddings *make intuitive sense* and *convey relationships between tokens*
    - <img style="width:250px;" src="/books/Transformers_LLMs/word2vec-meaningful.png"/>
- **Model Type 1. CBOW**
  - *Continuous Bag Of Words (CBOW)*  predicts a _target token_ based on the ***average of its context token embeddings***.
  - slower training time compared to CBOW because each target token generates 2 Ã— C data points
  - **Training**
    - _(1) Initialize network_: initialize a neural network with one hidden layer and fix the context size $C$ on each side of the target token. 
      - <img style="width:250px;" src="/books/Transformers_LLMs/CBOW-init.png"/>
    - _(2) Represent input_: Consider OHEs of all the tokens within the context window as input.
    - _(3) Forward pass_: Context token OHEs are projected and then averaged. The resulting embedding is passed through the output layer to predict the target token.
      - <img style="width:250px;" src="/books/Transformers_LLMs/CBOW-arch.png"/>
- **Model Type 2. Skip-gram**
  - *Skip-gram* predicts _context tokens_ within a fixed window _given the target token_.
  - **Training**
    - _(1) Initialize network_: initialize a neural network with one hidden layer fix the context size $C$ on each side of the target token.
      - <img style="width:250px;" src="/books/Transformers_LLMs/Skip_gram_init.png"/>
    - _(2) Represent input_: Consider the OHE of the target token as input
    - _(3) Forward pass_: After projecting the target token OHE, predict each of its context tokens.
      - <img style="width:250px;" src="/books/Transformers_LLMs/Skip_gram_arch.png"/>
- **Model Type 3. Negative sampling**
  - *Motivation*: the softmax operation requires summing over all classes $V$ and becomes too expensive if $|V|$ is large
  - *Negative sampling* reframes the multi-class classification into binary classficiation and simplifies by **_sampling a small number of negative examples_** instead of considering all of them

## Continuous encodings - GloVe

- *Global Vectors for word representation (GloVe)* is technique that leverages co-occurrences to derive word embeddings.
- **Algorithm**
  - (1) Construct co-occurrence matrix
    - 1.1 initialize a symmetric context window size
      - <img style="width:250px;" src="/books/Transformers_LLMs/glove-init1.png"/>
    - 1.2 build a co-occurence matrix $X$ where each $X_{i,j}$ denotes the number of times that a target word $i$ occurred with a context word $j$ ($X$ will be symmetric)
      - <img style="width:150px;" src="/books/Transformers_LLMs/glove-init2.png"/>
  - (2) Model co-occurrence
    - model the logarithmic co-occurrence $\log(X_{i,j})$ of a target word $i$ and context word $j$ using the target embedding $w_{t,i}$ and context embedding $w_{c,j}$ along with their respective bias terms $b_{t,i}$ and $b_{c,j}$
      - <img style="width:150px;" src="/books/Transformers_LLMs/glove-model.png"/>
  - (3) Learn weights
    - Use gradient descent to minimize the weighted squared loss $\mathcal{L}$ to learn the embeddings $w_t$ and $w_c$:
      - <img style="width:250px;" src="/books/Transformers_LLMs/glove-learn.png"/>
  - (4) Deduce embeddings
    - Given the symmetry that $w_t$ and $w_c$ play in this model, the final word embedding $w$ is given by
      - $\boxed{w=\frac{w_t+w_c}{2}}$
