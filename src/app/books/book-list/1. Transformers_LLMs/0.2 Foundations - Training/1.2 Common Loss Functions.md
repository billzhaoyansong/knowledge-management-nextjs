
# Common Loss Functions

- **1. Cross-entropy loss** (CE loss): also known as _logistic loss_ or _log loss_, used in classification tasks. It penalizes deviations of the predicted probability $\hat{y}$ from the actual class $y$.
  - _**Binary class**_: $\boxed{\text{BCE}(\hat{y}, y) = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]}$
    - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_BCE.png"/>
  - _**Multi-class**_: $\boxed{\text{CE}(\hat{y}, y) = - \sum_{i=1}^k y_i \log(\hat{y_i})}$
- **2. KL divergence**: _**K**ullback-**L**eibler divergence_ is a measure that compares two probability distributions $P = (p_i)_{i \in [[1,n]]}$ and $Q = (q_i)_{i \in [[1,n]]}$
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_KLDiv.png"/>
  - $\boxed{\text{KL}(P \Vert Q) = \sum_{i=1}^n p_i \log\left( \frac{p_i}{q_i} \right)}$
    - Example: Given $p_i, q_i$ probability distributions over values $i = 1, 2$, we note: $p_1 = p$ and $p_2 = 1 − p$; $q_1 = q$ and $q_2 = 1 − q$
      - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_KLDiv_Illus.png"/>
  - _**Applications**_: KL divergence is used in natural language processing, image recognition, and dimensionality reduction techniques such as t-SNE.
  - _Remark_: usually $\text{KL}(P \Vert Q) \neq \text{KL}(Q \Vert P)$
- **3. MAE**: Mean Absolute Error, also known as the $L_1$ loss, is a loss function used for regression tasks.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_L1_loss.png"/>
  - $\boxed{\text{MAE}(\hat{y}, y) = \frac{1}{n}\sum_{i=1}^n |\hat{y_i} - y_i|}$
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_L1_Loss_Illus.png"/>
  - _**Discussion**_: MAE is mostly insensitive to outliers because each absolute error contributes only linearly to the loss. It can be a good choice when we don’t want outliers to disproportionately influence the loss.
- **MSE**: Mean Squared Error, also known as the L2 loss, is a loss function used for regression tasks
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_L2_loss.png"/>
  - $\boxed{\text{MSE}(\hat{y}, y) = \frac{1}{n}\sum_{i=1}^n (\hat{y_i} - y_i)^2}$
    - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_L2_Loss_Illus.png"/>
  - _**Discussion**_: MSE is more sensitive to outliers than MAE because it squares each error term, which disproportionately increases the contribution of larger errors to the total loss.
- **4. RMSE**: Root Mean Squared Error is deduced from the MSE
  - $\boxed{\text{RMSE} = \sqrt{\text{MSE}}}$
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_RMSE_Loss_Illus.png"/>
  - _**Discussion**_: Compared to MSE, RMSE can be useful because it provides an error metric in the same unit as the target variable, making it easier to interpret the magnitude of prediction errors.
