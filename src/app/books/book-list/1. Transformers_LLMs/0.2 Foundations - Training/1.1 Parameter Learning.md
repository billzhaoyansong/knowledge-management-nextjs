# Parameter learning

- **Weight initialization**: Before starting model training, we need to initialize each parameter $\theta$ of the network to a certain value $\theta_0$ (this determines the starting optimization direction, influencing both the quality and speed of training convergence)
  - _**Xavier/Glorot**_ and _**He**_ initialization methods are commonly used, chosen based on the activation function used by the associated layer
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_initialization.png"/>
- **Epoch**:  a single iteration in which the model sees the entire training set once; parameters associated with epoch:
  - **_Training size $N$_**: Number of observations contained in the training set.
  - **_Batch size $b$_**: Number of observations processed by the model at each pass.
  - **_Number of steps $s$ per epoch_**: Number of iterations needed for the model to see the entire training set once
    - $\boxed{N=b \times s}$
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_epoch.png"/>
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_steps.png"/>
- **Loss function**: A loss function $\mathscr{L}$ is a function that evaluates how close model predictions $\hat{y}$ are to target values $y$.
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_loss.png"/>
  - Higher values denote worse performance with respect to target values, hence the wording "_loss_".
  - Sometimes write $\mathscr{L}(\theta)$ to indicate the dependency of the loss with respect to the model parameters $\theta$.
- **Classification label**: For a given class $i$, label $y_i$ is the ground truth that the model predicts via $y_i$.
  - _**Hard label**_: Each observation either belongs to class $i$ ($y_i = 1$) or it does not ($y_i = 0$) (can be used in image classification problems).
    - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_hard_label.png"/>
  - _**Soft label**_: Each observation belongs to class i with probability $y_i \in [0,1]$ (commonly used in the next word prediction task).
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_soft_label.png"/>
- **Backpropagation**: a model training procedure that aims at minimizing loss function $\mathscr{L}$ by adjusting model parameters $\theta$ through the calculation of gradients $\nabla\mathscr{L}(\theta)$.
- **Algorithm**
  - _**Forward step**_: Take a batch of training data and perform forward propagation to compute the model predictions $\hat{y}$ and deduce the loss $\mathscr{L}(\hat{y},y)$.
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_forward_step.png"/>
  - _**Backward step**_: Compute the gradient of the loss $\frac{\partial \mathscr{L} (\hat{y},y)}{\theta_i}$ parameter $\theta_i$ of the network.
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_backward_step.png"/>
  - _**Update step**_: Use the computed gradients to update each parameter $\theta_i$ of the network towards a direction that best decreases the loss $\mathscr{L}$
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_update_step.png"/>
- **Training stability**
  - _**exploding gradient**_ and _**vanishing gradient**_
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/exploding_gradient.png"/>
    - _**Gradient clipping**_: cap the gradient $\Vert \mathscr{L}\Vert$ with a ceiling $C$
      - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/gradient_clipping.png"/>
- **Practical tips**
  - _**Gradient checking**_: a sanity check method at the backward pass by comparing the analytical gradient value with the numerical gradient value
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/gradient_check.png"/>
  - _**Debugging a model by overfitting**_: the model tries to fit a mini-batch $\mathscr{B}$ $N$ times in a row. If it cannot make the loss minimal, then it means that we have a problem with the following possible culprits:
    - The model is _**too simple**_ to capture the underlying patterns in the data.
    - The loss function $\mathscr{L}$ is either incorrect or incorrectly implemented.
    - The backward pass is incorrectly implemented.
    - Hyperparameters such as learning rate have inappropriate values.
    - The model parameters $\theta$ have been initialized at bad values $\theta_0$
- **Learning Rate**: ($\alpha$ or $\eta$) controlling the pace at which weights $\theta$ get updated
  - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_learning_rate.png"/>
- **Warmup**: Use a lower learning rate Î± during the first steps $s$ to prevent model weights $\theta$ from being changed too dramatically due to noisy gradient estimates $\nabla \mathscr{L}$.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_warmup.png"/>
  - An example: $\boxed{\alpha(s) = C \min \left(\frac{1}{\sqrt{s}}, \frac{s}{s_w^{\frac{3}{2}}} \right)}$
    - $s_w$: the parameter of warmup steps
    - For small numbers of steps s, the learning rate increases linearly, which helps with the gradient
    - Starting from $s = s_w$, the learning rate decays, which mimics a traditional learning rate schedule
