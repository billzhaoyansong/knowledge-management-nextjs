# Regularization

- regularization methods are used to ensure that our model generalizes well (**avoid overfitting**).
- **1. Dropout**: a technique used in neural networks to prevent overfitting the training data.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_dropout.png"/>
  - _**Training**_
    - (1) _Forward pass_: Each neuron is dropped with probability $p$.
      - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_dropout_forward.png"/>
    - (2) _Backward pass_: Updates are only made to parameters associated with neurons that were not dropped.
      - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_dropout_backward.png"/>
  - _**Inference**_
    - None of the neurons gets dropped
- **2. Weight regularization**: to control the magnitude of weights and avoid overfitting the training set, "weight regularization" techniques can be performed on the model weights through a penalty term added to the loss $\mathscr{L}$
  - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_weight_regularization.png"/>
- **3. Early stopping**: stops the training process as soon as the validation loss reaches a plateau or starts to increase
  - <img style="width:75%;max-width:200px;" src="/books/Transformers_LLMs/training_early_stopping.png"/>
- **4. Batch normalization (BN)**: improves the stability of training by normalizing inputs across a batch. (BN can be applied before ReLU (Andrew Ng) or after ReLU (Fran√ßois Chollet))
  - _**Steps to complete BN**_
    - (1) _Compute statistics across the batch_: We compute the mean $\mu_B$ and variance $\sigma_b^2$ across the batch $B$:
      - $\mu_B=\frac{1}{m}\sum\limits_{i=1}^m x_i$ and variance $\sigma_B^2=\frac{1}{m}\sum\limits_{i=1}^m (x_i - \mu_B)^2$
      - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/training_BN_step1.png"/>
    - (2) _Compute normalized activation_: We normalize each dimension $k \in [[1, d]]$ using the quantities above along with a small quantity $\epsilon > 0$ used for numerical stability:
      - $\hat{x}_{i,k}=\frac{x_{i,k} - \mu_{B,k}}{\sqrt{\sigma_{B,k}^2+\epsilon}}$
    - (3) _Perform scaling and shifting_:
      - $\boxed{\text{BN}(x_i)=\gamma \hat{x}_i+\beta}$
        - $\gamma, \beta \in \mathbb{R}^d$ are learnable parameters