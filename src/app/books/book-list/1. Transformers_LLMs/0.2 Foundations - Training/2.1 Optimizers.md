
# Optimizers

- **Optimizer**: Given a model with parameters $\theta$, an _optimizer_ is a parameter update strategy aiming at finding optimal parameters $θ^*$ that minimize the loss.
  - In order to do that, it updates the parameters $\theta$ of the model at each iteration:
    1. _Forward pass_: Compute the loss $\mathscr{L}(\theta)$.
    2. _Backward pass_: Compute the gradient of the loss $\nabla\mathscr{L}(\theta)$ with respect to parameters $\theta$ and update parameters $\theta$ using the optimizer’s update rule.
  - Following are three ways to update:
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_update_weights.png"/>
  - The mini-batch method is widely used and optimized via the hyperparameter batch size $b$, typically set as a power of $2$ to leverage all available hardware optimization techniques such as memory alignment and parallel processing.
- **1. Gradient descent**: an optimizer defining an update rule of the model parameters $\theta$ based on the direction of the biggest loss decrease $-\nabla\mathscr{L}(\theta)$.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_gradient_descent.png"/>
  - _**Update formula**_: $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha \nabla \mathscr{L} (\theta_t)}$
    - $\alpha > 0$, typically $0.01$
- **2. Momentum**:  an optimizer that aims at accelerating the pace at which the model converges by dampening oscillations of the loss $\mathscr{L}$.
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_momentum.png"/>  
  - _**Update formula**_: $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha v_t}$
    - $v_{t+1} \leftarrow \beta v_t + (1- \beta) \nabla \mathscr{L} (\theta_t)$
      - $\alpha > 0$, typically $0.01$
      - $\beta \in [0,1]$ the "momentum", typically $0.9$
  - _**Advantages of Momentum**_
    - _Reduction of oscillations_: By smoothing out the parameter updates, momentum reduces the zigzagging effect, which leads to more stable and efficient convergence.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_momentum_adv1.png"/> 
    - _Overcoming local minima_:  By incorporating the previous gradients, momentum helps the optimizer maintain its direction and potentially escape shallow local minima, which improves the likelihood of reaching a global minimum.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_momentum_adv2.png"/> 
- **3. RMSProp**: **R**oot **M**ean **S**quared **Prop**agation (RMSProp) an optimizer that speeds up the learning process by controlling oscillations. It does so by taking into account the inertia from previous updates through the moving average of the squared loss gradients $(\nabla\mathscr{L}(\theta))^2$.
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_rmsprop.png"/> 
  - _**Update formula**_:  $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha \frac{\nabla \mathscr{L} (\theta_t)}{\sqrt{v_t}+\epsilon}}$
    - $v_{t+1} \leftarrow \beta v_t + (1- \beta) (\nabla \mathscr{L} (\theta_t))^2$
      - $\alpha > 0$, typically $0.01$
      - $\beta \in [0,1]$, the decay rate, typically $0.9$
      - $\epsilon \ll 1$, a numerical stability constant used to guard against divisions by zero.
  - _**Advantages of RMSProp**_
    - _Adaptive learning rates_: Weights with small gradients have their effective
 learning rate increased whereas those with larger gradients have their effective learning rate decreased.
    - _Mitigation of vanishing/exploding gradients_: By normalizing the gradients
 based on a moving average of squared gradients, the update formula prevents
 the gradients from becoming too small or too large, which ensures a more
 stable training.

- **4. Adam**: **Ada**ptive **m**oment estimation (Adam), a popular optimizer that aims at combining the benefits brought by the Momentum and RMSProp methods.
  - _**Update formula**_:  $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha \frac{m_t}{\sqrt{v_t}+\epsilon}}$
    - $m_{t+1} \leftarrow \beta_1 m_t + (1- \beta_1) \nabla \mathscr{L} (\theta_t)$
    - $v_{t+1} \leftarrow \beta_2 v_t + (1- \beta_2) (\nabla \mathscr{L} (\theta_t))^2$
