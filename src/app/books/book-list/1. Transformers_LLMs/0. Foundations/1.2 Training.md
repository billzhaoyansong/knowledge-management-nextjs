# Training

## Parameter learning

- **Weight initialization**: Before starting model training, we need to initialize each parameter $\theta$ of the network to a certain value $\theta_0$ (this determines the starting optimization direction, influencing both the quality and speed of training convergence)
  - _**Xavier/Glorot**_ and _**He**_ initialization methods are commonly used, chosen based on the activation function used by the associated layer
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_initialization.png"/>
- **Epoch**:  a single iteration in which the model sees the entire training set once; parameters associated with epoch:
  - **_Training size $N$_**: Number of observations contained in the training set.
  - **_Batch size $b$_**: Number of observations processed by the model at each pass.
  - **_Number of steps $s$ per epoch_**: Number of iterations needed for the model to see the entire training set once
    - $\boxed{N=b \times s}$
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_epoch.png"/>
    - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_steps.png"/>
- **Loss function**: A loss function $\mathscr{L}$ is a function that evaluates how close model predictions $\hat{y}$ are to target values $y$.
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_loss.png"/>
  - Higher values denote worse performance with respect to target values, hence the wording "_loss_".
  - Sometimes write $\mathscr{L}(\theta)$ to indicate the dependency of the loss with respect to the model parameters $\theta$.
- **Classification label**: For a given class $i$, label $y_i$ is the ground truth that the model predicts via $y_i$.
  - _**Hard label**_: Each observation either belongs to class $i$ ($y_i = 1$) or it does not ($y_i = 0$) (can be used in image classification problems).
    - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_hard_label.png"/>
  - _**Soft label**_: Each observation belongs to class i with probability $y_i \in [0,1]$ (commonly used in the next word prediction task).
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_soft_label.png"/>
- **Backpropagation**: a model training procedure that aims at minimizing loss function $\mathscr{L}$ by adjusting model parameters $\theta$ through the calculation of gradients $\nabla\mathscr{L}(\theta)$.
- **Algorithm**
  - _**Forward step**_: Take a batch of training data and perform forward propagation to compute the model predictions $\hat{y}$ and deduce the loss $\mathscr{L}(\hat{y},y)$.
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_forward_step.png"/>
  - _**Backward step**_: Compute the gradient of the loss $\frac{\partial \mathscr{L} (\hat{y},y)}{\theta_i}$ parameter $\theta_i$ of the network.
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_backward_step.png"/>
  - _**Update step**_: Use the computed gradients to update each parameter $\theta_i$ of the network towards a direction that best decreases the loss $\mathscr{L}$
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_update_step.png"/>
- **Training stability**
  - _**exploding gradient**_ and _**vanishing gradient**_
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/exploding_gradient.png"/>
    - _**Gradient clipping**_: cap the gradient $\Vert \mathscr{L}\Vert$ with a ceiling $C$
      - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/gradient_clipping.png"/>
- **Practical tips**
  - _**Gradient checking**_: a sanity check method at the backward pass by comparing the analytical gradient value with the numerical gradient value
    - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/gradient_check.png"/>
  - _**Debugging a model by overfitting**_: the model tries to fit a mini-batch $\mathscr{B}$ $N$ times in a row. If it cannot make the loss minimal, then it means that we have a problem with the following possible culprits:
    - The model is _**too simple**_ to capture the underlying patterns in the data.
    - The loss function $\mathscr{L}$ is either incorrect or incorrectly implemented.
    - The backward pass is incorrectly implemented.
    - Hyperparameters such as learning rate have inappropriate values.
    - The model parameters $\theta$ have been initialized at bad values $\theta_0$
- **Learning Rate**: ($\alpha$ or $\eta$) controlling the pace at which weights $\theta$ get updated
  - <img style="width:75%;max-width:350px;" src="/books/Transformers_LLMs/training_learning_rate.png"/>
- **Warmup**: Use a lower learning rate α during the first steps $s$ to prevent model weights $\theta$ from being changed too dramatically due to noisy gradient estimates $\nabla \mathscr{L}$.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_warmup.png"/>
  - An example: $\boxed{\alpha(s) = C \min \left(\frac{1}{\sqrt{s}}, \frac{s}{s_w^{\frac{3}{2}}} \right)}$
    - $s_w$: the parameter of warmup steps
    - For small numbers of steps s, the learning rate increases linearly, which helps with the gradient
    - Starting from $s = s_w$, the learning rate decays, which mimics a traditional learning rate schedule

## Optimizers

- **Optimizer**: Given a model with parameters $\theta$, an _optimizer_ is a parameter update strategy aiming at finding optimal parameters $θ^*$ that minimize the loss.
  - In order to do that, it updates the parameters $\theta$ of the model at each iteration:
    1. _Forward pass_: Compute the loss $\mathscr{L}(\theta)$.
    2. _Backward pass_: Compute the gradient of the loss $\nabla\mathscr{L}(\theta)$ with respect to parameters $\theta$ and update parameters $\theta$ using the optimizer’s update rule.
  - Following are three ways to update:
    - <img style="width:75%;max-width:350px;" src="/books/Transformers_LLMs/training_update_weights.png"/>
  - The mini-batch method is widely used and optimized via the hyperparameter batch size $b$, typically set as a power of $2$ to leverage all available hardware optimization techniques such as memory alignment and parallel processing.
- **1. Gradient descent**: an optimizer defining an update rule of the model parameters $\theta$ based on the direction of the biggest loss decrease $-\nabla\mathscr{L}(\theta)$.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_gradient_descent.png"/>
  - _**Update formula**_: $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha \nabla \mathscr{L} (\theta_t)}$
    - $\alpha > 0$, typically $0.01$
- **2. Momentum**:  an optimizer that aims at accelerating the pace at which the model converges by dampening oscillations of the loss $\mathscr{L}$.
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_momentum.png"/>  
  - _**Update formula**_: $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha v_t}$
    - $v_{t+1} \leftarrow \beta v_t + (1- \beta) \nabla \mathscr{L} (\theta_t)$
      - $\alpha > 0$, typically $0.01$
      - $\beta \in [0,1]$ the "momentum", typically $0.9$
  - _**Advantages of Momentum**_
    - _Reduction of oscillations_: By smoothing out the parameter updates, momentum reduces the zigzagging effect, which leads to more stable and efficient convergence.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_momentum_adv1.png"/> 
    - _Overcoming local minima_:  By incorporating the previous gradients, momentum helps the optimizer maintain its direction and potentially escape shallow local minima, which improves the likelihood of reaching a global minimum.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_momentum_adv2.png"/> 
- **3. RMSProp**: **R**oot **M**ean **S**quared **Prop**agation (RMSProp) an optimizer that speeds up the learning process by controlling oscillations. It does so by taking into account the inertia from previous updates through the moving average of the squared loss gradients $(\nabla\mathscr{L}(\theta))^2$.
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_rmsprop.png"/> 
  - _**Update formula**_:  $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha \frac{\nabla \mathscr{L} (\theta_t)}{\sqrt{v_t}+\epsilon}}$
    - $v_{t+1} \leftarrow \beta v_t + (1- \beta) (\nabla \mathscr{L} (\theta_t))^2$
      - $\alpha > 0$, typically $0.01$
      - $\beta \in [0,1]$, the decay rate, typically $0.9$
      - $\epsilon \ll 1$, a numerical stability constant used to guard against divisions by zero.
  - _**Advantages of RMSProp**_
    - _Adaptive learning rates_: Weights with small gradients have their effective
 learning rate increased whereas those with larger gradients have their effective learning rate decreased.
    - _Mitigation of vanishing/exploding gradients_: By normalizing the gradients
 based on a moving average of squared gradients, the update formula prevents
 the gradients from becoming too small or too large, which ensures a more
 stable training.

- **4. Adam**: **Ada**ptive **m**oment estimation (Adam), a popular optimizer that aims at combining the benefits brought by the Momentum and RMSProp methods.
  - _**Update formula**_:  $\boxed{\theta_{t+1} \leftarrow \theta_t - \alpha \frac{m_t}{\sqrt{v_t}+\epsilon}}$
    - $m_{t+1} \leftarrow \beta_1 m_t + (1- \beta_1) \nabla \mathscr{L} (\theta_t)$
    - $v_{t+1} \leftarrow \beta_2 v_t + (1- \beta_2) (\nabla \mathscr{L} (\theta_t))^2$
