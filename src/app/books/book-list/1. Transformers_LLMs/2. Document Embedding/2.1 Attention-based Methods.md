# Attention-based Methods

- TLDR
  - LSTMs and GRUs try to solve the vanishing gradient problem by introducing gating mechanisms, but these methods are imperfect in practice

- **Idea**
  - to help with the prediction task, additional degrees of freedom is introduced by injecting a context, that is a direct function of past tokens
  - <img style="width:350px;" src="/books/Transformers_LLMs/attention-idea.png"/>
- **Attention-based weights**
  - For a given token at position $i$, we introduce the _**context**_ $c_i$ that contains past information that we want to inject
    - <img style="width:100px;" src="/books/Transformers_LLMs/attention-context.png"/>
    - $\boxed{c_i = \sum\limits_{j=1}^{T_x} \alpha_{i,j} h_j}$
      - $h_{j}$: hidden state of position $j$
      - $\alpha_{i,j}=\frac{\exp(e_{i,j})}{\sum_{j=1}^{T_x} \exp (e_{i,j}) }$: the amount of attention paid from position $i$ to past token of position $j \in [[1, T_x]]$
        - $e_{i,j}$: output of an alignment model $\mathcal{A}$, that is a function of the hidden state $h_{i-1}$ and the hidden state $h_j$