# Recurrent Neural Networks (RNN)

- TLDR:
  - RNNs and variants were a state-of-the-art architecture for NLP task
  - RNNs has the limitation of vanishing gradients
  - To deal with vanishing gradients, gate-based methods, such as GRU, LSTM, and ELMo, are leveraged to remember cell states

## RNN Basics

- **Architecture**
  - <img style="width:350px;" src="/books/Transformers_LLMs/RNN-Arch.png"/>
  - inside each timestamp $t$:
    - <img style="width:150px;" src="/books/Transformers_LLMs/RNN-t.png"/>
    - $\boxed{h^{(t)}=\mathcal{A}_1\left(W_{hh} h^{(t-1)} + W_{hx}x^{(t)} + b_h \right)}$
    - $\boxed{\hat{y}^{(t)}=\mathcal{A}_2 \left(W_{yh}h^{(t)} + b_y \right)}$
      - $W_{hh}, W_{hx}, W_{yh}, b_y, b_h$: coefficients
      - $\mathcal{A}_{1}, \mathcal{A}_{2}$: activation functions
- **Training**
  - _**Loss**_
    - $\boxed{\mathscr{L}(\hat{y},y) = \sum\limits_{t=1}^{T_y} \mathscr{L}(\hat{y}^{(t)},y^{(t)})}$
  - _**Temporal Backpropagation**_
    - $\boxed{\frac{\partial \mathscr{L}^{(T)}}{\partial W} = \sum\limits_{t=1}^{T} \frac{\partial \mathscr{L}^{(T)}}{\partial W} \mid_{(t)}}$
- **Applications**
  - tailor RNN by changing input length $T_x$ and the output length $T_y$
    - <img style="width:350px;" src="/books/Transformers_LLMs/RNN-Application1.png"/>
    - <img style="width:350px;" src="/books/Transformers_LLMs/RNN-Application2.png"/>
- **Limitation**
  - Difficulty in Capturing Long-Term Dependencies Due to Vanishing Gradient
    - RNNs have trouble capturing long-term dependencies because of the _**vanishing gradient problem**_, where gradients become exceedingly small during backpropagation through time, making it difficult for the network to learn and retain information from earlier timesteps.
- **Gate**
  - gates are used to mitigate the _vanishing gradient problem_
    - $\boxed{\Gamma_G \left(h^{(t-1)}, x^{(t)} \right) = \sigma\left( W_G \left[ h^{(t-1)}, x^{(t)} \right] + b_G \right)}$
      - $W_G$ is a matrix and $b_G$ is a bias term
      - $\left[ h^{(t-1)}, x^{(t)} \right]$: vector formed by the concatenation of the hidden state $h^{(t-1)}$ and the input $x^{(t)}$
      - $\sigma(\cdot) \in \left( 0,1\right)$: sigmoid function
  - types of gates:
    - <img style="width:350px;" src="/books/Transformers_LLMs/gates1.png"/>
    - <img style="width:350px;" src="/books/Transformers_LLMs/gates2.png"/>

## Gated Recurrent Unit (GRU)

- **Architecture**
  - GRU maintains a cell state $c(t)$ that lets the information flow on
top of the usual hidden state $h(t)$.
    - <img style="width:250px;" src="/books/Transformers_LLMs/GRU-arch.png"/>
    - $\tilde{c}^{(t)}$: cell candidate
    - $c^{(t)}$: cell state
    - $h^{(t)}$: hidden state
    - <img style="width:350px;" src="/books/Transformers_LLMs/GRU-detail.png"/>
    - The formulas shown above are _not unique_ as there are multiple GRU variants

## Long-Short Term Memory (LSTM)

- **Definition**
  - LSTM is a generalization of the GRU architecture that adds gates to help the model remember information from the past, and forget parts that it finds not to be relevant.
- **Architecture**
  - <img style="width:250px;" src="/books/Transformers_LLMs/LSTM-arch.png"/>
  - <img style="width:350px;" src="/books/Transformers_LLMs/LSTM-detail.png"/>
  - _Training is longer_ because of the resulting added complexity.

## Embeddings from Language Models (ELMo)

- **Definition**
  - ELMo uses bidirectional LSTMs that produces word embeddings that are _**context-aware**_, and relies on character-level information which makes it robust to out-of-vocabulary words
- **Architecture**
  - <img style="width:350px;" src="/books/Transformers_LLMs/ELMo-arch.png"/>
  - _**How it works**_:
    - _(1) Tokenize and represent input_: 
      - First, we divide the input into characters
        - <img style="width:350px;" src="/books/Transformers_LLMs/elmo-step1.png"/>
      - We then apply convolutions to have n-gram character representations for each word.
        - <img style="width:350px;" src="/books/Transformers_LLMs/elmo-step2.png"/>
    - _(2) Compute hidden representation_
      - _Left-to-right hidden state $\overrightarrow{h_{t,l}}$_: hidden state considering all the tokens up itself, i.e. from positions $1, ..., t − 1$.
        - <img style="width:350px;" src="/books/Transformers_LLMs/ELMo-L-to-R.png"/>
      - _Right-to-left hidden state_ $\overleftarrow{h_{t,l}}$: hidden state consider all the tokens after itself, i.e. from positions $n, n − 1, ..., t + 1$.
        - <img style="width:350px;" src="/books/Transformers_LLMs/ELMo-R-to-L.png"/>
    - _(3) Compute final embedding_
      - Concatenate both the left-to-right and right-to-left last layer hidden states of the desired token $\boxed{\hat{y}_t = [\overrightarrow{h_{t,L}}, \overleftarrow{h_{t,L}}]}$
- **Limitations**
  - Computationally Expensive: 
    - LSTM-based, slower than Transformer models (e.g., BERT).
  - Bidirectional but Not Deeply Contextual: 
    - Later models (e.g., BERT) use Transformers for richer context.
  - Fixed Vocabulary: 
    - Though it handles subwords via CNNs, rare words may still suffer.