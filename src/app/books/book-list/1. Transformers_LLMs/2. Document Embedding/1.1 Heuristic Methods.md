# Heuristic Methods

- TLDR: 
  - Document embedding is to encode the meaning of a sentence by leveraging
the information contained in each of its tokens.
  - Hueristic methods are rule-based, but have proven to work quite well
despite their simplicity.


## BOW â€“ Bag Of Words

- **Definition**
  - *BOW* is a method that generates embeddings at the document level by _**considering token frequency**_ and _**relying on one-hot encoding representations**_
    - <img style="width:250px;" src="/books/Transformers_LLMs/bow.png"/>
- **Steps**
  - (1) _**Construct vocabulary**_: All tokens in the document are included in the vocabulary $V$
    - <img style="width:350px;" src="/books/Transformers_LLMs/bow-step1.png"/>
  - (2) _**Count occurrence**_: For each word, we associate the number of times it appeared in the document.
    - <img style="width:350px;" src="/books/Transformers_LLMs/bow-step2.png"/>
  - (3) _**Build document vector**_: Each document is represented by a vector of dimension $|V|$, which is the sum of word frequencies associated with their one-hot encoding representations.
    - <img style="width:150px;" src="/books/Transformers_LLMs/bow-step3.png"/>
- _**limitation**_
  -  <details><summary>the order of tokens does not matter in the vector representation</summary><img style="width:350px;" src="/books/Transformers_LLMs/bow-limitation.png"/>
  </details>
- **$n$-gram**
  - The $n$-gram model is an extension of BOW that relies on _**the frequency of sequences of $n$ consecutive words**_
    - <img style="width:250px;" src="/books/Transformers_LLMs/n-gram.png"/>
  - *BOW* is a special case of _$n$-gram_, with $n = 1$

## TF-IDF

- **Definition**
  - *Term Frequency-Inverse Document Frequency (TF-IDF)* considers (1) the _**frequency of a word in a given document**_ and (2) _**its prevalence across all documents**_ to _**filter out overly common words**_ (e.g. "the", "is") 
    - <img style="width:250px;" src="/books/Transformers_LLMs/TF-IDF.png"/>
- **How-to**
  - _**(1) Term frequency**_: count the occurrence of a given word and normalize it by the count of all words appearing in the document.
    - <img style="width:250px;" src="/books/Transformers_LLMs/TF1.png"/>
    - $\boxed{\text{TF}(t,d) = \frac{f_{t,d}}{\sum\limits_{t' \in d} f_{t',d}}}$
      - $f_{t,d}$: number of occurence of term $t$ in a document $d$
  - _**(2) Inverse document frequency**_: consider the total number of documents along with the number of documents in which the term appears.
    - <img style="width:250px;" src="/books/Transformers_LLMs/IDF1.png"/>
    - <img style="width:250px;" src="/books/Transformers_LLMs/IDF2.png"/>
    - $\boxed{\text{IDF}(t,\mathscr{D}) = \log \left(\frac{N_{\mathscr{D}}}{N_{t,\mathscr{D}}} \right)}$
      - $N_{\mathscr{D}}$: number of total documents
      - $N_{\mathscr{t,D}}$: number of documents in which the term $t$ appears
  - _**(3) TF-IDF**_
    - $\boxed{\text{TF-IDF}(t,d,\mathscr{D}) = \text{TF}(t,d) \times \text{IDF}(t,\mathscr{D})}$