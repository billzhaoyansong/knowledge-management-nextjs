# Evaluation

## Data Splits

- **Training set** is used for the _**model $f$ to learn patterns**_ within
the features $x_{\text{train}}$ against given labels $y_{\text{train}}$.
- **Evaluation set**, also called the _validation set_ or _dev set_, is used to _**tune the model (e.g. hyperparameter, threshold)**_.
- **Test set** is only used to report results

## Metrics

- **Confusion matrix (CM)**: is used to have a granular view of the performance of a model in a _binary classification_ setting and consists of True Positive (TP), True Negative (TN), False Positive(FP), and False Negative (FN)
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_confusion_matrix.png"/>
- **Accuracy**: gives the overall performance
  - <img style="width:75%;max-width:200px;" src="/books/Transformers_LLMs/training_accuracy.png"/>
- **Precision** and **Recall**: are used when classes are imbalanced or of unequal importance
  - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_precision_recall.png"/>
- **$F_1$ score** is the harmonic mean of _precision_ and _recall_
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_f1.png"/>
  - a high $F_1$  can be achieved only when both precision and recall are high
- **Rate**: The normalized versions of TP, FN, TN, FP are also commonly used:
  - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_rates.png"/>
- **ROC**: Receiver Operating Characteristic curve plots FPR and TPR values for different binary classification thresholds, quantifying how well a given binary classifier is able to distinguish the two classes
  - <img style="width:75%;max-width:300px;" src="/books/Transformers_LLMs/training_roc.png"/>
  - Steps to construct ROC
    - (1) _Low threshold_: All observations are predicted to be in the positive class. We have $TPR = 1$ but also $FPR = 1$.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_roc_step1.png"/>
    - (2) _Medium threshold_: Observations with a score higher than the threshold $t$ are predicted to belong to the positive class, and the others are predicted to belong to the negative class. We have $0 < TPR < 1$ and $0 < FPR < 1$.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_roc_step2.png"/>
    - (3) _High threshold_: All observations are predicted to be in the negative class. We have $FPR = 0$ but also $TPR = 0$.
      - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_roc_step3.png"/>
- **AUC**: The Area Under the ROC Curve (AUC), quantifies the ability of the model to distinguish between the two classes that it is predicting
  - The AUC of a classification model is generally between $0.5$ and $1$, such that the higher the score, the better.
    - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_auc.png"/>
  - Typical ROC curves for different situations:
    - <img style="width:75%;max-width:400px;" src="/books/Transformers_LLMs/training_auc_scenarios.png"/>

## Bias-variance trade-off

- **Bias**: is the difference between the average prediction of the model and the true value it aims to predict.
  - <img style="width:75%;max-width:200px;" src="/books/Transformers_LLMs/training_bias.png"/>
- **Variance**: quantifies how much the model predictions would change if the underlying training data changed.
  - <img style="width:75%;max-width:200px;" src="/books/Transformers_LLMs/training_variance.png"/>
- **Trade-off**: The simpler the model, the higher the bias, and the more complex the
model, the higher the variance.
  - <img style="width:75%;max-width:500px;" src="/books/Transformers_LLMs/training_bias_variance_tradeoff.png"/>