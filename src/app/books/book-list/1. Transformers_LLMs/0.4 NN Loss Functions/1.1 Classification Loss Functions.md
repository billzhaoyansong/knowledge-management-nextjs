# Loss Functions - Classification

## Cross-entropy loss (CE loss)

- Intro
  - CE loss is also known as _logistic loss_ or _log loss_, used in classification tasks. It penalizes deviations of the predicted probability $\hat{y}$ from the actual class $y$.
  - _**Use**_ CE loss for standard classification tasks _**where your target is a fixed**_, "hard" label (e.g., this image is a cat [0, 1, 0]).
- **Binary class**: 
  - $\text{BCE}(\hat{y}, y) = -[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$
  - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_BCE.png"/>
- **Multi-class**: 
  - $\text{CE}(\hat{y}, y) = - \sum_{i=1}^k y_i \log(\hat{y_i})$

## KL divergence

- Intro
  - _**K**ullback-**L**eibler divergence_ is a measure that compares two probability distributions $P = (p_i)_{i \in [[1,n]]}$ and $Q = (q_i)_{i \in [[1,n]]}$
  - _**Use**_ KL Divergence for more complex tasks _**where your target is also a probability distribution**_ (e.g., you want your model's output to match the softmax distribution of the target).
  - <img style="width:75%;max-width:250px;" src="/books/Transformers_LLMs/training_KLDiv.png"/>
  - $\text{KL}(P \Vert Q) = \sum_{i=1}^n p_i \log\left( \frac{p_i}{q_i} \right)$
  - _Remark_: usually $\text{KL}(P \Vert Q) \neq \text{KL}(Q \Vert P)$
    - Example: Given $p_i, q_i$ probability distributions over values $i = 1, 2$, we note: $p_1 = p$ and $p_2 = 1 − p$; $q_1 = q$ and $q_2 = 1 − q$
      - <img style="width:75%;max-width:150px;" src="/books/Transformers_LLMs/training_KLDiv_Illus.png"/>
- Applications
  - KL divergence is used in natural language processing, image recognition, and dimensionality reduction techniques such as t-SNE.
