# Matrix Factorization

- Motivation
  - **Dimensionality reduction** simplifies complex datasets while preserving meaningful patterns.
    - Benefits include **computational efficiency** and **noise reduction**.
  - **Scenario**: Predicting user preferences for unseen items by identifying hidden features (e.g., latent genres in movies).
  - **Matrices**: Rectangular arrays representing user-item interactions.
  - **Factorization**: Decompose a matrix $R$ into latent factors:  
    - $$ R \approx U \cdot V^T $$
      - $U$: User-feature matrix (how users relate to hidden genres).
      - $V$: Item-feature matrix (how items relate to hidden genres).

---

- Constructing the Factorization Using SVD
  - **Singular Value Decomposition (SVD)** decomposes $R$ into:  
    - $$ R = U \Sigma V^T $$
      - $\Sigma$: Diagonal matrix of singular values.
      - **Truncated SVD**: Retain top-$k$ singular values to capture significant latent features.
  - **Challenges**: Handling missing values, incremental updates ("folding in" new users)

---

- Constructing the Factorization Using Funk SVD
  - **Funk SVD** (popularized during the Netflix Prize):
    - Minimize prediction error using gradient descent:  
      - $$ \min_{U,V} \sum_{(u,i)} (r_{ui} - \mathbf{u}_u \cdot \mathbf{v}_i^T)^2 + \lambda (\|U\|^2 + \|V\|^2) $$
    - **Regularization** ($\lambda$) prevents overfitting.
    - Supports **implicit feedback** (clicks, views) alongside explicit ratings.
  - Doing Recommendations with Funk SVD
    - Predict ratings:  
        $$
        \hat{r}_{ui} = \mathbf{u}_u \cdot \mathbf{v}_i^T + b_u + b_i + \mu
        $$
      - $b_u$: User bias, $b_i$: Item bias, $\mu$: Global average rating.
    - Rank items by predicted scores for personalized recommendations.