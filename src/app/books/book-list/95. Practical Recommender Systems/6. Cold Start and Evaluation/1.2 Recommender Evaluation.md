# Recommender Evaluation

- Offline Evaluation Metrics
  - **Rating Prediction**:  
    - **RMSE (Root Mean Squared Error)**:  
      - $ \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N (r_{ui} - \hat{r}_{ui})^2} $  
        - $ r_{ui} $: Actual rating.  
        - $ \hat{r}_{ui} $: Predicted rating.  
    - **MAE (Mean Absolute Error)**:  
      - $ \text{MAE} = \frac{1}{N} \sum_{i=1}^N |r_{ui} - \hat{r}_{ui}| $  

  - **Ranking Quality**:  
    - **Precision**: Fraction of relevant recommendations:  
      - $ \text{Precision} = \frac{\text{True Positives}}{\text{True Positives + False Positives}} $  
    - **Recall**: Fraction of relevant items captured:  
      - $ \text{Recall} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}} $  

  - **Diversity & Coverage**:  
    - **Diversity**: How varied recommendations are.  
    - **Coverage**: Fraction of catalog items recommended.  

- Online Evaluation
  - **A/B Testing**:  
    - Compare recommender variants (A: control, B: new algorithm) on live users.  
    - Metrics: Click-through rate (CTR), conversion rate, session duration.  

  - **Explore/Exploit Trade-off**:  
    - **Exploit**: Recommend known high-performing items.  
    - **Explore**: Recommend new items to gather data.  

- Implementation
  - **Offline Testing**:  
    - Split data into **training** (80%) and **test sets** (20%).  
    - Compute RMSE/MAE for rating predictions.  
  - **Online Testing**:  
    - Deploy A/B tests via the dashboard.  
    - Track user engagement metrics.
