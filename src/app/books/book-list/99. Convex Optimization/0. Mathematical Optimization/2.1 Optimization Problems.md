# Optimization Problems

<table class='table table-bordered'>
<tr>
<th>Problem</th><th>Least-Square Problems</th><th>Linear Programming Problems</th><th>Convex Optimization</th><th>Nonlinear Optimization</th>
</tr>

<tr>
<th>Mathematical Formulation</th>
<td>

$$\min \Vert Ax-b\Vert^2_2=\sum_{i=1}^k(a^T_ix-b)^2$$

</td>
<td>

$\begin{aligned}\min\space&c^Tx\\ \text{s.t.}\space&a_i^Tx\leq b_i,\quad i=1,...,m \end{aligned}$

</td>
<td>

$\begin{aligned}\min\space&f_0(x)\\ \text{s.t.}\space&f_i(x)\leq b_i,\quad i=1,...,m \end{aligned}$ 

where functions $f_0,...,f_m$ are convex, i.e. satisfy 

$$f_i(\alpha x+\beta y)\leq\alpha f_i(x)+\beta f_i(x)$$ 

$\forall x,y\in R^n$, and all $\alpha,\beta\in R$ with $\alpha+\beta=1$

</td>
<td></td>
</tr>

<tr>
<th>Solving</th>
<td>

* current algorithms can be very accurate and extremely reliable for moderate size of variables</li>
* solving LSP with large number of variables are achievable by exploiting the structure in matrix A</li>
* solving extremely large problems can be a challenge
* there is an analytical solution $x=(A^TA)^{-1}A^Tb$

</td>
<td>

* there are a variety of very effective methods for solving them
    * simplex method
    * interiorpoint methods
* if the problem is sparse or has some  other exploitable structure, we can often solve problems with large number of variables and constraints
* it is still a challenge to solve extremely large linear programs

</td>
<td>

* effective methods to solve it
  * interior-point methods (yet to claim it is a mature technology like solving least squares problems and linear programming problems)

</td>
<td>

no effective methods for solving the general nonlinear programming problem

* Local Optimization
  * only require differentiability of the objective and constraint functions
  * disadvantages
    * initial guess or starting point is critical
    * little information is provided about how far from (globally) optimal the local solution is
    * sensitive to algorithm parameter values
  * Using a local optimization method is trickier because it involves
    * experimenting with the choice of algorithm
    * adjusting algorithm parameters
    * and finding a good enough initial guess (when one instance is to be solved)
    * a method for producing a good enough initial guess
* Global Optimization
  * it compromises efficiency
  * Global optimization is used for problems with a small number of variables
  
</td>
</tr>
<tr>
<th>Using</th>
<td>

* Weighted least-squares
  $$\sum_{i=1}^k w_i(a^T_i x-b_i)^2, \forall w_i>0$$
* regularization
  * $\sum_{i=1}^k w_i(a^T_i x-b_i)^2+\rho\sum_{i=1}^n x_i^2$
    * $\rho>0$
    * the extra terms penalize large values of $x$
    * results becomes sensitive

</td>

<td>

* some problems can be transformed to an equivalent linear program with skills ready-to-use
  * Chebyshev approximation problem
    * $\min\max_{i=1,...,k}|a_i^Tx-b_i|$ is equivalent to $\begin{aligned}\min&\space t\\ \text{s.t.}&\space -t\bold{1}\leq Ax-b\leq t\bold{1}\end{aligned}$

</td>
<td>

* recognizing convex optimization problems and those that can be transformed to convex optimization problems can be **challenging**
* once convex problem is recognized and formulated, solving the problem is like least-squares or linear programming technology

</td>
<td>

Role of Convex Optimization in Nonconvex Problems
* initialization for local optimization
  * find an approximate but convex formulation of the problem, and solve it to obtain an exact solution as the starting point for a local optimization method.
* bounds for global optimization
  * **relaxation**: each nonconvex constraint is replaced with a looser but convex constraint
  * **Lagrangian relaxation**: a lower bound on optimal value of the nonconvex problem is provided by solving the Lagrangian dual problem.

</td>

</tr>
</table>
