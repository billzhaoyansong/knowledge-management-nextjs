# Introduction

- Prelogue
  - _Book: "Deep Reinforcement Learning Hands-On - Third Edition"_
  - This book introduced the implementations of RL algorithms using [Gymnasium library](https://github.com/Farama-Foundation/Gymnasium), which provides a wide range of environments yet without RL algorithms
- RL Frameworks
  - [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3?tab=readme-ov-file): ideal for beginners due to its excellent documentation, clean implementation, and easy-to-follow examples
  - [RLlib](https://docs.ray.io/en/master/rllib/index.html): strongest choice for production deployment due to its scalability, distributed training capabilities, and robust implementation

## RL formalisms

- RL entities - **agent** and **environment**
- communication channels — **actions**, **reward**, and **observations**
  - <img style="width:75%;max-width:500px;" src="/books/Reinforcement Learning Hands-On/RL Formalities.png" />
- **Reward**
  - a scalar value obtained every second, or every environment interation, or once-in-a-lifetime
- **Agent**
  - somebody/something who/that interacts with the environment by executing certain actions, making observations, and receiving eventual rewards for this
- **Environment**
  - everything outside of an agent
- **Actions** (Discrete/Continuous)
  - things an agent can do in the environment
- **Observations**
  - the second information channel for an agent (_the first being the reward_)
  - vs. _reward_:
    - reward is the main force that drives the agent’s learning process
  - vs. _state_:
    - state of an environment most of the time is _internal_

## Theoretical Foundations

- **1. Markov process** (MP) / Markov chain
  - an _observe-only_ system with the _**markov property**_: future system dynamics from any state have to depend on this state only
    - <details><summary><i>stationary</i> implication of markov property</summary>

      - underlying transition distribution for any state does not change over time
      - Non-stationarity means that there is some hidden factor that influences our system dynamics, and this factor is not included in observations. However, this contradicts the Markov property, which requires the underlying probability distribution to be the same for the same state regardless of the transition history.
  - **_state space_** ($S$): the set of all finite states
  - _**episode**_: a complete observations
  - _**transition matrix**_: a $N \times N$ square matrix capturing transition probabilities
  - <details><summary><i>MP Examples & Visualization</i></summary>

    - **Naïve Weather System**
      - <img style="width:75%;max-width:500px;" src="/books/Reinforcement Learning Hands-On/MP Weather.png" />
      - 
       |       | Sunny | Rainy |
       | ----- | ----- | ----- |
       | Sunny | 0.8   | 0.2   |
       | Rainy | 0.1   | 0.9   |
    - **Office Worker**
      - <img style="width:75%;max-width:500px;" src="/books/Reinforcement Learning Hands-On/MP Worker.png" />
      -
        |          | Home | Coffee | Chat | Computer |
        | -------- | ---- | ------ | ---- | -------- |
        | Home     | 0.6  | 0.4    | 0    | 0        |
        | Coffee   | 0    | 0.1    | 0.7  | 0.2      |
        | Chat     | 0    | 0.2    | 0.5  | 0.3      |
        | Computer | 0.2  | 0.2    | 0.1  | 0.5      |
      - the transition matrix can be estimated from our observations/episodes
    </details>
- **2. Markov reward process** (MRP)
  - _**return**_: sum of subsequent rewards for an episode
    - $G_t=R_{t+1} + \gamma R_{t+2} + \cdots =\sum\limits_{k=0}^\infty \gamma^k R_{t+k+1}$
    - $\gamma$ limits the horizon we calculate values for
      - <details><summary><i>Implications</i></summary>

        - $\gamma = 1$ means all rewards are equally important (which is not practical), leading to an infinitely large return value
        - $\gamma = 0$ means only next reward is important and the rest are negligible
      </details>
    - challenge: return quantity can vary widely as it was defined for every specific episode
  - _**value of the state (a.k.a state-value)**_: expected/average return, more practical than _return_
    - $V(s) = \mathbb{E} \left[G \mid S_t=s\right]$
    - <details><summary><i>Example</i></summary>

      - <img style="width:75%;max-width:300px;" src="/books/Reinforcement Learning Hands-On/MRP Worker.png" />
      - with a simple case: $\gamma = 0$
        - $V (\text{chat}) = − 1 ⋅ 0.5 + 2 ⋅ 0.3 + 1 ⋅ 0.2 = 0.3$
        - $V (\text{coffee}) = 2 ⋅ 0.7 + 1 ⋅ 0.1 + 3 ⋅ 0.2 = 2.1$
        - $V (\text{home}) = 1 ⋅ 0.6 + 1 ⋅ 0.4 = 1.0$
        - $V (\text{computer}) = 5 ⋅ 0.5 + (−3) ⋅ 0.1 + 1 ⋅ 0.2 + 2 ⋅ 0.2 = 2.8$
    </details>
- **3. Markov decision process** (MDP)
  - by choosing an action from  _**action space**_ ($A$), the agent can affect the probabilities of the target states
  - _**episode**_: in MDP, each episode contains observations, actions, and rewards
    - <img style="display:inline-block;width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/MDP episode.png" />
      - Every cell represents the agent’s step in the episode
  - conditioning transition matrix with actions turns into a cuboid of shape $|S| \times |S| \times |A|$
    - _source state_ as the height ($i$)
    - _target state_ as the width ($j$)
    - _action_ as the depth ($k$) of the transition table
    - <details><summary><i>visualization</i></summary>

      - <img style="display:inline-block;width:75%;max-width:300px;" src="/books/Reinforcement Learning Hands-On/MDP 2D.png" /> <img style="display:inline-block;width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/MDP 3D.png" />

    </details>
  - <details><summary><i>Example: imperfect robot that lives in a 3×3 grid</i></summary>

    - actions: _turn left_, _turn right_, and _go forward_
    - states: robot's position (3x3) plus orientation (up, down, left, and right) => $3 \times 3 \times 4 = 36$ states
    - transition probabilities: $90\%$ works, $10\%$ no change
    - <img style="display:inline-block;width:75%;max-width:500px;" src="/books/Reinforcement Learning Hands-On/MDP grid world.png" />
  </details>
- **4. Policy**
  - probability distribution over actions for every possible state:
    - $\pi\left( a \mid s \right) = P \left[ A_t = a \mid S_t = s \right]$
    - policy defined as probability ~~instead of a concrete action~~ introduces randomness into an agent’s behavior
  - if our policy is fixed (when the policy returns the same actions for the same states), then our MDP becomes a MRP, the transition and reward matrices can be reduced with a policy’s probabilities and get rid of the action dimensions.

## Taxonomy of RL Methods

- Model-free or model-based
  - model-free methods connect observations/values to actions, ~~without building a model of the environment or reward~~
    - model-free methods are usually easier to train
    - all of the methods in this book are from the model-free category
  - model-based methods make predictions on the next observation and/or reward, based on which the agent chooses the best possible action
    - pure model-based methods are used in deterministic environments, such as board games with strict rules
- Value-based or policy-based
  - **policy-based methods** approximate the **policy with a probability distribution** over the available actions.
    - <img style="display:inline-block;width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/policy-based.png" />
  - **value-based methods** calculates the **value of every possible action** and chooses the action with the best value
- On-policy or off-policy
  - **off-policy methods** have the ability to learn from **historical data**
  - **on-policy methods** require **fresh data** for training, generated from the policy we’re currently updating