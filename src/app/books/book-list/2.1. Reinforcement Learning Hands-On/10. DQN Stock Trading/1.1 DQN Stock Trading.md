# DQN Stock Trading

## Problem Statement

- **Observation**:
  - _**$N$ past bars**_, where each has `open`, `high`, `low`, and `close` prices
    - `high`, `low`, `close` prices are represented as a percentage of the open price, to help the system find repeating patterns in the price level
  - _**An indicator**_, about whether the share was bought some time ago (only one share at a time will be possible)
  - _**Profit/loss**_, that we currently have from our current position (the share bought)
- **Action** (after every minute's bar):
  - _**Do nothing**_: Skip the bar without taking an action
  - _**Buy a share**_: If the agent has already got the share, nothing will be bought; otherwise, we will pay the commission, which is usually some small percentage of the current price
  - _**Close the position**_: If we do not have a previously purchased share, nothing will happen; otherwise, we will pay the commission for the trade
- **Reward**
  - As the first option, we can split the reward into multiple steps during our ownership of the share. In that case, the reward on every step will be equal to the last barâ€™s movement.
  - Alternatively, the agent can receive the reward only after the close action and get the full reward at once.

## The Trading Environment

- <details><summary>Class Diagram</summary>

    ```mermaid
    ---
    config:
        look: neo
        layout: elk
    ---
    classDiagram
    direction LR
    
        class Actions {
            <<enum>>
            Skip = 0
            Buy = 1
            Close = 2
        }

        class State {
            - int bars_count
            - float commission_perc
            - bool reset_on_close
            - bool reward_on_close
            - bool volumes
            - bool have_position
            - float open_price
            - _prices : data.Prices
            - _offset
            + reset(prices: data.Prices, offset: int)
            + encode() np.ndarray
            + step(action: Actions) Tuple[float, bool]
            + _cur_close() float
            + shape : Tuple[int, ...]
        }

        class State1D {
            + encode() np.ndarray
            + shape : Tuple[int, ...]
        }

        class StocksEnv {
            - _prices: Dict[str, data.Prices]
            - _state: State
            - action_space
            - observation_space
            - random_ofs_on_reset
            - _instrument
            + reset()
            + step(action_idx: int) Tuple[np.ndarray, float, bool, bool, dict]
            + from_dir(data_dir: str, **kwargs)
        }

        Actions <.. StocksEnv : uses
        Actions <.. State : uses
        State <|-- State1D
        State1D <.. StocksEnv : uses
        State <.. StocksEnv : uses
        StocksEnv -- "1" State
        StocksEnv -- "1" State1D

        State -- "1" data.Prices

        %% Note: data.Prices and gym.Env are external dependencies.
        class data.Prices {
            <<external>>
        }
        class gym.Env {
            <<external>>
        }

        StocksEnv --|> gym.Env
    ```

  </details>