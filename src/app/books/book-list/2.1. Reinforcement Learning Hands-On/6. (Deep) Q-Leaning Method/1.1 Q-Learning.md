# Q-Learning

- Motivation:
  - The Value Iteration Method is _model-based_, requiring to know (1) transition probability, (2) reward matrix, and (3) state-value / action-value (q-value)
  - **Q-Learning Method is model-free**, only need to know (1) q-value

## Tabular Q-Learning

- model-free, off-policy, value-based
- <img style="width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/Tabular Q-Learning.jpg" />
- **Steps**
  - _step 1_: Start with an empty table for $Q(s,a)$
  - _step 2_: Obtain $(s, a, r, s')$ from the environment.
  - _step 3_: Make a Bellman update $Q(s,a) \leftarrow (1- \alpha)Q(s,a) + \alpha \left(r + \gamma \max_{a' \in A} Q(s',a')\right)$
    - <details><summary><i>Why a new Bellman update formula?</i></summary>

      - _**directly assigning**_ new values to replace existing values would _**make training unstable**_
      - this new update formula approximates $Q(s,a)$ using a ‚Äúblending‚Äù technique, _**allowing**_ values of $Q$ to converge smoothly

    </details>
  - _step 4_: Play episode and check convergence conditions. If not met, repeat from _step 2_.
- Implementations
  - <details><summary>Agent</summary>

    ```python
    class Agent:
    
        def __init__(self, env: gym.Env, gamma: float = 0.9, alpha: float = 0.2):
            self.env = env
            self.q_table = defaultdict(float)
            obs, _ = self.env.reset()
            self.obs = obs
            self.gamma = gamma
            self.alpha = alpha
            
        def sample_env(self):
            old_obs = self.obs
            act = self.env.action_space.sample()
            new_obs, reward, stop, truc, _ = self.env.step(act)
            if stop or truc:
                self.obs, _ = self.env.reset()
            else:
                self.obs = new_obs
            return old_obs, act, new_obs, reward
        
        def _best_value_and_action(self, obs):
            best_value, best_action = 0., None
            for act in range(self.env.action_space.n):
                act_value = self.q_table[(obs, act)]
                if best_action is None or best_value < act_value:
                    best_value = act_value
                    best_action = act        
            return best_value, best_action
        
        def value_update(self, old_obs, act, new_obs, reward):
            new_obs_best_val, _ = self._best_value_and_action(new_obs)
            obs_new_val = reward + self.gamma * new_obs_best_val
            obs_old_val = self.q_table[(old_obs, act)]
            self.q_table[(old_obs, act)] = (1 - self.alpha) * obs_old_val + self.alpha * obs_new_val
        
        def play_episode(self, play_env: gym.Env):
            obs, _ = play_env.reset()
            total_reward = 0.0
            while True:
                best_value, best_action = self._best_value_and_action(obs)
                new_obs, reward, stop, truc, _ = play_env.step(best_action)
                total_reward += reward
                if stop or truc:
                    play_env.reset()
                    break
                
                obs = new_obs
            return total_reward
    ```

    </details>
  - <details><summary>Training & Evaluation</summary>

    ```python
    env = gym.make('FrozenLake-v1')
    play_env = gym.make('FrozenLake-v1')
    
    agent = Agent(env)
    
    best_reward, iter_no = 0., 0
    
    while True:
        iter_no += 1
        
        # print("Iter NO: %d" % (iter_no))
        
        old_obs, act, new_obs, reward = agent.sample_env()
        agent.value_update(old_obs, act, new_obs, reward)
        
        test_reward = 0.
        for _ in range(TEST_EPISODES):
            test_reward += agent.play_episode(play_env)
        test_reward /= TEST_EPISODES
        
        if test_reward > best_reward:
            print("%d: Best test reward updated %.3f -> %.3f" % (iter_no, best_reward, test_reward)) 
            best_reward = test_reward
        if test_reward > 0.80: 
            print("Solved in %d iterations!" % iter_no) 
            break 
    ```

    </details>

## Deep Q-Learning

- model-free, off-policy, value-based
- **Challenges & Solutions**
  - <details><summary>1: address <i><b>efficient sampling</b></i> using <i><b>episilon-greedy method</b></i></summary>

    - address exploration vs. exploitation dilemma
    - episolon-greedy: starting with $\epsilon = 1.0$ (100% random actions) and slowly decrease it to some small value, such as 5% or 2% random actions

    </details>


  - <details><summary>2: address <i><b>non-IID</b></i> using <i><b>replay buffer</b></i></summary>

    - why non-iid
      - samples won't be independent as they belong to the same episode
      - distribution of training data will be a result of some policy (random, episolon-greedy), but desirably the distribution of training data should fit the optimal policy
    - replay buffer
      - use a large fixed and fresh buffer of our past experience and sample training data from it, allowing training data more-or-less independent
      - _prioritized replay buffer_: more sophisticated sampling approach

  </details>

  - <details><summary>3: address <i><b>unstability from correlation</b></i> between steps using <i><b>target network</b></i></summary>

    - what is the challenge
      - When we perform an update of our NNs‚Äô parameters to make $Q(s,a)$ closer to the desired result, we can indirectly alter the value produced for $Q(s',a')$ and other states nearby, making training very unstable
    - target network
      - keep a copy of our network and use it for the $Q(s',a')$ value in the Bellman equation. This network is synchronized with our main network only periodically

  </details>

- <img style="width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/Deep Q-Learning.jpg" />
- **Steps**
  - step 1: Initialize parameters for $Q(s,a)$ and $\hat{Q}(s,a)$ with random weights, $ùúñ ‚Üê 1.0$, and empty the _**replay buffer**_.
  - step 2: With probability $ùúñ$, select a random action $a$; otherwise, $a = \argmax_a Q(s,a)$.
  - step 3: Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.
  - step 4: Store the transition $(s, a, r, s')$ in the replay buffer.
  - step 5: Sample a random mini-batch of transitions from the replay buffer.
  - step 6: For every transition in the buffer, calculate the target:
    - $y = r$ if the episode has ended
    - $y = r + \gamma \max_{a' \in A} \hat{Q}(s', a')$, otherwise
  - step 7: Calculate the loss $\mathscr{L} = (Q(s,a) - y)^2$
  - step 8: Update $Q(s,a)$ using the SGD algorithm by minimizing the loss in respect to the model parameters.
  - step 9: Every $N$ steps, copy weights from $Q$ to $QÃÇ$.
  - step 10: Repeat from step 2 until converged.
- **Debugging**
  - <details><summary>Constantly Low Reward</summary>

    - **_What it looks like_**: A graph showing a consistently low, flat reward curve
    - **_What it means_**: policy is stuck at a simple, suboptimal strategy, and isn't discovering the actions that lead to higher rewards
    - **_Possible causes_**: Not enough exploration
    - **_Solution_**: Increase exploration rate (e.g. `exploration_fraction = 0.3`)

  </details>

  - <details><summary>Training Instability</summary>

    - **_What it looks like_**: A graph showing the loss fluctuating wildly
    - **_What it means_**: The model is unstable and not learning effectively
    - Possible causes 1: **_High learning rate_**:
      - Solution: Decrease learning rate (adjustment: `learning_rate = 0.0001`)
    - Possible causes 2: **_Correlated transitions_**:
      - Solution: Add more noise to the transitions (adjustment: `noise_scale = 0.1`)
    - Possible causes 3: **_Target network updated too frequently_**:
      - Solution: Decrease target network update frequency (adjustment: `target_update_interval = 1000`)
      - Reason: Provides a stable target for the Q-values to converge towards, preventing training instability
    - Possible causes 4: **_Replay buffer size too small_**:
      - Solution: Increase replay buffer size (adjustment: `buffer_size = 100_000`)
      - Reason: Provides more diverse samples for training, reducing correlation between transitions

  </details>

## CartPole

- <details><summary>Implementation with Stable Baselines3</summary>

    ```python
    import gymnasium as gym
    from stable_baselines3 import DQN
    from stable_baselines3.common.evaluation import evaluate_policy
    from stable_baselines3.common.callbacks import EvalCallback
    import os

    # Create directories to save logs and models
    log_dir = "logs/"
    model_dir = "models/"
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(model_dir, exist_ok=True)


    # --- 1. Create the Environments ---
    # It's best practice to have separate environments for training and evaluation.
    # DO NOT render the training environment. It slows everything down massively.
    train_env = gym.make("CartPole-v1")

    # The EvalCallback will use this environment to evaluate the agent periodically
    eval_env = gym.make("CartPole-v1")


    # --- 2. Setup a Callback to Save the Best Model ---
    # This callback will evaluate the agent every `eval_freq` steps and save the
    # model if it achieves a new best average reward.
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=os.path.join(model_dir, 'best_model'),
        log_path=log_dir,
        eval_freq=1000, # Evaluate every 1000 steps
        deterministic=True,
        render=False
    )


    # --- 3. Initialize the DQN Agent with Better Hyperparameters ---
    model = DQN(
        "MlpPolicy",
        train_env,
        learning_rate=1e-4,             # Slower learning rate for stability
        buffer_size=100000,             # Much larger replay buffer
        learning_starts=5000,           # Start learning after 5000 steps
        batch_size=64,
        tau=1.0,                        
        gamma=0.99,
        train_freq=4,
        gradient_steps=1,
        target_update_interval=2500,    # Much less frequent target network updates
        exploration_fraction=0.3,       # Explore for a longer duration (30% of total steps)
        exploration_initial_eps=1.0,
        exploration_final_eps=0.05,
        verbose=1
    )

    # --- 4. Train the Agent ---
    # We pass the callback to the learn method
    model.learn(total_timesteps=50000, progress_bar=True, log_interval=10, callback=eval_callback)


    # --- 5. Evaluate the Trained Agent ---
    # The best model was saved by the callback, so we load it now.
    # Note: If you did not use a callback, you would just use the `model` object directly.
    best_model = DQN.load(os.path.join(model_dir, 'best_model', "best_model.zip"))

    # Use a separate, new environment for the final evaluation
    eval_env_final = gym.make("CartPole-v1")
    mean_reward, std_reward = evaluate_policy(best_model, eval_env_final, n_eval_episodes=20)
    print(f"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}")


    # --- 6. Test and Render the Trained Agent ---
    # Create a new environment with human rendering to see the agent in action.
    test_env = gym.make("CartPole-v1", render_mode="human")

    obs, info = test_env.reset()
    for _ in range(1000):
        action, _states = best_model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = test_env.step(action)
        test_env.render()
        # The new gymnasium API returns two done flags: terminated and truncated
        if terminated or truncated:
            obs, info = test_env.reset()

    test_env.close()
    ```

  </details>

## Pong

- <img style="width:75%;max-width:200px;" src="/books/Reinforcement Learning Hands-On/pong.png" />
- Challenge in Pong (Atari game)
  - address _**partially observable MDPs (POMDPs)**_ by **using $k$ subsequent frames** together as the observation
    - <details><summary><i>POMDPs</i></summary>

      - in Atari game, one single image is not enough to capture all the important information
      - A POMDP is basically an MDP without the Markov property
        - e.g. card games in which you don‚Äôt see your opponents‚Äô cards

      </details>
