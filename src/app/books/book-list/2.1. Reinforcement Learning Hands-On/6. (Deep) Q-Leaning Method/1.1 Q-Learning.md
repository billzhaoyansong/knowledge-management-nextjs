# Q-Learning

- Motivation:
  - The Value Iteration Method is _model-based_, requiring to know (1) transition probability, (2) reward matrix, and (3) state-value / action-value (q-value)
  - **Q-Learning Method is model-free**, only need to know (1) q-value

## Tabular Q-Learning

- **Steps**
  - _step 1_: Start with an empty table for $Q(s,a)$
  - _step 2_: Obtain $(s, a, r, s')$ from the environment.
  - _step 3_: Make a Bellman update $Q(s,a) \leftarrow (1- \alpha)Q(s,a) + \alpha \left(r + \gamma \max_{a' \in A} Q(s',a')\right)$
    - <details><summary><i>Why a new Bellman update formula?</i></summary>

      - _**directly assigning**_ new values to replace existing values would _**make training unstable**_
      - this new update formula approximates $Q(s,a)$ using a ‚Äúblending‚Äù technique, _**allowing**_ values of $Q$ to converge smoothly

    </details>
  - _step 4_: Play episode and check convergence conditions. If not met, repeat from _step 2_.
- Implementations
  - <details><summary>Agent</summary>

    ```python
    class Agent:
    
        def __init__(self, env: gym.Env, gamma: float = 0.9, alpha: float = 0.2):
            self.env = env
            self.q_table = defaultdict(float)
            obs, _ = self.env.reset()
            self.obs = obs
            self.gamma = gamma
            self.alpha = alpha
            
        def sample_env(self):
            old_obs = self.obs
            act = self.env.action_space.sample()
            new_obs, reward, stop, truc, _ = self.env.step(act)
            if stop or truc:
                self.obs, _ = self.env.reset()
            else:
                self.obs = new_obs
            return old_obs, act, new_obs, reward
        
        def _best_value_and_action(self, obs):
            best_value, best_action = 0., None
            for act in range(self.env.action_space.n):
                act_value = self.q_table[(obs, act)]
                if best_action is None or best_value < act_value:
                    best_value = act_value
                    best_action = act        
            return best_value, best_action
        
        def value_update(self, old_obs, act, new_obs, reward):
            new_obs_best_val, _ = self._best_value_and_action(new_obs)
            obs_new_val = reward + self.gamma * new_obs_best_val
            obs_old_val = self.q_table[(old_obs, act)]
            self.q_table[(old_obs, act)] = (1 - self.alpha) * obs_old_val + self.alpha * obs_new_val
        
        def play_episode(self, play_env: gym.Env):
            obs, _ = play_env.reset()
            total_reward = 0.0
            while True:
                best_value, best_action = self._best_value_and_action(obs)
                new_obs, reward, stop, truc, _ = play_env.step(best_action)
                total_reward += reward
                if stop or truc:
                    play_env.reset()
                    break
                
                obs = new_obs
            return total_reward
    ```

    </details>
  - <details><summary>Training & Evaluation</summary>

    ```python
    env = gym.make('FrozenLake-v1')
    play_env = gym.make('FrozenLake-v1')
    
    agent = Agent(env)
    
    best_reward, iter_no = 0., 0
    
    while True:
        iter_no += 1
        
        # print("Iter NO: %d" % (iter_no))
        
        old_obs, act, new_obs, reward = agent.sample_env()
        agent.value_update(old_obs, act, new_obs, reward)
        
        test_reward = 0.
        for _ in range(TEST_EPISODES):
            test_reward += agent.play_episode(play_env)
        test_reward /= TEST_EPISODES
        
        if test_reward > best_reward:
            print("%d: Best test reward updated %.3f -> %.3f" % (iter_no, best_reward, test_reward)) 
            best_reward = test_reward
        if test_reward > 0.80: 
            print("Solved in %d iterations!" % iter_no) 
            break 
    ```

    </details>

## Deep Q-Learning

- **Challenges & Solutions**
  - <details><summary>1: address <i><b>efficient sampling</b></i> using <i><b>episilon-greedy method</b></i></summary>

    - address exploration vs. exploitation dilemma
    - episolon-greedy: starting with $\epsilon = 1.0$ (100% random actions) and slowly decrease it to some small value, such as 5% or 2% random actions

    </details>


  - <details><summary>2: address <i><b>non-IID</b></i> using <i><b>replay buffer</b></i></summary>

    - why non-iid
      - samples won't be independent as they belong to the same episode
      - distribution of training data will be a result of some policy (random, episolon-greedy), but desirably the distribution of training data should fit the optimal policy
    - replay buffer
      - use a large fixed and fresh buffer of our past experience and sample training data from it, allowing training data more-or-less independent
      - _prioritized replay buffer_: more sophisticated sampling approach

  </details>

  - <details><summary>3: address <i><b>unstability from correlation</b></i> between steps using <i><b>target network</b></i></summary>

    - what is the challenge
      - When we perform an update of our NNs‚Äô parameters to make $Q(s,a)$ closer to the desired result, we can indirectly alter the value produced for $Q(s',a')$ and other states nearby, making training very unstable
    - target network
      - keep a copy of our network and use it for the $Q(s',a')$ value in the Bellman equation. This network is synchronized with our main network only periodically

  </details>

- **Steps**
  - step 1: Initialize parameters for $Q(s,a)$ and $\hat{Q}(s,a)$ with random weights, $ùúñ ‚Üê 1.0$, and empty the _**replay buffer**_.
  - step 2: With probability $ùúñ$, select a random action $a$; otherwise, $a = \argmax_a Q(s,a)$.
  - step 3: Execute action $a$ in an emulator and observe the reward, $r$, and the next state, $s'$.
  - step 4: Store the transition $(s, a, r, s')$ in the replay buffer.
  - step 5: Sample a random mini-batch of transitions from the replay buffer.
  - step 6: For every transition in the buffer, calculate the target:
    - $y = r$ if the episode has ended
    - $y = r + \gamma \max_{a' \in A} \hat{Q}(s', a')$, otherwise
  - step 7: Calculate the loss $\mathscr{L} = (Q(s,a) - y)^2$
  - step 8: Update $Q(s,a)$ using the SGD algorithm by minimizing the loss in respect to the model parameters.
  - step 9: Every $N$ steps, copy weights from $Q$ to $QÃÇ$.
  - step 10: Repeat from step 2 until converged.