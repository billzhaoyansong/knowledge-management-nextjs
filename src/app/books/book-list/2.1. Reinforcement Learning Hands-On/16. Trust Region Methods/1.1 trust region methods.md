# Trust Region Methods

## Intro

- Motivation
  - _**balance the stability and training efficiency**_ of policy updates by adjusting the step size of the policy update:
    - on the one hand, too large step size may lead to unstable policy updates
    - on the other hand, too small step size may lead to slow convergence
- How to adjust the step size?
  - constrain the steps by _**checking the KL divergence between the new policy and the old policy**_

## Proximal Policy Optimization (PPO)

- on-policy, model-free, policy-based
- Leveraging _**Actor-Critic**_ architecture as A2C:
  - _**Actor (Policy Network)**_: Outputs action probabilities (or a continuous action distribution).
  - _**Critic (Value Network)**_: Estimates the value function $V(s)$ to compute advantages $A(s,a)$
- Differences from A2C:
  - **Policy gradient (Key Difference)**:
    - $J_{A2C}(\theta) = \mathbb{E}_{t}[\nabla_\theta \log \pi_\theta(a_t|s_t) A(s_t,a_t)]$
      - this update may lead to _a very large update_ to the policy weights by blindly maximizing the value
    - $J^{clip}_{PPO}(\theta) = \mathbb{E}_{t}[\min(r_t(\theta)A_t,clip(r_t(\theta),1-\epsilon,1+\epsilon)A_t)]$
      - $clip(x,a,b) = \max(a, \min(x,b))$
      - $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$
      - the objective limits the ratio of the new policy to the old policy to be within $[1-\epsilon,1+\epsilon]$
  - Advantage function:
    - (1) Monte Carlo (MC) Returns
      - $A_t = \sum_{k=0}^{T} \gamma^k r_{t+k} - V(s_t)$
      - Pros: Unbiased but high variance.
    - (2) Temporal Difference (TD) Residuals
      - $A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$
      - Pros: Biased but low variance.
    - (3) Generalized Advantage Estimation (GAE)
      - $A_t = \sum_{i=t}^{\infty} (\gamma \lambda)^{i-t} \delta_{t+i}$
        - $\delta_{t} = r_{t} + \gamma V(s_{t+1}) - V(s_{t})$
      - Balances bias/variance with a parameter $\lambda$:
        - $\lambda = 0.95$ commonly

## Soft Actor-Critic (SAC)

- model-free, policy-based, off-policy
- Differences from DDPG:
  - (1) **Built-in exploration via entropy regularization**
    - Goal for standard RL algorithms: maximize the expected reward
      - $J(\theta) = \mathbb{E}_{r \sim \pi}\left[\sum_{t=0}^{T} \gamma^t r(s_t,a_t)\right]$
    - _**Goal for SAC**_: maximize the expected reward and entropy
      - $J(\theta) = \mathbb{E}_{r \sim \pi}\left[\sum_{t=0}^{T} \gamma^t r(s_t,a_t) + \alpha \mathcal{H}(\pi_\theta(\cdot|s_t))\right]$
        - $\alpha$: the entropy regularization parameter
        - $\mathcal{H}(\pi_\theta(\cdot|s_t))$: the entropy of the policy
  - (2) Architecture
    - **Two critic networks $Q_1(s,a)$ and $Q_2(s,a)$ and their target networks $Q'_1(s,a)$ and $Q'_2(s,a)$**
      - based on sampled minibatch $(s,a,r,s')$
        1. sample the next action $a'$ from the policy $\pi_\theta(\cdot|s')$ and its log probability $\log \pi_\theta(a'|s')$
        2. get the target Q-values $Q'_1(s',a')$ and $Q'_2(s',a')$
        3. the the full target $y= r + \gamma \min(Q'_1(s',a'),Q'_2(s',a') - \alpha \log \pi_\theta(a'|s'))$
        4. compute the loss for each critic network
             - $Loss_{Q1} = MSE(Q_1(s,a) , y) = (y - Q_1(s,a))^2$
             - $Loss_{Q2} = MSE(Q_2(s,a) , y) = (y - Q_2(s,a))^2$
        5. update the critic networks
             - $Q_1 \leftarrow Q_1 - \eta \nabla_{Q_1} Loss_{Q1}$
             - $Q_2 \leftarrow Q_2 - \eta \nabla_{Q_2} Loss_{Q2}$
    - **Actor network**
      - based on sampled minibatch $(s,a,r,s')$
        1. sample the next action $a'$ from the policy $\pi_\theta(\cdot|s')$ and its log probability $\log \pi_\theta(a'|s')$
        2. get the min Q-values $\min(Q_1(s',a'),Q_2(s',a'))$
        3. compute the loss for the actor network
           - $Loss_{Policy} = (\alpha \log \pi_\theta(a'|s') - \min(Q_1(s',a'),Q_2(s',a')))$
        4. update the actor network
           - $\theta \leftarrow \theta - \eta \nabla_\theta Loss_{Policy}$
