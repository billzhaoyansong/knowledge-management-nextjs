# REINFORCE Method

## Policy-Based Methods Intro

- Why Policy-Based Methods over Q-Learning?
  - Q-Learning **cannot handle continuous actions** (recall that bellman equation for Q-learning $Q(s,a) = r + \gamma \max_{a'} Q(s',a')$)
- Policy Gradient
  - $\nabla_{\theta} J(\theta) = \sum_{s \in S} \sum_{a \in A} \nabla_{\theta} \pi_{\theta}(a|s) \cdot Q^{\pi}(s,a)$
    - This gradient defines the direction to change our network’s parameters to improve the policy in terms of the accumulated total reward.
- Policy-Based vs Value-Based
  | Aspect            | Policy-Based Methods                                           | Value-Based Methods                                                            |
  | ----------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------ |
  | What they learn   | Directly learn the policy $\pi_{\theta}(a \vert s)$               | Learn the value function $Q(s,a)$                                              |
  | Policy type       | On-policy, requiring fresh samples                             | Off-policy, can use replay buffers from old-policy, human demonstrations, etc. |
  | Sample efficiency | Less sample efficient, requiring more environment interactions | More sample efficient, can benefit from large replay buffers                   |

## REINFORCE Algorithm

- <img style="width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/REINFORCE.jpg" />
- **Steps**
  - step 1: Initialize the network $\theta$ with random weights.
  - step 2: Play N full episodes, saving their (s,a,r,s′) transitions.
  - step 3: For every step, $t$, of every episode, $k$, calculate the discounted total reward for the subsequent steps:
    - $Q_{k,t} = \sum_{t'=t}^{T} \gamma^{t'-t} r_{k,t'}$
  - step 4: Calculate the loss function for all transitions:
    - $\mathcal{L} = - \sum_{k,t} Q_{k,t} \log \left( \pi(s_{k,t}, a_{k,t}) \right)$
  - step 5: Perform an SGD update of weights, minimizing the loss.
  - step 6: Repeat from step 2 until convergence is achieved.
