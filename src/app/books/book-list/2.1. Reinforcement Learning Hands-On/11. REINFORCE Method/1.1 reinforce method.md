# REINFORCE Method

## Policy-Based Methods Intro

- Why Policy-Based Methods over Q-Learning?
  - Q-Learning **cannot handle continuous actions** (recall that bellman equation for Q-learning $Q(s,a) = r + \gamma \max_{a'} Q(s',a')$)
- Policy Gradient
  - $\nabla_{\theta} J(\theta) = \sum_{s \in S} \sum_{a \in A} \nabla_{\theta} \pi_{\theta}(a|s) \cdot Q^{\pi}(s,a)$
    - This gradient defines the direction to change our network’s parameters to improve the policy in terms of the accumulated total reward.
- Policy-Based vs Value-Based
  | Aspect            | Policy-Based Methods                                           | Value-Based Methods                                                            |
  | ----------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------ |
  | What they learn   | Directly learn the policy $\pi_{\theta}(a \vert s)$               | Learn the value function $Q(s,a)$                                              |
  | Policy type       | On-policy, requiring fresh samples                             | Off-policy, can use replay buffers from old-policy, human demonstrations, etc. |
  | Sample efficiency | Less sample efficient, requiring more environment interactions | More sample efficient, can benefit from large replay buffers                   |

## REINFORCE Algorithm

- **Steps**
  - step 1: Initialize the network $\theta$ with random weights.
  - step 2: Play N full episodes, saving their (s,a,r,s′) transitions.
  - step 3: For every step, $t$, of every episode, $k$, calculate the discounted total reward for the subsequent steps:
    - $Q_{k,t} = \sum_{t'=t}^{T} \gamma^{t'-t} r_{k,t'}$
  - step 4: Calculate the loss function for all transitions:
    - $\mathcal{L} = - \sum_{k,t} Q_{k,t} \log \left( \pi(s_{k,t}, a_{k,t}) \right)$
  - step 5: Perform an SGD update of weights, minimizing the loss.
  - step 6: Repeat from step 2 until convergence is achieved.

- **Disadvantages** of REINFORCE and Cross-Entropy Method
  - _**Full episodes are required**_
    - _**solution**_: use network to estimate $V (s)$ and use this estimation to obtain $Q(s,a)$ (_**actor-critic method**_)
  - _**High gradient variance**_
    - in the policy gradient formula, gradient is proportional to the discounted reward from the given state.
    - _**solution**_: from the reward, subtract a baseline, which can be:
      - Some constant value, which is normally the mean of the discounted rewards
      - The moving average of the discounted rewards
      - The value of the state, $V (s)$
  - _**Suboptimal policies**_
    - in DQN, the epsilong-greedy method is used to jump out of local optima, but policy-based (on-policy) methods always stick to the same policy.
    - _**solution**_: use entropy bonus
      - $\mathcal{L} = - \sum_{k,t} Q_{k,t} \log \left( \pi(s_{k,t}, a_{k,t}) \right) + \alpha H(\pi)$
        - $H(\pi) = - \sum_{a \in A} \pi(a \vert s) \log \pi(a \vert s)$
        - $\alpha$ is a hyperparameter that controls the strength of the entropy regularization
  - _**High correlation of samples**_
    - in DQN, this problem is solved by using experience replay.
    - _**solution**_: use parallel environments