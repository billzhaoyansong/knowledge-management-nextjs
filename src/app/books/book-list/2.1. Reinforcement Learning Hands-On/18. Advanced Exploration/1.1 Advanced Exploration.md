# Advanced Exploration

## Intro

- Why Exploration is important?
  - Effective exploration avoids revisiting familiar states and increases the learning speed
  - Effective exploration brings more good-reward samples in sparse reward environments
- Problems in _**$\epsilon$-greedy**_ exploration and _**entropy loss**_
  - Both methods rely on random actions to encourage exploration. However, _**pure randomness can be inefficient because it may repeatedly visit uninformative or already-known states**_, fail to discover rare but important states, and does not prioritize actions that could lead to higher rewards or novel experiences

## Alternative Exploration Methods

- **Noisy networks**
  - add Gaussian noise to the network‚Äôs weights and learn the noise parameters (mean and variance) using backpropagation
  - <details><summary>Comparison with ùúñ-greedy</summary>

      - In ùúñ-greedy, randomness is added to the actions
      - In noisy networks, randomness is injected into part of the network itself
      - <img style="width: 75%;max-width: 400px;" src="books/Reinforcement Learning Hands-On/noisy net.png" alt="Comparison with ùúñ-greedy" />

    </details>

- **Count-based exploration**
  - visit states that have not been explored before, when the state space is not large and distinguishable, by assigning an _**intrinsic reward**_ to the state based on the number of times it has been visited
  - _**Methods**_:
    - _**bandits exploration**_: $r_i = c \frac{1}{\sqrt{\tilde{N}(s)}}$
      - $\tilde{N}(s)$: count or pseudo-count of times we have seen the state $s$
      - $c$: weight of the intrinsic reward
    - curiosity-driven exploration:
      - training and exploration is driven $100\%$ by the novelty of the agent‚Äôs experience

- **Prediction-based methods**
  - philosophy: accurate predictions mean the state is familiar and less worth exploring.
  - _**Components**_:
    - _**The Predictor Network (Trained NN)**_: This network is actively trained. Its job is to look at the current state and the action the agent takes, and then predict a representation of the next state.
    - _**The Target Network (Untrained NN)**_: Randomly Initialized and Its weights are set randomly at the beginning and are never updated.
  - _**Input**_ for both network: current observation $s_t$
  - _**Intrinsic reward**_ and _**Loss**_ for the trained network:
    - MSE between outputs of the predictor and target networks