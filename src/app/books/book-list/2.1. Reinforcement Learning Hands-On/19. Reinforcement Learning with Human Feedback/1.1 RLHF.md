# Reinforcement Learning with Human Feedback

## Motivation

- Reward function is hard to design in real-life situations
- RLHF enables to avoid low-level reward function tweaking, allowing humans to give feedback to the agent's behavior

## Implementation

- replace the reward from the environment with a neural network called _**reward predictor**_
- <img style="width: 75%;max-width: 400px;" src="books/Reinforcement Learning Hands-On/RLHF.jpg" alt="RLHF" />