# Bellman Equation

- **State-Value** (Value of State) and Optimality
  - $\begin{aligned}V^{\pi}(s) &= \mathbb{E}_{a \sim \pi(\cdot \mid s)}\left[R(s,a) + \gamma \mathbb{E}_{s' \sim P(\cdot \mid s,a)} [V^{\pi }(s')]\right] \\ &= \sum\limits_{a \in A} \pi(a \mid s) \sum\limits_{s' \in S} P(s' \mid s, a) \left[ R(s' \mid s, a) + \gamma V^{\pi}(s') \right] \textit{(Compact Form)} \\ &= \sum\limits_{a \in A} \pi(a \mid s) \left[ R(s, a) + \gamma \sum\limits_{s' \in S} P(s' \mid s, a) V^{\pi}(s') \right] \end{aligned}$ _(Policy Iteration)_
    - <details><summary><i>notations</i></summary>

      - $V^{\pi}(s)$: Value function of state $s$ under policy
      - $\pi(a | s)$: Probability of taking action $a$ in state $s$ under policy
      - $P(s' | s, a)$: Transition probability from state $s$ to state $s'$ when taking action $a$
      - $R(s, a)$: Reward obtained after taking action $a$ in state $s$
      - $Î³$: Discount factor controlling the importance of future rewards

      </details>

    - <details><summary><i>an example</i></summary>

      - <img style="width:75%;max-width:200px;" src="/books/Reinforcement Learning Hands-On/state-value-example.jpg" />
      - assume $\gamma = 1$ and return of $S1=1, S2=2,S3=3,S4=4$
      - $V(S0)= 0.6 \times (0.7 \times 1 + 0.3 \times 2) + 0.4 \times (0.2 \times 3 + 0.8 \times 4)$

      </details>
  - _**Optimality**_
    - $\begin{aligned}V^*(s) &= \max\limits_{a \in A} \sum\limits_{s' \in S} P(s' | s, a) \left[ R(s' \mid s, a) + \gamma V^*(s') \right] \\ &= \max\limits_{a \in A}\left[ R(s, a) + \gamma \sum\limits_{s' \in S} P(s' | s, a) V^*(s') \right]\end{aligned}$ _(Value Iteration)_
- **Action-Value** (Value of Action) and Optimality
  - $\begin{aligned}Q^{\pi}(s,a) &= R(s,a) + \gamma \mathbb{E}_{s' \sim P(\cdot \mid s,a)} \left[ \mathbb{E}_{a' \sim \pi(\cdot \mid s')} [Q^{\pi}(s',a')] \right] \\ &=\sum\limits_{s' \in S} P(s' \mid s, a) \left[R(s' \mid s, a) + \gamma  \sum_{a'} \pi(a' \mid s') Q^{\pi} (s',a') \right] \\ &= R(s, a) + \gamma \sum\limits_{s' \in S} P(s' \mid s, a) \sum_{a'} \pi(a' \mid s') Q^{\pi} (s',a') \end{aligned}$
  - _**Optimality in General**_
    - $\begin{aligned}Q^*(s, a) &= \sum_{s'} P(s' \mid s, a) \left[ R(s, a) + \gamma \max_{a'} Q^*(s', a') \right] \\ &=  R(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \max_{a'} Q^* (s',a') \end{aligned}$ _(Q-Learning)_
  - _**Optimality in Deterministic Environment**_
    - $Q^*(s, a)=R(s,a)+\gamma \max\limits_{a' \in A} Q^*(s',a')$ _(Q-Learning)_
- **Relationships** between State-Value and Action-Value
  - $V^{\pi}(s)=\sum\limits_{a\in A} \pi(a|s)Q^{\pi}(s,a)$
  - $Q^{\pi}(s,a) = \sum\limits_{s'\in S} P(s' \mid s, a)\left[R(s' \mid s,a) + \gamma V^{\pi}(s')\right]$
  - $V^*(s)=\max\limits_{a\in A} Q^{\pi}(s,a)$