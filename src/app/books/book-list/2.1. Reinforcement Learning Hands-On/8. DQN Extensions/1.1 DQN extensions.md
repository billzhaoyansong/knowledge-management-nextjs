# DQN Extensions

- **N-step DQN**
  - to improve convergence speed and stability with a simple unrolling of the Bellman equation
  - however, it is **not** an ultimate solution
  - in practice, small values of $n$ (two or three) usually work well
  - <details><summary><i>Motivational example</i></summary>

    - Bellman equation on Q value is recursive: 
    - $\begin{aligned}Q(s_t,a_t) &= r_t + \gamma \max_{a' \in A} Q(s_{t+1},a') \\ &= r_{t} + \gamma \max_{a' \in A} [r_{a', t+1} + \gamma \max_{a'' \in A} Q(s_{t+2},a'')] \\ \vdots \end{aligned}$
    - <img style="width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/n-step dqn.png" />

        - in one-step case: 
        - $Q(s_1,a) ←r_1 + \gamma Q(s_2,a)$
        - $Q(s_2,a) ←r_2 + \gamma Q(s_3,a)$
        - $Q(s_3,a) ←r_3$
            - The first two updates will be useless, as our current $Q(s_2,a)$ and $Q(s_3,a)$ are incorrect and contain initial random values. The only useful update will be update 3
        - in two-step case:
        - $Q(s_1,a) ←r_1 + \gamma r_2 + \gamma^2 Q(s_3,a)$
        - $Q(s_2,a) ←r_2 + \gamma r_3$
        - $Q(s_3,a) ←r_3$
            - on the first loop over the updates, the correct values will be assigned to both $Q(s_2,a)$ and $Q(s_3,a)$. On the second iteration, the value of $Q(s_1,a)$ will also be properly updated.
    - **multiple steps** improve the propagation speed of values, which **improves convergence**
    - However, unrolling the Bellman equation with large steps makes DQN fail to converge:
        - For instance, during early training when actions are random, our $Q(s_t,a_t)$ estimate may be too low since random actions don't follow optimal paths. Larger step sizes in Bellman unrolling amplify this error.
        - Large replay buffers can worsen this by sampling outdated transitions from poor policies

  </details>

---

- **Double DQN**
  - Motivation
    - rooting from the max operation in the Bellman equation, basic DQN tends to overestimate the values for Q, leading to suboptimal policies
  - Solution
    - Original DQN: $Q(s_t,a_t) ←r_t + \gamma \max_{a' \in A} \hat{Q}(s_{t+1},a')$
    - Double DQN: $Q(s_t,a_t) ←r_t + \gamma \hat{Q}(s_{t+1},\arg\max_{a' \in A} Q(s_{t+1},a'))$
      - where $\hat{Q}$ is the target network
---
- **Noisy Networks**
  - Motivation (for exploration)
    - _epsilon-greedy policy_ is **state-independent**, which means the agent is just as likely to take a random action in a critical, well-understood state as it is in a brand-new, unseen state.
    - fine-tuning the exploration rate is a **time-consuming** process
  - Solution
    - Noisy Networks: Instead of a standard linear layer in the neural network, which computes $y = wx + b$, a NoisyNet layer computes: $y = (μ_w + σ_w * ε_w)x + (μ_b + σ_b * ε_b)$
      - $μ$ is the main, learnable weight.
      - $σ$ is the standard deviation (the amount of noise), which is also learnable.
      - $ε$ is a random noise variable that is resampled on each step.
  - <details><summary><i>Implementation with Stable Baselines3</i></summary>

    ```python
    # Note: You can remove all `exploration_*` parameters now.
    model = DQN(
        "MlpPolicy",
        env,
        policy_kwargs=dict(noisy=True) # This is the key change
        # ... other params
    )
    ```
  </details>
---
- **Prioritized replay buffer**
  - Motivation (for training)
    - uniform sampling of our experience is not the best way to train
  - Solution
    - Prioritized replay buffer, sample based on priority, which is calculated based on the error between the predicted and target Q-values
  - _no implementation in SB3_
---
- **Dueling DQN**
  - Motivation
    - standard DQN learns a single function $Q(s,a)$, but in some states, the choice of action is not very important
      - e.g. in the case of the CartPole environment, the choice of action is not very important, as the pole will always fall down; On the other hand, when the pole
    - to improve convergence speed by making our network’s architecture more closely represent the problem that we are solving
  - Solution
    - Dueling DQN: Instead of learning a single function $Q(s,a)$, we learn two functions: $V(s)$ and $A(s,a)$
      - $V(s)$ is the value function, which estimates the value of the state
      - $A(s,a)$ is the advantage function, which estimates the advantage of the action in the state
        - $Q(s,a) = V(s) + A(s,a)$
---

- **Categorical DQN**
  - Motivation
    - How to go beyond the single expected value of the action and work with full distributions
    - Both the Q-learning and value iteration methods work with $V$ or $Q$ by predicting a stochastic single average 
  - Solution
    - Categorical DQN: Instead of predicting a single average value, we predict a distribution of possible values with the updated Bellman equation:
      - $Z(x,a) \overset{D}{=} R(x,a) + \gamma Z(x',a')$
        - $Z(x,a)$ is the distribution of possible values for the action $a$ in the state $x$
        - $R(x,a)$ is the reward distribution function
        - $A \overset{D}{=} B$: $A$ and $B$ are two random variables with the same distribution
      - Loss function: KL divergence


