# PyTorch

## Tensors

- **Definition**
  - In DL, a tensor is any _**multi-dimensional array**_
      - <img style="display:inline-block;width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/tensor.png" />
      - _In mathematics, a tensor is a mapping between vector spaces, often represented as a multi-dimensional array, but with deeper semantic meaning._
  - Types of Tensors: `torch.FloatTensor` (corresponding to a 32-bit float), `torch.ByteTensor` (an 8-bit unsigned integer), `torch.LongTensor` (a 64-bit signed integer), etc
  - `tensor.to('cuda' | 'cpu')`: conversion between GPU tensor and CPU tensor
- **Operations & Attributes & Methods**
  - _**Creation**_ [(for more)](https://docs.pytorch.org/docs/stable/torch.html#creation-ops)
    - <details><summary>common creation methods</summary>

      - `dtype` can be: `torch.int32`, `torch.float64`, etc. [(for more)](https://docs.pytorch.org/docs/stable/tensors.html#data-types)

        | Operation                  | Code Example                                                                                                                        | Notes                                                                                         |
        | -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
        | Scalar                     | `torch.tensor(72)`                                                                                                                  | Access the value via `x.item()`                                                               |
        | All Zeros                  | `torch.zeros(size=[3, 4], dtype=torch.float32)`                                                                                     | Specify `size` and optional `dtype`                                                           |
        | All Ones                   | `torch.ones(size=[3, 4])`                                                                                                           | Specify `size`                                                                                |
        | Full                       | `torch.full(size=[3, 4], 3.1415)`                                                                                                   | Fill tensor with a specified value                                                            |
        | Full Like                  | `torch.full_like(input=[[3, 4],[1,2,3]], 3.1415)`                                                                                   | Create tensor like `input` but filled with a specified value                                  |
        | Tensor Conversion          | `torch.tensor([[3,2,1], [4,5,6]], dtype=torch.int32)`                                                                               | Convert Python list to tensor with optional `dtype`                                           |
        | From Existing Tensor       | `tensor.new_zeros()`, </br> `tensor.new_ones()`, </br> `tensor.new_empty()`, </br> `tensor.new_full()`, </br> `tensor.new_tensor()` | create new tensors that inherit properties (`dtype`, `device`, etc.) from the existing tensor |
        | Random `Float32` (Normal)  | `torch.randn(size=[2,3])`                                                                                                           | Random numbers from standard normal distribution $(−\infty, \infty)$                          |
        | Random `Float32` (Uniform) | `torch.rand(size=[2,3])`                                                                                                            | Random numbers from uniform distribution on $[0, 1)$                                          |
        | Random `Int64` (Uniform)   | `torch.randint(low=3, high=5, (3,))`                                                                                                | Random integers from uniform distribution between `low` and `high`                            |

    </details>
  - _**Tensor Attributes**_ [(for more)](https://docs.pytorch.org/docs/stable/tensors.html#tensor-class-reference)
    - <details><summary>common attributes</summary>
  
        | Property       | Type             | Description                                                           |
        | -------------- | ---------------- | --------------------------------------------------------------------- |
        | `is_cuda`      | `bool`           | Whether the Tensor is stored on the GPU                               |
        | `is_quantized` | `bool`           | Whether the Tensor is quantized                                       |
        | `is_leaf`      | `bool`           | Whether the Tensor is created by user                                 |
        | `is_meta`      | `bool`           | Whether the Tensor is a meta tensor                                   |
        | `device`       | `str`            | The device where this Tensor is located (e.g., `'cpu'` or `'cuda:0'`) |
        | `require_grad` | `bool`           | Whether the Tensor requires gradients to be calculated                |
        | `grad`         | `None \| Tensor` | `None` by default; becomes a `Tensor` after calling `backward()`      |
        | `ndim`         | `int`            | Number of dimensions (rank) of the Tensor                             |
        | `itemsize`     | `int`            | Size in bytes of an individual element                                |
        | `nbytes`       | `int`            | Total size in bytes of the Tensor (`nbytes = itemsize × nelement()`)  |

    </details>

  - _**Math**_ Operations as Tensor Methods [(for more)](https://docs.pytorch.org/docs/stable/torch.html#pointwise-ops)
    - <details><summary>common math operations</summary>

      | Operation                                                        | Formula                                                                         | Description                                      | Shape Requirements                                                          |
      | ---------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------ | --------------------------------------------------------------------------- |
      | **`abs()`**                                                      | -                                                                               | Creates a new tensor with absolute values        | Same as input                                                               |
      | **Trigonometric Ops**<br>`cos()`, `acos()`,<br>`sin()`, `asin()` | -                                                                               | Creates new tensor with trig/inverse-trig values | Same as input                                                               |
      | **`add(number, alpha=1)`**                                       | `out = original + alpha * number`                                               | Element-wise addition with scaling               | Same as input                                                               |
      | **`addbmm(batch1, batch2, *, beta=1, alpha=1)`**                 | $\beta\text{input} + \alpha(\sum_{i=0}^{b-1}\text{batch1}_i @ \text{batch2}_i)$ | Batch matrix-matrix product and add              | `batch1`: (b×n×m)<br>`batch2`: (b×m×p)<br>`input`: (n×p) → `out`: (n×p)     |
      | **`addmm(mat1, mat2, *, beta=1, alpha=1)`**                      | $\beta\text{input} + \alpha(\text{mat1} @ \text{mat2})$                         | Matrix multiplication and add                    | `mat1`: (n×m)<br>`mat2`: (m×p)<br>`input`: (n×p) → `out`: (n×p)             |
      | **`sspaddmm(mat1, mat2, *, beta=1, alpha=1)`**                   | Same as `addmm()`                                                               | Sparse matrix variant of `addmm`                 | `input` and `mat1` must be sparse                                           |
      | **`addcmul(tensor1, tensor2, *, value=1)`**                      | $\text{input} + \text{value}×\text{tensor1}×\text{tensor2}$                     | Element-wise multiply-add                        | All inputs broadcastable                                                    |
      | **`addmv(mat, vec, *, beta=1, alpha=1)`**                        | $\beta\text{input} + \alpha(\text{mat} @ \text{vec})$                           | Matrix-vector product and add                    | `mat`: (n×m)<br>`vec`: (m)<br>`input` broadcastable to (n) → `out`: (n)     |
      | **`addr(vec1, vec2, *, beta=1, alpha=1)`**                       | $\beta\text{input} + \alpha(\text{vec1} \otimes \text{vec2})$                   | Outer product and add                            | `vec1`: (n)<br>`vec2`: (m)<br>`input` broadcastable to (n×m) → `out`: (n×m) |
    </details>

  - _**Comparision**_ Operations as Tensor Methods [(for more)](https://docs.pytorch.org/docs/stable/torch.html#comparison-ops)
  - _**Reduction**_ Operations as Tensor Methods [(for more)](https://docs.pytorch.org/docs/stable/torch.html#reduction-ops)
    - <details><summary>common reduction operations</summary>

        | Operation                                                                                       | Description                                                                                               |
        | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
        | `max()` <br/> `min()` <br/> `mean()` <br/> `median`    <br/> `sum`                              | Returns the max/min/mean/median/sum value of all elements in the input tensor.                            |
        | `argmax()` <br/> `argmin()`                                                                     | Returns the _indices_ of max/min value of all elements in the input tensor.                               |
        | `amax(dim, keepdim=False)` <br/> `amin(dim, keepdim=False)` <br/> `aminmax(dim, keepdim=False)` | Returns the _indices_ of max/min value of each slice of the input tensor in the given dimension(s) `dim`. |
        | `any()` <br/> `all()`                                                                           | Tests if _any_/_all_ elements in input evaluate to True.                                                  |

      </details>
  - _**Indexing, Slicing, Joining, Mutating**_ Operations as Torch Methods [(for more)](https://docs.pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops)
    - <details><summary>common operations</summary>

        | Code Example                         | Notes                                                                                                                                                                              |
        | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
        | `torch.cat(tensors, dim=0)`          | concatenates the given sequence of tensors                                                                                                                                         |
        | `torch.stack(tensors, dim=0)`        | creates a new dimension                                                                                                                                                            |
        | `torch.transpose(input, dim0, dim1)` | returns a transposed tensor with `dim0` and `dim1` are swapped                                                                                                                     |
        | `torch.squeeze(input, dim)`          | returns a tensor with all specified dimensions of input of size $1$ removed.                                                                                                       |
        | `torch.unsqueeze(input, dim)`        | returns a new tensor with a dimension of size one inserted at the specified position. [(e.g)](https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze) |

      - note: `torch.vstack()` and `torch.hstack()` are actually _concatenation_ methods
    </details>

- **Gradients in Tensor**
  - leaf tensors _(tensor created by user)_ have to explicitly state `requires_grad=True` during creation
  - non-leaf tensors will inherit this attribute from leaf tensors
    - <details><summary><i>an example</i></summary>

      - <img style="display:inline-block;width:75%;max-width:300px;" src="/books/Reinforcement Learning Hands-On/tensor gradient.png" />
      ```python
      # v1, v2 are leaf tensors
      v1, v2 = torch.tensor([1.0, 1.0], requires_grad=True), torch.tensor([2.0, 2.0])

      # v_sum, v_res are non-leaf tensors
      v_sum = v1 + v2
      v_res = (v_sum*2).sum()

      # grad on v1 can be evaluated only after backward() invoked
      v_res.backward()
      print(v1.grad) # v2 doesn't have grad 
      ```

    </details>

## Torch.nn.functional

- Intro
  - in `pytorch.nn.functional`, provide utility functions [(full reference)](https://docs.pytorch.org/docs/stable/nn.functional.html)
- **Common functions**
  - <details><summary>Sparse Functions</summary>

    <table>
        <tr>
            <th>Name</th>
            <th>Notes</th>
        </tr>
        <tr>
          <td>
            <code>
            nn.functional.one_hot(tensor, num_classes=-1)
            </code>
          </td>
          <td>
            <code>
            >>> F.one_hot(torch.tensor([2,3]), num_classes=6)
            </code><br/>
            <code>
            tensor([[0, 0, 1, 0, 0, 0], [0, 0, 0, 1, 0, 0]])
            </code>
          </td>
        </tr>
        <tr>
          <td>
            <code>nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)</code>
          </td>
          <td>
          Generate a simple lookup table that looks up embeddings in <code>weight</code>
          </td>
        </tr>
        <tr>
          <td>
          </td>
          <td>
          </td>
        </tr>
    </table>

    </details>

## NN Building Blocks

- `nn.Module`
  - the basic building block in PyTorch, and is abstracted in the `nn.Module` base class
  - Modules can also contain other Modules, allowing them to be nested in a tree structure.
  - <details><summary>useful base methods in <code>nn.Module</code></summary>

    - `parameters()`: returns an iterator of all variables that require gradient computation
    - `zero_grad()`: initializes all gradients of all parameters to zero.
    - `to(device)`: moves all module parameters to a given device (CPU or GPU).
    - `state_dict()`: returns the dictionary with all module parameters and is useful for model serialization.
    - `load_state_dict()`: initializes the module with the state dictionary

    </details>

    - [for all class reference](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)
- **Default Modules** [(for more)](https://docs.pytorch.org/docs/stable/nn.html)
  - <details><summary>Linear layers</summary>

    | Name            | Notes                                                                                                                |
    | --------------- | -------------------------------------------------------------------------------------------------------------------- |
    | `nn.Linear`     | affine linear transformation: $y=x A^T + b$                                                                          |
    | `nn.LazyLinear` | A `torch.nn.Linear` module where _in_features_ is inferred (increasing flexibility with slightly performance suffer) |
    | `nn.Bilinear`   | affine linear transformation: $y=x_1^T A x_2 + b$                                                                    |
    | `nn.Identity`   | A placeholder identity operator                                                                                      |

    </details>

  - <details><summary>Dropout layers</summary>

    | Name                                                     | Notes                                                                                           |
    | -------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
    | `nn.Dropout`                                             | During training, randomly zeroes some of the elements of the input tensor with probability $p$. |
    | `nn.Dropout1d` <br/> `nn.Dropout2d` <br/> `nn.Dropout3d` | Randomly zero out entire channels.                                                              |

    </details>
  
  - <details><summary>Pooling layers</summary>

    | Name                                                     | Notes                                                                                     |
    | -------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
    | `nn.MaxPool1d` <br/> `nn.MaxPool2d` <br/> `nn.MaxPool3d` | Applies a 1D/2D/3D max pooling over an input signal composed of several input planes.     |
    | `nn.AvgPool1d` <br/> `nn.AvgPool2d` <br/> `nn.AvgPool3d` | Applies a 1D/2D/3D average pooling over an input signal composed of several input planes. |

    </details>
  
  - <details><summary>Convolution layers</summary>
    </details>
  
  - <details><summary>Padding layers</summary>
    </details>
  
  - <details><summary>Shuffle layers</summary>
    </details>
  - <details><summary>Loss functions</summary>

    | Name                                   | Notes                     |
    | -------------------------------------- | ------------------------- |
    | `nn.L1Loss`                            |                           |
    | `nn.MSELoss`                           |                           |
    | `nn.BCELoss` and `nn.BCEWithLogits`    | binary cross-entropy loss |
    | `nn.CrossEntropyLoss` and `nn.NLLLoss` | maximum likelihood        |
    |                                        |                           |
    | `nn.KLDivLoss`                         |                           |

    </details>

- **Custom Modules** [(for more)](https://docs.pytorch.org/docs/stable/notes/modules.html)
  - A skeleton of custom modules

    ```python
    class CustomNet(nn.Module):
        def __init__(self):
            super().__init__()
            pass

        def forward(self, x):
            pass
    ```

  - Some implementations
    - <details><summary>Dynamic Net</summary>

      ```python
        class DynamicNet(nn.Module):
            def __init__(self, num_layers):
                super().__init__()
                self.linears = nn.ModuleList(
                    [MyLinear(4, 4) for _ in range(num_layers)])
                self.activations = nn.ModuleDict({
                    'relu': nn.ReLU(),
                    'lrelu': nn.LeakyReLU()
                })
                self.final = MyLinear(4, 1)
            def forward(self, x, act):
                for linear in self.linears:
                    x = linear(x)
                x = self.activations[act](x)
                x = self.final(x)
                return x

        dynamic_net = DynamicNet(3)
        sample_input = torch.randn(4)
        output = dynamic_net(sample_input, 'relu')
      ```

      </details>

## Optimizer and Training

- **Optimizers**
  - stored in `torch.optim` [(for more)](https://docs.pytorch.org/docs/stable/optim.html)
  - common optimizers are `optim.SGD`, `optim.Adam`, `optim.RMSprop`, `optim.Adagrad`
- **Training**
  - combine module, optimizer, and loss function
  - <details><summary>a concise example</summary>

    ```python
    # initialize data
    x = torch.randn(3, 1, requires_grad=True)
    y_r = 3 * x**2 + torch.rand(3,1)
    y = y_r.detach()

    # specify module, optimizer, loss
    l0 = nn.Linear(1, 1)
    optimizer = torch.optim.SGD(l0.parameters())
    loss_fn = nn.MSELoss()

    # where real training begins
    for i in range(10000):
        optimizer.zero_grad()
        out = l0(x)
        loss = loss_fn(out, y)
        loss.backward()
        optimizer.step()
    ```

    </details>

## Monitoring with TensorBoard

- Intro
  - available in the `torch.utils.tensorboard` package
- How to Use
  - Write Data
    - By default, a unique directory in the `runs` directory will be created
    - The name of the new directory includes the current date and time and hostname
    - the writer does a periodical flush (by default, every two minutes);  to flush explicitly, it has the `flush()` method
    - <details><summary>a concise example</summary>

      ```python
      from torch.utils.tensorboard.writer import SummaryWriter 

      writer = SummaryWriter() 

      funcs = {"sin": math.sin, "cos": math.cos, "tan": math.tan}
      for angle in range(-360, 360): 
          angle_rad = angle * math.pi / 180 
          for name, fun in funcs.items(): 
              val = fun(angle_rad) 
              writer.add_scalar(name, val, angle)

      writer.close()
      ```

      </details>
  - Start
    - `tensorboard --logdir --bind_all runs`

## High-level Libraries

- Intro
  - standard PyTorch is at _low-level_, providing flexibility yet with routine tasks, including data preparation, calculation metrics
  - **high-level** libraries, such as `ptlearn`, `fastai`, `ignite`, are developed to simplify those routines [(for full high-level libraies)](https://landscape.pytorch.org/)