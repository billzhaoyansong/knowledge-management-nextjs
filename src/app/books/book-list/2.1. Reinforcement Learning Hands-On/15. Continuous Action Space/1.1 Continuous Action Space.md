# Continuous Action Space

## Intro

- Motivation
  - most of the environments that include continuous action spaces are related to the physical world
  - there are some details to consider before using the advantage actor-critic (A2C) or deep Q-network (DQN) methods to tackle problems with continuous action spaces
- Representations
  - purely continuous action spaces are represented as the `gym.spaces.Box` class
  - action spaces that are a combination of discrete and continuous actions are represented as the `gym.spaces.Tuple` class
- Environments
  - [MuJoCo](https://gymnasium.farama.org/environments/mujoco/)
  - [PyBullet](https://pybullet.org/wordpress/)

## A2C

- <img src="/books/Reinforcement Learning Hands-On/A2C Continuous Action Space.jpg" alt="A2C" width="500"/>
- **Adaptation** for continuous action spaces
  - _in **discrete cases**, only one action with several mutually exclusive discrete values is available, of which the policy is represented by the probability distribution_
    - _therefore, only 2 heads are needed in the NN: one for the policy and one for the value function_
  - in **continuous cases** with $N$ actions, the policy will be represented by $2$ vector of $N$ values: one vector for the mean $\mu$ and one vector for the variance $\sigma^2$
    - therefore, $3$ heads are needed in the NN
      - one for the mean value of actions
      - one for the variance of actions, 
      - and one for the value of the state
- in benchmarks (e.g., MuJoCo, PyBullet), A2C often _**underperforms**_:
  - PPO: Uses clipped objectives for more stable updates.
  - SAC/TD3: Leverage off-policy samples + target networks for better sample efficiency and stability.
- Why A2C underperforms?
  - _**A2C**_ uses vanilla policy gradients with no constraint on step size; _**PPO**_ introduces clipped/adaptive objective functions (e.g., PPO-Clip) or KL-divergence penalties to enforce "trust regions." 
  - _**A2C**_ is on-policy, discards all past data after each update; _**SAC/TD3**_ are off-policy, leverage replay buffers to reuse historical data.
- Use Cases:
  - Ideal for moderate-complexity tasks (e.g., simple robotics, pendulum swing-up) where sample efficiency isnâ€™t critical. Avoid for very high-dimensional controls (e.g., humanoid locomotion) without heavy tuning.

## Deep Deterministic Policy Gradient (DDPG)

- Intro
  - An actor-critic method with off-policy property
- <img src="/books/Reinforcement Learning Hands-On/DDPG.jpg" alt="DDPG" width="500"/>