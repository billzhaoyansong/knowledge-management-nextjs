# Cross-Entropy Method (CEM)

## High-Level Idea

- model-free, policy-based, on-policy
- Goal: 
  - CEM is an optimization technique to find the best parameters (e.g., policies in RL) by iteratively:
    1. Sampling candidate solutions.
    2. Selecting the top-performing ones ("elites").
    3. Updating the sampling distribution toward these elites.
- Analogy: Imagine tuning a radio antenna:
  - Randomly adjust the antenna (sample policies).
  - Keep positions with the clearest signal (top rewards).
  - Adjust future randomness toward these good positions.
- **Theoretical Foundation**
  - _**Importance Sampling**_: Approximate a hard-to-compute distribution by reweighting samples from another distribution.
  - _**KL Divergence**_: Measure the "distance" between two distributions.
- How to implement for RL problem
  - After the agent plays several episodes — where randomness in the environment and action selection leads to varying performance — the Cross-Entropy Method discards poor episodes and retrains on the better ones.
- <img style="width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/CEM.jpg" />
- **Steps**
  1. Play $N$ episodes using our current model and environment.
  2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use a percentile of all rewards, such as the $50$th or $70$th.
  3. Throw away all episodes with a reward below the boundary.
  4. Train on the remaining ”elite” episodes (with rewards higher than the boundary) using observations as the input and issued actions as the desired output.
  5. Repeat from step 1 until we become satisfied with the result.
- Limitations
  - Sample Inefficiency
    - Relies on Monte Carlo sampling, which discards most samples (non-elites)
  - Local Optima
    - The elite samples may dominate the update, causing premature convergence to suboptimal solutions

## Cartpole

- Observation Space
  - `Box(4,)`
    - `[-4.8 -inf -0.41887903 -inf], [4.8 inf 0.41887903 inf]`
      - Cart position: Value in $[−4.8, 4.8]$ range
      - Cart velocity: Value in $(-\infty, \infty)$ range
      - Pole angle: Value in radians in $[−0.418, 0.418]$ range
      - Pole angular velocity: Value in $(-\infty, \infty)$ range
- Action Space
  - `Discrete(2,)`
    - `0`: push to left; `1`: push to right
- Policy Net
  - <details><summary>a simple implementation</summary>

    ```python
    class Net(nn.Module):
      def __init__(self, obs_size: int, hidden_size: int,
                  n_actions: int):
          super(Net, self).__init__()
          self.net = nn.Sequential(
              nn.Linear(obs_size, hidden_size),
              nn.ReLU(),
              nn.Linear(hidden_size, n_actions)
          )

      def forward(self, x: torch.Tensor) -> torch.Tensor:
          return self.net(x)
    ```

  </details>

- Utility Data Class
  - <details><summary>codes</summary>

    ```python
    @dataclass
    class EpisodeStep:
      observation: np.ndarray
      action: int

    @dataclass
    class Episode:
      reward: float
      steps: tt.List[EpisodeStep]
    ```

    </details>

- **Step 1**: Episodes Generation with Current Policy Net
  - <details><summary><i>implementation</i></summary>

    ```python
    from collections.abc import Generator

    def generate_batches(env: gym.Env, net: Net, batch_size: int) \
      -> Generator[tt.List[Episode], None, None]:
      batch = []
      episode_reward, episode_steps = 0.0, []
      obs, _ = env.reset()
      sm = nn.Softmax(dim=1) # convert to probability
      while True:
          obs_v = torch.tensor(obs, dtype=torch.float32)

          # torch.nn.Module is designed to process input in batches, 
          # even if the batch size is just 1.
          # unsqueeze(0) wraps obs_v to [obs_v]
          act_probs_v = sm(net(obs_v.unsqueeze(0)))

          # [0] takes out the results
          # from [act_probs] to act_probs
          act_probs = act_probs_v.data.numpy()[0]

          action = np.random.choice(len(act_probs), p=act_probs)
          next_obs, reward, is_done, is_trunc, _ = env.step(action)
          episode_reward += float(reward)
          step = EpisodeStep(observation=obs, action=action)
          episode_steps.append(step)
          if is_done or is_trunc:
              e = Episode(reward=episode_reward, steps=episode_steps)
              batch.append(e)
              episode_reward = 0.0
              episode_steps = []
              next_obs, _ = env.reset()
              if len(batch) == batch_size:
                  yield batch
                  batch = []
          obs = next_obs
    ```

    </details>

- **Step 2-3**: Episodes Filtering
  - <details><summary><i>Implementation</i></summary>

    ```python
    def filter_batch(batch: list[Episode], percentile: float) -> \
        tuple[torch.FloatTensor, torch.LongTensor, float, float]:
      
      rewards = list(map(lambda s: s.reward, batch))
      reward_bound = float(np.percentile(rewards, percentile))
      reward_mean = float(np.mean(rewards))

      train_obs: list[np.ndarray] = []
      train_act: list[int] = []
      for episode in batch:
          if episode.reward < reward_bound:
              continue
          train_obs.extend(map(lambda step: step.observation, episode.steps))
          train_act.extend(map(lambda step: step.action, episode.steps))

      # vstack converts np array into python list
      train_obs_v = torch.FloatTensor(np.vstack(train_obs))
      train_act_v = torch.LongTensor(train_act)
      return train_obs_v, train_act_v, reward_bound, reward_mean
    ```

    </details>

- **Step 4**: Training on Elite Episodes
  - <details><summary><i>Implementation</i></summary>

    ```python
    env = gym.make("CartPole-v1")

    #env = gym.make("CartPole-v1", render_mode="rgb_array") 
    #env = gym.wrappers.RecordVideo(env, video_folder="video")

    obs_size = env.observation_space.shape[0]
    n_actions = int(env.action_space.n)

    net = Net(obs_size, HIDDEN_SIZE, n_actions)
    print(net)
    objective = nn.CrossEntropyLoss()
    optimizer = optim.Adam(params=net.parameters(), lr=0.01)
    writer = SummaryWriter(comment="-cartpole")

    # generate batches
    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):

      # for each batch, filter out inferior episodes
      obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)

      # update the policy model with elite episode
      optimizer.zero_grad()
      action_scores_v = net(obs_v)
      loss_v = objective(action_scores_v, acts_v)
      loss_v.backward()
      optimizer.step()
      print("%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f" % (
            iter_no, loss_v.item(), reward_m, reward_b))
      writer.add_scalar("loss", loss_v.item(), iter_no)
      writer.add_scalar("reward_bound", reward_b, iter_no)
      writer.add_scalar("reward_mean", reward_m, iter_no)
      if reward_m > 475:
        print("Solved!")
        break
    writer.close()
    ```

    </details>

## FrozenLake

- <img style="display:inline-block;width:75%;max-width:150px;" src="/books/Reinforcement Learning Hands-On/frozenlake.png" />
- Observation Space
  - `Discrete(16,)`
    - a single number to indicate location at $4 \times 4$ grid
- Action Space
  - `Discrete(4,)`
    - a single integer to indicate actions of up, down, left, right
- Challenges comparing with CartPole:
  - **Discount factor applied to the reward**: making the reward for short episodes higher than the reward for long ones
  - **Rare successful episodes**: keeping them for several iterations to train on them
  - **Longer training time and larger batches of played episodes**
- Policy Net
  - <details><summary>Option 1: create a new policy net compatible with input of FrozenLake</summary>

    ```python
    class OneHotEncodingNet(Net):
      def __init__(self, obs_size, hidden_size, n_actions):
          super(OneHotEncodingNet, self).__init__(
              obs_size, hidden_size, n_actions)
          self.obs_size = obs_size

      def forward(self, x):
          x = torch.tensor(x, dtype=torch.int64)
          x = nn.functional.one_hot(x, num_classes=self.obs_size)
          x = torch.tensor(x, dtype = torch.float)
          return self.pipeline(x)
    ```

    </details>

    - _Challenge_: tedious modifications are required in _Episodes Generation_ and _Net Training_
  - <details><summary>Option 2: wrap the FrozenLake environment to adapt to the previous <code>Net</code> (Preferrable)</summary>

    ```python
    class DiscreteOneHotWrapper(gym.ObservationWrapper): 
      def __init__(self, env: gym.Env): 
          super(DiscreteOneHotWrapper, self).__init__(env) 
          assert isinstance(env.observation_space, gym.spaces.Discrete) 
          shape = (env.observation_space.n, ) 
          self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32) 
  
      def observation(self, observation): 
          res = np.copy(self.observation_space.low) 
          res[observation] = 1.0 
          return res
    ```

    </details>

    - _Why Preferrable_: codes written for `Net` do not require modification
- **Step 2-3**: Episodes Filtering
  - <details><summary>Implementation</summary>

    ```python
    def filter_batch(batch: tt.List[Episode], percentile: float) -> \ 
        tt.Tuple[tt.List[Episode], tt.List[np.ndarray], tt.List[int], float]: 
      
      # reflect discount reward
      reward_fun = lambda s: s.reward * (GAMMA ** len(s.steps)) 
      disc_rewards = list(map(reward_fun, batch)) 
      reward_bound = np.percentile(disc_rewards, percentile) 
  
      train_obs: tt.List[np.ndarray] = [] 
      train_act: tt.List[int] = [] 
      elite_batch: tt.List[Episode] = [] 
  
      for example, discounted_reward in zip(batch, disc_rewards): 
          if discounted_reward > reward_bound: 
              train_obs.extend(map(lambda step: step.observation, example.steps)) 
              train_act.extend(map(lambda step: step.action, example.steps)) 
              elite_batch.append(example) 
  
      return elite_batch, train_obs, train_act, reward_bound
    ```

    </details>

- **Step 4**: Training on Elite Episodes
  - <details><summary>Implementation</summary>

    ```python
    full_batch = [] 
    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)): 
        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))

        # elite episodes from previous batch are injected also
        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE) 

        # only train with successful episodes
        if not full_batch: 
            continue 
        obs_v = torch.FloatTensor(obs) 
        acts_v = torch.LongTensor(acts) 
        full_batch = full_batch[-500:]
    ```

    </details>