# Cross-Entropy Method

## High-Level Idea

- model-free, policy-based, on-policy
- Philosophy
  - draws an **analogy** between _**(1) selecting actions in RL**_ and _**(2) predicting classes in supervised classification**_
    - the number of classes = the number of actions
- How to implement
  - After the agent plays several episodes — where randomness in the environment and action selection leads to varying performance — the Cross-Entropy Method discards poor episodes and retrains on the better ones.
- Steps
  1. Play $N$ episodes using our current model and environment.
  2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use a percentile of all rewards, such as the $50$th or $70$th.
  3. Throw away all episodes with a reward below the boundary.
  4. Train on the remaining ”elite” episodes (with rewards higher than the boundary) using observations as the input and issued actions as the desired output.
  5. Repeat from step 1 until we become satisfied with the result.
- Limitations
  - episodes have to be finite and preferably short
  - total reward for the episodes should have enough variability
  - intermediate reward during the episode is beneficial

## Cartpole

- Observation Space
  - `Box(4,)`
    - `[-4.8 -inf -0.41887903 -inf], [4.8 inf 0.41887903 inf]`
      - Cart position: Value in $[−4.8, 4.8]$ range
      - Cart velocity: Value in $(-\infty, \infty)$ range
      - Pole angle: Value in radians in $[−0.418, 0.418]$ range
      - Pole angular velocity: Value in $(-\infty, \infty)$ range
- Action Space
  - `Discrete(2,)`
    - `0`: push to left
    - `1`: push to right
- Policy Net
  - <details><summary>a simple implementation</summary>

    ```python
    class Net(nn.Module):
      def __init__(self, obs_size: int, hidden_size: int,
                  n_actions: int):
          super(Net, self).__init__()
          self.net = nn.Sequential(
              nn.Linear(obs_size, hidden_size),
              nn.ReLU(),
              nn.Linear(hidden_size, n_actions)
          )

      def forward(self, x: torch.Tensor) -> torch.Tensor:
          return self.net(x)
    ```

  </details>

- Utility Data Class
  - <details><summary>codes</summary>

    ```python
    @dataclass
    class EpisodeStep:
      observation: np.ndarray
      action: int

    @dataclass
    class Episode:
      reward: float
      steps: tt.List[EpisodeStep]
    ```

    </details>

- **Step 1**: Episodes Generation with Current Policy Net
  - <details><summary><i>implementation</i></summary>

    ```python
    from collections.abc import Generator

    def generate_batches(env: gym.Env, net: Net, batch_size: int) \
      -> Generator[tt.List[Episode], None, None]:
      batch = []
      episode_reward, episode_steps = 0.0, []
      obs, _ = env.reset()
      sm = nn.Softmax(dim=1) # convert to probability
      while True:
          obs_v = torch.tensor(obs, dtype=torch.float32)

          # torch.nn.Module is designed to process input in batches, 
          # even if the batch size is just 1.
          # unsqueeze(0) wraps obs_v to [obs_v]
          act_probs_v = sm(net(obs_v.unsqueeze(0)))

          # [0] takes out the results
          # from [act_probs] to act_probs
          act_probs = act_probs_v.data.numpy()[0]

          action = np.random.choice(len(act_probs), p=act_probs)
          next_obs, reward, is_done, is_trunc, _ = env.step(action)
          episode_reward += float(reward)
          step = EpisodeStep(observation=obs, action=action)
          episode_steps.append(step)
          if is_done or is_trunc:
              e = Episode(reward=episode_reward, steps=episode_steps)
              batch.append(e)
              episode_reward = 0.0
              episode_steps = []
              next_obs, _ = env.reset()
              if len(batch) == batch_size:
                  yield batch
                  batch = []
          obs = next_obs
    ```

    </details>

- **Step 2-3**: Episodes Filtering
  - <details><summary><i>Implementation</i></summary>

    ```python
    def filter_batch(batch: list[Episode], percentile: float) -> \
        tuple[torch.FloatTensor, torch.LongTensor, float, float]:
      rewards = list(map(lambda s: s.reward, batch))
      reward_bound = float(np.percentile(rewards, percentile))
      reward_mean = float(np.mean(rewards))

      train_obs: list[np.ndarray] = []
      train_act: list[int] = []
      for episode in batch:
          if episode.reward < reward_bound:
              continue
          train_obs.extend(map(lambda step: step.observation, episode.steps))
          train_act.extend(map(lambda step: step.action, episode.steps))

      # vstack converts np array into python list
      train_obs_v = torch.FloatTensor(np.vstack(train_obs))
      train_act_v = torch.LongTensor(train_act)
      return train_obs_v, train_act_v, reward_bound, reward_mean
    ```

    </details>

- **Step 4**: Training on Elite Episodes
  - <details><summary><i>Implementation</i></summary>

    ```python
    env = gym.make("CartPole-v1")

    #env = gym.make("CartPole-v1", render_mode="rgb_array") 
    #env = gym.wrappers.RecordVideo(env, video_folder="video")

    obs_size = env.observation_space.shape[0]
    n_actions = int(env.action_space.n)

    net = Net(obs_size, HIDDEN_SIZE, n_actions)
    print(net)
    objective = nn.CrossEntropyLoss()
    optimizer = optim.Adam(params=net.parameters(), lr=0.01)
    writer = SummaryWriter(comment="-cartpole")

    # generate batches
    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):

      # for each batch, filter out inferior episodes
      obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)

      # update the policy model with elite episode
      optimizer.zero_grad()
      action_scores_v = net(obs_v)
      loss_v = objective(action_scores_v, acts_v)
      loss_v.backward()
      optimizer.step()
      print("%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f" % (
            iter_no, loss_v.item(), reward_m, reward_b))
      writer.add_scalar("loss", loss_v.item(), iter_no)
      writer.add_scalar("reward_bound", reward_b, iter_no)
      writer.add_scalar("reward_mean", reward_m, iter_no)
      if reward_m > 475:
        print("Solved!")
        break
    writer.close()
    ```

    </details>

## FrozenLake

- Observation Space
  - `Discrete(16,)`
    - indicate location at $4 \times 4$ grid
- Action Space
  - `Discrete(4,)`
    - up, down, left, right