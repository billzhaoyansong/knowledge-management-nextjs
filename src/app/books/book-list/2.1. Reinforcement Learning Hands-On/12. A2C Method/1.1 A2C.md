# Advantage Actor-Critic (A2C)

## Motivation

- REINFORCE updates policy only at full episodes using total reward $R(Ï„)$
  - Consequence: A single lucky early action with high reward reinforces all episode actions, causing high variance and unstable learning.
  - Solution: to avoid using full episodes, use a _**value network**_ to estimate $V(s)$ and use this estimation to obtain $Q(s,a)$
- REINFORCE updates policy with $Q(s,a)$, which is environment-dependent
  - Consequence: leading to high variance and unstable learning.
  - Solution: update policy with _**advantage $A(s,a) = Q(s,a) - V(s)$**_ ($V(s)$ is called the baseline here) to make the gradient environment-independent.
- REINFORCE does not have an effective exploration mechanism
  - Consequence: leading to an early suboptimal policy.
  - Solution: add _**entropy bonus**_ to encourage exploration.
- REINFORCE does not have a mechanism to break the correlation between steps
  - Consequence: leading to correlated samples (not I.I.D.).
  - Solution: use _**multiple environments**_ to collect samples and achieve I.I.D. samples.

## Components and Steps

- <img style="width:90%;max-width:600px;" src="/books/Reinforcement Learning Hands-On/A2C.jpg" />
- **Components**
  - _**The Actor (The Policy Network)**_
    - parameters: $\theta$
    - input: state $s$
    - output: action $\pi_{\theta}(a|s)$
  - _**The Critic (The Value Network)**_
    - parameters: $\phi$
    - input: state $s$
    - output: value $v_{\phi}(s)$
  - _**The Advantage Function**_
    - $A(s,a) = Q(s,a) - V(s)$
- **Steps**
  - step 1: Initialize the policy network $\pi_{\theta}$ and the value network $v_{\phi}$ with random weights.
  - step 2: Run the current policy $\pi_{\theta}$ in multiple environments for $N$ steps, collecting the transitions $(s_t, a_t, r_t, s_{t+1}, d_t)$.
  - step 3: Calculate the advantage for each step:
    - $Q(s_t, a_t) = \sum\limits_{k=0}^{n-1} \gamma^k r_{t+k} + \gamma^n v_{\phi}(s_{t+n})$
    - $A(s_t, a_t) = Q(s_t, a_t) - v_{\phi}(s_t)$
      - $v_{\phi}(s_t)$: the state-value predicted by $v_{\phi}$
      - _if terminated at $K<N$, all subsequent rewards and state-values are 0_
  - step 4: calculate the loss functions:
    - Actor (Policy Network) loss: $L_{\theta} = \frac{1}{N} \sum\limits_{t=0}^{N-1} \left[A(s_t, a_t) \log \pi_{\theta}(a_t|s_t)\right]$
    - Critic (Value Network) loss: $L_{\phi} = \frac{1}{N} \sum\limits_{t=0}^{N-1} (Q(s_t, a_t) - v_{\phi}(s_t))^2$
      - <details>
        <summary style="display:inline-block;text-decoration:underline;font-style:italic;">
        
        Why use $Q(s_t, a_t) - v_{\phi}(s_t)$ instead of $A(s_t, a_t)$?
        
        </summary>
        
          - Mathematically, $Q(s_t, a_t) - v_{\phi}(s_t) = A(s_t, a_t) + v_{\phi}(s_t) - v_{\phi}(s_t) = A(s_t, a_t)$
          - but in implementation, $Q(s_t, a_t) - v_{\phi}(s_t)$ is the target value for the critic network
  
      </details>
  - step 5: add entropy bonus (encourages exploration)
    - $L_{\text{total}} = L_{\theta} + \alpha L_{\phi} - \beta L_{\text{entropy}}$
      - $L_{\text{entropy}} = - \sum\limits_{t = 0}^{N-1} \pi_{\theta}(a_t|s_t) \log \pi_{\theta}(a_t|s_t)$
  - step 6: update networks:
    - $\theta \leftarrow \theta - \eta_{\theta} \nabla_{\theta} L_{\theta}$
    - $\phi \leftarrow \phi - \eta_{\phi} \nabla_{\phi} L_{\phi}$
  - step 7: repeat steps 2-6 until convergence is achieved.
    - <details><summary><i>A step-by-step update loop</i></summary>
  
        - **Suppose:**
          - n_steps (N in our discussion) = 5
          - num_workers (parallel environments) = 8
        - **Step 1: Data Collection (The Rollout)**
          - The Coordinator sends the current policy $\pi_{\theta}$ to all 8 workers.
          - Each of the 8 workers runs its own environment for exactly N=5 steps.
          - Worker 1 collects: (s_0, a_0, r_0), (s_1, a_1, r_1), (s_2, a_2, r_2), (s_3, a_3, r_3), (s_4, a_4, r_4) and it also needs the next state s_5 to bootstrap from.
          - Worker 2 does the same in its own, independent environment, and so on for all 8 workers.
          - This entire step happens in parallel.
        - **Step 2: Batch Aggregation**
          - The Coordinator waits for all 8 workers to finish their 5 steps.
          - It then gathers all the collected data into one large batch.
          - The total size of this batch is n_steps * num_workers = 5 * 8 = 40 data samples. Each sample is a tuple (s_t, a_t, r_t, s_{t+1}).
        - **Step 3: Calculation on the Batch**
          - Now, the Coordinator has a large batch of 40 samples. It performs the following calculations on this entire batch:
            - Calculate N-step Returns (y_t): For each of the 40 samples, it calculates the target value. Let's look at the very first sample from Worker 1, (s_0, a_0, r_0). To get its target y_0, the coordinator uses the future rewards that have already been collected in the batch:
              - $y_0 = r_0 + \gamma r_1 + \gamma^2 r_2 + \gamma^3 r_3 + \gamma^4 r_4 + \gamma^5 V_{\phi}(s_5)$        
            - The coordinator does this for every single one of the 40 samples, creating a vector of 40 target values.
            - Calculate Advantages (A_t): For each of the 40 samples, it now calculates the advantage:
              - $A_t = y_t - V_{\phi}(s_t)$
            - This results in a vector of 40 advantage values.
        - **Step 4: The Single Gradient Update**
          - The Coordinator now has everything it needs:
            - A batch of 40 states (s_t)
            - A batch of 40 actions (a_t)
            - A batch of 40 advantages (A_t)
            - A batch of 40 N-step targets (y_t)
          - It feeds this entire batch into the loss function:
            - $L_{\text{total}} = L_{\theta} + \alpha L_{\phi} - \beta L_{\text{entropy}}$
          - It then performs one single optimization step (e.g., `optimizer.step()`) using the gradient calculated from this total loss over the entire batch of 40 samples.
          - The loop then repeats from Step 1.

      </details>
