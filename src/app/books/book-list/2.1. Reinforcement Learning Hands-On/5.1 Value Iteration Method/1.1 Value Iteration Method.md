# Value Iteration Method

## High Level Idea

- Intro
  - Model-Based, Value-Based, Off-Policy
    <details><summary><i>Why?</i></summary>

    - _**Model-Based**_: known transition probabilities and rewards
    - _**Value-Based**_: calculate state values and action values
    - _**Off-Policy**_: use a fixed policy (typically random or epsilon-greedy) to generate training data, which means the **_policy used to collect experience can be different from the policy being learned_**. This separation allows for more efficient data collection and better exploration of the state space.

    </details>

  - with _**known**_ transition probabilities and rewards, calculate state values and action values
- **Steps**
  1. initialize the values of all states, $V_s$, to some initial value (usually zero)
  2. for every state $s$, perform Bellman update:
     - $V_s=\max\limits_{a} \sum\limits_{s' \in S} P(s' | s, a) (R(s' | s,a) + \gamma V_{s'})$
  3. Repeat step 2 for certain steps until stable
  - <details><summary><i>a small example</i></summary>

    - <img style="width:75%;max-width:200px;" src="/books/Reinforcement Learning Hands-On/value-iteration-small-example.png" />
    - $V(s1) = 1 + γ(2 + γ(1 + γ(2 + ...))) = \sum\limits_{i=0}^{\infty}\left(1\cdot γ^{2i} + 2\cdot γ^{2i+1}\right)=\frac{1+2 \gamma}{1-\gamma^2}$
    - $V(s2) = 2 + γ(1 + γ(2 + γ(1 + ...))) = \sum\limits_{i=0}^{\infty}\left(2 \cdot γ^{2i} + 1 \cdot γ^{2i+1}\right)$

    ```python
    # verify summation form with python
    >>> sum([0.9**(2*i) + 2*(0.9**(2*i+1)) for i in range(50)]) 
    14.736450674121663 
    >>> sum([2*(0.9**(2*i)) + 0.9**(2*i+1) for i in range(50)]) 
    15.262752483911719
    ```

    - **_How is the summation form deduced_**
      - step 1: expand the expression to identify the pattern:
        - First expansion:
          - $V(s1) = 1 + \gamma (2 + \gamma (\text{rest}))$
          - $V(s1) = 1 + \gamma \cdot 2 + \gamma^2 (1 + \gamma (\text{rest}))$
        - Second expansion:
          - $V(s1) = 1 + 2\gamma + \gamma^2 (1 + \gamma (2 + \gamma (\text{rest})))$
          - $V(s1) = 1 + 2\gamma + \gamma^2 \cdot 1 + \gamma^3 \cdot 2 + \gamma^4 (1 + \gamma (\text{rest}))$
        - Third expansion:
          - $V(s1) = 1 + 2\gamma + \gamma^2 + 2\gamma^3 + \gamma^4 + 2\gamma^5 + \dots$
      - step 2: observe the Pattern
        - From the expansion, we see that the coefficients alternate between $1$ and $2$:
          - The **even powers of $\gamma$** (i.e., $\gamma^0, \gamma^2, \gamma^4, \dots$) have coefficient **1**.
          - The **odd powers of $\gamma$** (i.e., $\gamma^1, \gamma^3, \gamma^5, \dots$) have coefficient **2**.
      - Thus, the series can be written as:
        $V(s1) = \sum_{i=0}^{\infty} \left( 1 \cdot \gamma^{2i} + 2 \cdot \gamma^{2i+1} \right)$

    </details>

- Limitations
  - state space must be **discrete and small** ❌
    - _(value iteration method can directly apply on FrozenLake, ~~but not directly applicable to CartPole~~)_
  - prior known transition probability and reward matrix is rare in practice
    - estimate with experience
      - for transition probability, maintain counters for $(s_0,s_1,a)$ and normalize
      - for reward, just memorize
- Comparison with CEM
  - Major speed improvement in training (fewer iterations) shown in Value Iteration vs. CEM, due to
    - Value Iteration looks at individual values of the state/action
    - while CEM looks at whole episodes, requiring at least one successful episode to start learning

## FrozenLake

- <img style="width:75%;max-width:400px;" src="/books/Reinforcement Learning Hands-On/value-iteration.jpg" />
- Central data structures
  - _**Reward table**_: `dict[(s0, a, s1), float]`
  - _**Transitions table**_: `dict[(s0,a), dict[s1, int]]`
  - _**Value table**_:
    - state table: `dict[s, float]`, or
    - action table: `dict[(s,a), float]`
      - <details><summary>implications</summary>

        - action table (Q-function) is more favorite due to simplicity in implementation
        - but action table version requires four times more memory for the value table

        </details>
- **Steps**
  - step 1-2: play 100 random steps from the environment, populating the reward and transition tables.
  - step 3: perform a value iteration loop over all states, updating our value table
  - step 4: play several full episodes to check our improvements using the updated value table. 
    - also update our reward and transition tables
    - stop training if the average reward for those test episodes is above the 0.8 boundary
- Implementations
  - State Value Table Version
    - <details><summary>Agent and Tables</summary>

      ```python
      State = int
      Action = int
      RewardKey = tt.Tuple[State, Action, State]
      TransitKey = tt.Tuple[State, Action]

      class Agent:
        def __init__(self):
            self.env = gym.make(ENV_NAME)
            self.state, _ = self.env.reset()
            self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)
            self.transits: tt.Dict[TransitKey, Counter] = defaultdict(Counter)
            self.values: tt.Dict[State, float] = defaultdict(float)

        def play_n_random_steps(self, n: int):
            for _ in range(n):
                action = self.env.action_space.sample()
                new_state, reward, is_done, is_trunc, _ = self.env.step(action)
                rw_key = (self.state, action, new_state)
                self.rewards[rw_key] = float(reward)
                tr_key = (self.state, action)
                self.transits[tr_key][new_state] += 1
                if is_done or is_trunc:
                    self.state, _ = self.env.reset()
                else:
                    self.state = new_state

        def calc_action_value(self, state: State, action: Action) -> float:
            target_counts = self.transits[(state, action)]
            total = sum(target_counts.values())
            action_value = 0.0
            for tgt_state, count in target_counts.items():
                rw_key = (state, action, tgt_state)
                reward = self.rewards[rw_key]
                val = reward + GAMMA * self.values[tgt_state]
                action_value += (count / total) * val
            return action_value

        def select_action(self, state: State) -> Action:
            best_action, best_value = None, None
            for action in range(self.env.action_space.n):
                action_value = self.calc_action_value(state, action)
                if best_value is None or best_value < action_value:
                    best_value = action_value
                    best_action = action
            return best_action

        def play_episode(self, env: gym.Env) -> float:
            total_reward = 0.0
            state, _ = env.reset()
            while True:
                action = self.select_action(state)
                new_state, reward, is_done, is_trunc, _ = env.step(action)
                rw_key = (state, action, new_state)
                self.rewards[rw_key] = float(reward)
                tr_key = (state, action)
                self.transits[tr_key][new_state] += 1
                total_reward += reward
                if is_done or is_trunc:
                    break
                state = new_state
            return total_reward

        def value_iteration(self):
            for state in range(self.env.observation_space.n):
                state_values = [
                    self.calc_action_value(state, action)
                    for action in range(self.env.action_space.n)
                ]
                self.values[state] = max(state_values)
      ```

      </details>

    - <details><summary>Whole Process</summary>

        ```python
        test_env = gym.make(ENV_NAME)
        agent = Agent()
        writer = SummaryWriter(comment="-v-iteration")

        iter_no = 0
        best_reward = 0.0
        while True:
            iter_no += 1
            agent.play_n_random_steps(100)
            agent.value_iteration()

            reward = 0.0
            for _ in range(TEST_EPISODES):
                reward += agent.play_episode(test_env)
            reward /= TEST_EPISODES
            writer.add_scalar("reward", reward, iter_no)
            if reward > best_reward:
                print(f"{iter_no}: Best reward updated {best_reward:.3} -> {reward:.3}")
                best_reward = reward
            if reward > 0.80:
                print("Solved in %d iterations!" % iter_no)
                break
        writer.close()
        ```
      </details>

  - Action Value Table Version
    - <details><summary>Agent and Tables</summary>

      ```python
      class Agent:
        def __init__(self):
            self.env = gym.make(ENV_NAME)
            self.state, _ = self.env.reset()
            self.rewards: tt.Dict[RewardKey, float] = defaultdict(float)
            self.transits: tt.Dict[TransitKey, Counter] = \
                defaultdict(Counter)
            self.values: tt.Dict[TransitKey, float] = defaultdict(float)

        def play_n_random_steps(self, n: int):
            for _ in range(n):
                action = self.env.action_space.sample()
                new_state, reward, is_done, is_trunc, _ = \
                    self.env.step(action)
                rw_key = (self.state, action, new_state)
                self.rewards[rw_key] = float(reward)
                tr_key = (self.state, action)
                self.transits[tr_key][new_state] += 1
                if is_done or is_trunc:
                    self.state, _ = self.env.reset()
                else:
                    self.state = new_state

        def select_action(self, state: State) -> Action:
            best_action, best_value = None, None
            for action in range(self.env.action_space.n):
                action_value = self.values[(state, action)]
                if best_value is None or best_value < action_value:
                    best_value = action_value
                    best_action = action
            return best_action

        def play_episode(self, env: gym.Env) -> float:
            total_reward = 0.0
            state, _ = env.reset()
            while True:
                action = self.select_action(state)
                new_state, reward, is_done, is_trunc, _ = \
                    env.step(action)
                rw_key = (state, action, new_state)
                self.rewards[rw_key] = float(reward)
                tr_key = (state, action)
                self.transits[tr_key][new_state] += 1
                total_reward += reward
                if is_done or is_trunc:
                    break
                state = new_state
            return total_reward

        def value_iteration(self):
            for state in range(self.env.observation_space.n):
                for action in range(self.env.action_space.n):
                    action_value = 0.0
                    target_counts = self.transits[(state, action)]
                    total = sum(target_counts.values())
                    for tgt_state, count in target_counts.items():
                        rw_key = (state, action, tgt_state)
                        reward = self.rewards[rw_key]
                        best_action = self.select_action(tgt_state)
                        val = reward + GAMMA * self.values[(tgt_state, best_action)]
                        action_value += (count / total) * val
                    self.values[(state, action)] = action_value
      ```

      </details>
    - <details><summary>Whole Process</summary>

        ```python
        test_env = gym.make(ENV_NAME)
        agent = Agent()
        writer = SummaryWriter(comment="-q-iteration")

        iter_no = 0
        best_reward = 0.0
        while True:
            iter_no += 1
            agent.play_n_random_steps(100)
            agent.value_iteration()

            reward = 0.0
            for _ in range(TEST_EPISODES):
                reward += agent.play_episode(test_env)
            reward /= TEST_EPISODES
            writer.add_scalar("reward", reward, iter_no)
            if reward > best_reward:
                print(f"{iter_no}: Best reward updated "
                    f"{best_reward:.3} -> {reward:.3}")
                best_reward = reward
            if reward > 0.80:
                print("Solved in %d iterations!" % iter_no)
                break
        writer.close()
        ```

      </details>
