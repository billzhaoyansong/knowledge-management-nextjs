# Gymnasium API

- https://github.com/Farama-Foundation/Gymnasium

## Critical Entities

- `Env`
  - `action_space`: all valid actions
  - `observation_space`: all valid observations
  - `step(action)`: update an environment with actions, returns a tuple of
    - `observation`: an element of the environment’s observation_space as the next observation due to the agent actions
    - `reward: float`: reward as a result of taking the action
    - `terminated: bool`: whether the agent reaches the terminal state. If true, the user needs to call `reset()`.
    - `truncated: bool`: whether the truncation condition outside the scope of the MDP is satisfied , such as timelimit and agent physically going out of bounds. If true, the user needs to call `reset()`.
    - `info: dict`: auxiliary diagnostic information (helpful for debugging, learning, and logging)
  - `reset()`: reset the environment to an initial state
- `Space`
  - `sample()`: This returns a random sample from the space.
  - `contains(x)`: This checks whether the argument, `x`, belongs to the space’s domain.
- <details><summary>Class Diagram</summary>

    ```mermaid
    ---
    config:
        look: neo
        layout: elk
    ---
    classDiagram
    direction LR
        class Env {
            +metadata: dict[str, Any]
            +render_mode: str
            +spec: EnvSpec
            +**step**(action: ActType) tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]
            +**reset**(*, seed: int | None = None, options: dict[str, Any] | None = None) tuple[ObsType, dict[str, Any]]
            +render() RenderFrame | list[RenderFrame] | None
            +close()
        }
        class Space {
            +shape: tuple[int, ...] | None
            +dtype
            +is_np_flattenable: bool
            +np_random: Generator
            +**sample**(mask = None, probability = None) T_cov
            +**contains**(x) bool
            +**seed**(seed: int | None = None) int | list[int] | dict[str, int]
            +to_jsonable(sample_n: Sequence[T_cov]) list[Any]
            +from_jsonable(sample_n: list[Any]) list[T_cov]
        }
        class Box {
            +low: float
            +high: float
            +is_bounded(manner: str = 'both'|'below'|'above') bool
        }
        class Discrete {
            +n: int(#elements of space)
        }
        class MultiBinary {
        }
        class MultiDiscrete {
        }
        class Text {
        }
        class Dict {
            +spaces: None | dict[str, Space] | Sequence[tuple[str, Space]]
        }
        class Tuple {
            +spaces: Iterable[Space[Any]]
        }
        class Sequence {
            +spaces: Space[Any]
        }
        class Graph {
            +node_space: Union[Box, Discrete]
            +edge_space: Union[None, Box, Discrete]
        }
        class OneOf {
            +spaces: Iterable[Space[Any]]
        }

        Env "1" --> Space : +**action_space**
        Env "1" --> Space : +**observation_space**
        Box --|> Space
        Discrete --|> Space
        MultiBinary --|> Space
        MultiDiscrete --|> Space
        Text --|> Space
        Dict --|> Space
        Tuple --|> Space
        Sequence --|> Space
        Graph --|> Space
        OneOf --|> Space

        class Env:::Peach
        class Space:::Pine
        class Box:::Aqua
        class Discrete:::Aqua
        class MultiBinary:::Aqua
        class MultiDiscrete:::Aqua
        class Text:::Aqua
        class Dict:::Aqua
        class Tuple:::Aqua
        class Sequence:::Aqua
        class Graph:::Aqua
        class OneOf:::Aqua

        classDef Peach :, stroke-width:1px, stroke-dasharray:none, stroke:#FBB35A, fill:#FFEFDB, color:#8F632D
        classDef Pine :, stroke-width:1px, stroke-dasharray:none, stroke:#254336, fill:#27654A, color:#FFFFFF
        classDef Aqua :,stroke-width:1px, stroke-dasharray:none, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A
    ```

    </details>

## Critical Codes

- **Create Environment**
  - ```python
    import gymnasium as gym
    env = gym.make("CartPole-v1")

    # with HumanRendering, which opens a separate graphical window 
    # in which the image from the environment is being shown interactively
    # MUST invoke env.close() at last
    env = gym.make("CartPole-v1", render_mode="rgb_array") 
    env = gym.wrappers.HumanRendering(env)

    # with RecordVideo, which produces a video file
    # MUST invoke env.close() at last
    env = gym.make("CartPole-v1", render_mode="rgb_array") 
    env = gym.wrappers.RecordVideo(env, video_folder="video")
    ```
  - for an extensive list of predefined environments,refer to [classic](https://gymnasium.farama.org/environments/classic_control/), [box2d](https://gymnasium.farama.org/environments/box2d/), [toy text](https://gymnasium.farama.org/environments/toy_text/), [mujoco](https://gymnasium.farama.org/environments/mujoco/), [Atari](https://ale.farama.org/environments/), [External Environments](https://gymnasium.farama.org/environments/third_party_environments/)
- **Create Agent**
  - ```python
    total_reward, total_steps = 0.0, 0
    obs, _ = env.reset()

    while True: 
        action = env.action_space.sample() 
        obs, reward, is_done, is_trunc, _ = env.step(action) 
        total_reward += reward 
        total_steps += 1 
        if is_done: 
            break 
 
    print("Episode done in %d steps, total reward %.2f" % (total_steps, total_reward))
    env.close()
    ```

## Wrapper

- What is it?
  - Wrappers are a convenient way to **modify an existing environment** without having to alter the underlying code directly
  - <details><summary>Wrappers Tables</summary>

    - [**Action Wrappers**](https://gymnasium.farama.org/api/wrappers/action_wrappers/#available-action-wrappers)

      - 
        | Name                                           | Description                                                                                                                  |
        | ---------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
        | `TransformAction(env, func, action_space)`     | Applies a function to the action before passing the modified value to the environment step function                          |
        | `ClipAction(env)`                              | Clips the action pass to step to be within the environment’s action_space                                                    |
        | `RescaleAction(env, min_action, max_action)`   | Affinely (linearly) rescales a Box action space (a must) of the evironment to within the range of `[min_action, max_action]` |
        | `StickyAction(env, repeat_action_probability)` | Adds a probability that the action is repeated for the same step function                                                    |
    - [**Observation Wrappers**](https://gymnasium.farama.org/api/wrappers/observation_wrappers/#observation-wrappers)
  
      - 
        | Name                                                                                                                          | Description                                                                                                                           |
        | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
        | `TransformObservation(env, func, observation_space)`                                                                          | Applies a function to the observation received from the environment’s `Env.reset()` and `Env.step()` that is passed back to the user. |
        | `DelayObservation(env, delay)`                                                                                                | Adds a delay to the returned observation from the environment.                                                                        |
        | `DtypeObservation(env, dtype)`                                                                                                | Modifies the dtype of an observation array to a specified dtype.                                                                      |
        | `FilterObservation(env, filter_keys: Sequence[str \| int])`                                                                   | Filters a `Dict` or `Tuple` observation spaces by a set of keys or indexes                                                            |
        | `FlattenObservation(env)`                                                                                                     | Flattens the environment’s observation space and each observation from reset and step functions.                                      |
        | `FrameStackObservation(env, stack_size, padding_type = 'reset'(default) \| 'zero' \| custom)`                                 | Stacks the observations from the last N time steps in a rolling manner.                                                               |
        | `GrayscaleObservation(env: Env[ObsType, ActType], keep_dim: bool = False)`                                                    | Converts an image observation computed by reset and step from RGB to Grayscale.                                                       |
        | `MaxAndSkipObservation(env, skip: int = 4)`                                                                                   | Skips the N-th frame (observation) and return the max values between the two last observations.                                       |
        | `NormalizeObservation(env, epsilon: float = 1e-8)`                                                                            | Normalizes observations to be centered at the mean with unit variance.                                                                |
        | `AddRenderObservation(env, render_only: bool = True, render_key: str = 'pixels', obs_key: str = 'state')`                     | Includes the rendered observations in the environment’s observations.                                                                 |
        | `ResizeObservation(env: Env[ObsType, ActType], shape: tuple[int, int])`                                                       | Resizes image observations using OpenCV to a specified shape.                                                                         |
        | `ReshapeObservation(env, shape: int \| tuple[int, ...])`                                                                      | Reshapes Array based observations to a specified shape.                                                                               |
        | `RescaleObservation(env, min_obs: np.floating \| np.integer \| np.ndarray, max_obs: np.floating \| np.integer \| np.ndarray)` | Affinely (linearly) rescales a Box observation space of the environment to within the range of `[min_obs, max_obs]`.                  |
        | `TimeAwareObservation(env, flatten: bool = True, normalize_time: bool = False, *, dict_time_key: str = 'time')`               | Augment the observation with the number of time steps taken within an episode.                                                        |

    - [**Reward Wrappers**](https://gymnasium.farama.org/api/wrappers/reward_wrappers/)
  
      - 
        | Name                                                 | Description                                                                                                  |
        | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
        | `TransformReward(env, func)`                         | Applies a function to the reward received from the environment’s step.                                       |
        | `NormalizeReward(env, gamma = 0.99, epsilon = 1e-8)` | Normalizes immediate rewards such that their exponential moving average has an approximately fixed variance. |
        | `ClipReward(env, min_reward, max_reward)`            | Clips the rewards for an environment between an upper and lower bound.                                       |
    </details>

