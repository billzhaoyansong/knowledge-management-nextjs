# 11.1 Introduction

- _Gradient descent_ is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems
- it tweaks parameters iteratively in order to minimize a cost function

## The process

1. it initializes the parameter vector $\mathbf{\theta}$ of the cost function (e.g. MSE) with random values (random initialization)
2. repeatedly:
   - it measures the local gradient according to $\mathbf{\theta}$
   - it goes in the direction of descending gradient
3. once the gradient is $0$, it reaches a minimum

## Cautions:

- for __a convext function with a slope that never changes abruptly__ (its derivative is Lipschitz continuous), applying a gradient descent is guaranteed to approach arbitrarily closely the global minimum
- __step-size, or learning rate__, can affect the training speed gravely
- The more parameters a model has, the more dimensions this __parameter space__ has, and the harder the search is
  - searching for a needle in a 300-dimensional haystack is much trickier than in 3 dimensions
- if the features have very different scales, the cost function has the shape of an elongated bowl (rationale for feature scaling)

    <img width="30%" src="books\Hands-On ML\gd-elongated-bowel.png" />

---
