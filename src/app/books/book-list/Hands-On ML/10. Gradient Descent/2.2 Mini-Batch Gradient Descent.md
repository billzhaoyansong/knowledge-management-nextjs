# 11.4 Mini-Batch Gradient Descent

- philosophy:
  - mini-batch GD computes the gradients on small random sets of instances called mini-batches
  - main advantage of mini-batch GD over stochastic GD:
    - get a performance boost from hardware optimization of matrix operations, especially when using GPUs
    - The algorithmâ€™s progress in parameter space is less erratic than with stochastic GD

---
