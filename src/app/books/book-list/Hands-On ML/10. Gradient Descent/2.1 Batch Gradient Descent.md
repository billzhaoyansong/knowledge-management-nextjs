# 11.2 Batch Gradient Descent

- philosophy:
  - at each gradient descent step, batch gradient descent used the __whole batch of training data__ to calculate gradient (actually, _full gradient descent_ would probably be a better name)
- pros: gradient descent scales well with the number of features
  - training a linear regression model when there are hundreds of thousands of features is much faster using gradient descent than using the Normal equation or SVD decomposition
- cons: it is _terribly slow on very large training sets_
  
## Applying on a linear regression model

- take mean-square-error as the cost function
- __tweak parameters__ based on gradient descent
  - $\mathbf{\theta}^{\text{(next step)}}=\mathbf{\theta}-\eta \nabla_{\mathbb{\theta}}\text{MSE}(\mathbf{\theta})$
    - $\eta$: learning rate
    - gradient vector of the cost function
      - $\nabla_{\mathbb{\theta}}\text{MSE}(\mathbf{\theta})=\begin{pmatrix}\frac{\partial}{\partial\theta_0}\text{MSE}(\theta)\\\frac{\partial}{\partial\theta_1}\text{MSE}(\theta)\\\vdots\\\frac{\partial}{\partial\theta_n}\text{MSE}(\theta) \end{pmatrix}=\frac{2}{m}\mathbf{X}^\intercal(\mathbf{X}\mathbf{\theta}-\mathbf{y})$
        - $m$: number of samples
        - $n$: number of features

  ```python
  eta = 0.1  # learning rate
  n_epochs = 1000
  m = len(X_b)  # number of instances

  np.random.seed(42)
  theta = np.random.randn(2, 1)  # randomly initialized model parameters

  for epoch in range(n_epochs):
      # The @ operator performs matrix multiplication. 
      # If A and B are NumPy arrays, then A @ B is equivalent to np.matmul(A, B)
      # cannot use @ on pure Python arrays (i.e., lists of lists).
      gradients = 2 / m * X_b.T @ (X_b @ theta - y)
      theta = theta - eta * gradients
  ```

  - __epoch__: one complete pass through the entire training dataset
- find the __learning rate__
  - use grid search
  - limit the number of epochs so that grid search can eliminate models that take too long to converge, specifically
    - to set a very large number of epochs
    - to interrupt the algorithm when the gradient vector becomes tiny (norm becomes smaller than a tiny number $\epsilon$ (called the __tolerance__))
    - for a fixed learning rate, the iterations to reach the optimum with a tolerance $\epsilon$: $O(1/\epsilon)$