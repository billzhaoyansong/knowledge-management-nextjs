# 7.4 Performance Measures

- <details><summary>❌ Accuracy</summary>

  - accuracy is generally __not__ the preferred performance measure for _classifiers_, especially when you are dealing with skewed datasets
  - $\text{accuracy}=\frac{TP+TN}{TP+TN+FP+FN}$

    <details><summary>an example to show the reason</summary>

    ```python
    from sklearn.model_selection import cross_val_score
    cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")

    from sklearn.dummy import DummyClassifier

    dummy_clf = DummyClassifier()
    dummy_clf.fit(X_train, y_train_5)

    # this dummy classifier will give an accuracy slightly worse than SGD classifier. 
    # but we confirm that dummy classifier is absolutely worse
    # so we need a new performance measures
    cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring="accuracy")
    ```

    </details>
  </details>

- ✅ __Precision__
  - $\text{precision}=\frac{TP}{TP+FP}$ (the accuracy in the predicted positives)
    - in some contexts, you really care about precision more
    - e.g. a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision)

    <details><summary>A trivial way to have perfect precision</summary>

    - A trivial way to have perfect precision is to create a classifier that always makes negative predictions, except for one single positive prediction on the instance it’s most confident about.
    - If this one prediction is correct, then the classifier has 100% precision (precision = 1/1 = 100%).
    - so, precision is typically used along with _recall_

    </details>

- ✅ __Recall__ (also called sensitivity or the true positive rate (TPR))
  - $\text{recall}=\frac{TP}{TP+FN}$ (the accuracy in the actual positives)
    - in some contexts, you really care about recall more
    - e.g. a classifier to detect shoplifters in surveillance images: it is probably fine if your classifier only has 30% precision as long as it has 99% recall (many false alerts, but almost all shoplifters will get caught).

- ✅ __$F_1$ score__
  - $F_1=\frac{2}{\frac{1}{\text{precision}}+\frac{1}{\text{recall}}}$
    - $F_1$ score is the _harmonic mean_ of precision and recall (gives much more weight to low values)
    - classifier will only get a high $F_1$ score if both recall and precision are high

```python
from sklearn.metrics import precision_score, recall_score, f1_score

precision_score(y_train_5, y_train_pred)  
# == 3530 / (687 + 3530) = 0.8370879772350012

recall_score(y_train_5, y_train_pred)  
# == 3530 / (1891 + 3530) = 0.6511713705958311

f1_score(y_train_5, y_train_pred)
# 0.7325171197343846
```