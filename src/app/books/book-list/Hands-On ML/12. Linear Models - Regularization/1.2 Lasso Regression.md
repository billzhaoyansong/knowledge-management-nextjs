# 13.3 Lasso Regression

- **L**east **a**bsolute **s**hrinkage and **s**election **o**perator regression
- the cost function for Lasso regression
  - $J(\bm{\theta})=\text{MSE}(\bm{\theta})+2\alpha\sum_{i=1}^n|\theta_i|$

    <img src="books\Hands-On ML\lasso-regression-comparison.png" />

    - on the left, _linear regression_ with lasso regularization
    - on the right, _polynomial regression_ with lasso regularization
  - lasso regression tends to eliminate the weights of the least important features
    - it automatically performs feature selection
- Lasso vs Ridge, and why lasso performs feature selection
  
  <img width="60%" src="books\Hands-On ML\lasso vs ridge.png" />

  - In the top-left plot,
    - the contours represent the $ℓ_1$ loss ($|θ_1| + |θ_2|$), which drops linearly as you get closer to any axis. 
    - For example, if you initialize the model parameters to $θ_1 = 2$ and $θ_2 = 0.5$, running gradient descent will decrement both parameters equally (as represented by the dashed yellow line); 
    - therefore $θ_2$ will reach 0 first (since it was closer to 0 to begin with). 
    - After that, gradient descent will roll down the gutter until it reaches $θ_1 = 0$ (with a bit of bouncing around, since the gradients of ℓ1 never get close to 0: they are either –1 or 1 for each parameter).
  - In the top-right plot,
    - the contours represent lasso regression’s cost function (i.e., an MSE cost function plus an $ℓ_1$ loss).
    - The small white circles show the path that gradient descent takes to optimize some model parameters that were initialized around $θ_1 = 0.25$ and $θ_2 = –1$: notice once again how the path quickly reaches $θ_2 = 0$, then rolls down the gutter and ends up bouncing around the global optimum (represented by the red square).
    - If we increased $\alpha$, the global optimum would move left along the dashed yellow line, 
    - if we decreased $\alpha$, the global optimum would move right (in this example, the optimal parameters for the unregularized MSE are $θ_1 = 2$ and $θ_2 = 0.5$).
  - In the bottom-left plot,
    - you can see that the $ℓ_2$ loss decreases as we get closer to the origin, so gradient descent just takes a straight path toward that point.
  - In the bottom-right plot,
    - the contours represent ridge regression’s cost function (i.e., an MSE cost function plus an $ℓ_2$ loss). As you can see, the gradients get smaller as the parameters approach the global optimum, so gradient descent naturally slows down. This limits the bouncing around, which helps ridge converge faster than lasso regression.
- with lasso class

```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
```

- with SGD

```python
SGDRegressor(penalty="l1", alpha=0.1)
```
