# 14.2 Softmax Regression (Multinomial Logistic Regression)

- generalized logistic regression

## The Model

- softmax score for class $k$
  - $s_k(\mathbf{x})=\left(\bm{\theta}^{(k)}\right)^T\mathbf{x}$
- softmax function for class $k$
  - $\hat{p_k}=\sigma(\mathbf{s(x)})_k=\frac{\exp s_k(\mathbf{x})}{\sum_{j=1}^K \exp s_j(\mathbf{x})}\in [0,1]$
- softmax regression classifier prediction
  - $\hat{y}=\argmax_k \sigma(\mathbf{s(x)})_k$

## Cross Entrophy Cost Function

- $J(\mathbf{\Theta})=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log\left(\hat{p_k}^{(i)}\right)$
  - when $K=2$, the cost function is equivalent to the logistic regression cost function
  - cross entropy gradient vector for class $k$
    - $\nabla_{\bm{\theta}^{(k)}}J\left(\mathbf{\Theta}\right)=\frac{1}{m}\left(\hat{p_k}^{(i)}-y_k^{(i)}\right)\mathbf{x}^{(i)}$
  
## The Code

```python
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = iris["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

softmax_reg = LogisticRegression(C=30, random_state=42)
softmax_reg.fit(X_train, y_train)

softmax_reg.predict([[5, 2]])
softmax_reg.predict_proba([[5, 2]]).round(2) # array([[0.  , 0.04, 0.96]])
```