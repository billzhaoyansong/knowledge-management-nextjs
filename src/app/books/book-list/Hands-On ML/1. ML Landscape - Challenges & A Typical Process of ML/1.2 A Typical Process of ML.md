# 2.2 A Typical Process of A ML Project

## The Process

1. You studied the data.
2. You selected a model.
3. You trained it on the training data (i.e., the learning algorithm searched for the model parameter values that minimize a cost function).
4. Finally, you applied the model to make predictions on new cases (this is called inference), hoping that this model will generalize well.
   - if not, try to use more attributes (employment rate, health, air pollution, etc.), get more or better-quality training data, or perhaps select a more powerful model (e.g., a polynomial regression model).

## Testing and Validating

- in step 3, the problem of using 1 __training set__ and 1 __test set__ is that, the best model is adapted for that particular test set
  - solution: holdout validation: holdout part of the training set as __validation set__ or __development(dev) set__, steps:
    1. train multiple ML models on the reduced training set(_oringinal training set - validation set_)
    2. select the model that performs best on the validation set
    3. train the best model with full training set(_reduced training set + validation set_)
    4. evaluate the best model with test set to estimate the generalization error
- the selection of the validation set is also a challenge
  - if it is too small, the the model evaluations will be imprecise
  - if it is too large, the traning set will be too small
  - solution: __cross-validation__
    - using many small validation sets. Each model is evaluated once per validation set after it is trained on the rest of the data.
    - By averaging out all the evaluations of a model, you get a much more accurate measure of its performance.
    - drawback: the training time is multiplied by the number of validation sets.

## Data Mismatch

- in the case of large amount of data for training, but this data probably won’t be perfectly representative of the data that will be used in production.
  - e.g. a mobile app to take pictures of flowers and automatically determine their species. You can easily download millions of pictures of flowers on the web, but they won’t be perfectly representative of the pictures that will actually be taken using the app on a mobile device.
  - the validation set and test set must be as representative as possible of the data you expect to use in production
  - although you can shuffle you data and put half in the validation set and half in the test set, if your model doesn't perform well in production, you will not know whether this is because of overfitting, or whether this is just due to the _data mismatch_ between the web pictures and the mobile app pictures
- solution: holdout another part of the training set as __train-dev set__, steps:
  1. train your model on the residual training set
  2. evaluate on the _train-dev set_
     - if it performs poorly, must be overfitting, so you should try to simplify or regularize the model, get more training data, and clean up the training data
     - if it performs well, then you can proceed
  3. evaluate the model on the _dev set_
     - if it performs poorly, then the problem must be coming from the data mismatch
       - to tackle this problem by preprocessing the web images to make them look more like the pictures that will be taken by the mobile app, and then retraining the model
     - if it performs well, then you can proceed
  4. evaluate it one last time on the _test set_ to know how well it is likely to perform in production
