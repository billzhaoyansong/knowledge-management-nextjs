# Ratings

## Ratings

- User-Item Matrix:
  - A foundational structure where rows represent **users**, columns represent **items**, and cells indicate preferences (e.g., ratings or interactions).  
  - **Sparsity Problem**: Most entries are empty due to limited user-item interactions, posing challenges for recommendation algorithms.
- Explicit vs. Implicit Ratings
  - **Explicit Ratings**: Direct user feedback (e.g., star ratings). While straightforward, these can be biased or sparse.  
  - **Implicit Ratings**: Derived from **user behavior** (e.g., clicks, purchases, dwell time). More reliable but require interpretation.  
    - Calculating Implicit Ratings
      - **Binary Approach**: Marks interactions as present (1) or absent (0). Simple but lacks nuance.  
      - **Time-Decay**: Prioritizes recent interactions (e.g., Hacker News’ gravity formula).  
      - **Behavior-Based**: Combines weighted events (e.g., `buy`, `details`, `moredetails`) into a composite score:  
       $$
       IR_{i,u} = w_1 \cdot \#\text{buy} + w_2 \cdot \#\text{moredetails} + w_3 \cdot \#\text{details}
       $$
      - **Normalization**: Scales ratings to a standard range (e.g., 0–10) for consistency.

## Others

- Implementation
  - **Data Retrieval**: SQL queries aggregate user interactions (e.g., counting `buy` events per user-item pair).  
  - **Machine Learning**: Framing rating prediction as a classification problem (e.g., using naïve Bayes classifiers).  
- Practical Considerations
  - **Event Relevance**: Not all interactions equally indicate preference (e.g., repeated clicks vs. purchases).  
  - **Cut-Offs**: Limiting event counts to avoid skew (e.g., `min(event_count, max_threshold)`).