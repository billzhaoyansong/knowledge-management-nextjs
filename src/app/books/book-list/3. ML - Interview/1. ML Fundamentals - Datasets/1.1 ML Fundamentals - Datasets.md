# ML Fundamentals - Datasets

- [ML Fundamentals - Datasets](#ml-fundamentals---datasets)
  - [1. How does one collect data and prepare a dataset for training?](#1-how-does-one-collect-data-and-prepare-a-dataset-for-training)
  - [2. What are some common problems in collecting data for training a model?](#2-what-are-some-common-problems-in-collecting-data-for-training-a-model)
  - [3. How to determine whether collected data is suitable for modeling?](#3-how-to-determine-whether-collected-data-is-suitable-for-modeling)
  - [4. What are several ways to handle label imbalance in a dataset?](#4-what-are-several-ways-to-handle-label-imbalance-in-a-dataset)
  - [5. How to deal when labels are missing?](#5-how-to-deal-when-labels-are-missing)


## 1. How does one collect data and prepare a dataset for training?

- **Answer**:
  1. **Collect the data**: Gather data you need to train the model. The data can come from existing streams generated by your system, such as user behavior, or from new data collections through conducting user research. Make sure to employ appropriate **sampling techniques** – such as random or stratified sampling – based on the task you want to perform. Evaluate the significance of streaming data into the model versus batching
  2. **Clean the data**: Check for _missing_ or _duplicate_ data, identify _outliers_, remove _irrelevant_ information and correct any errors. 
     - If feature _values are missing_, data points may be discarded or imputed using an approach like mean-value imputation, or more sophisticated techniques.
  3. **Label the data**: Assign labels to the data if the task is supervised learning. This may involve collecting user interaction data, or possibly hiring human annotators.
  4. **Split the data**: Divide the data into train, validation, and test sets. The training set is used to train the model, the validation set is used to evaluate the performance of the model during training (e.g., for early stopping, hyperparameter tuning), while the test set is used to evaluate model performance after training. Training and validation sets can be folded into a single dataset if employing techniques like cross-validation. The test set provides an unbiased estimate of model performance, which can also be used to compare different modeling approaches
  5. **Preprocess the data**: Apply necessary preprocessing steps to the data, such as _**normalization, scaling or transforming**_ the data into a format suitable for the model.
  6. **Check for balance**: Imbalanced data occurs when one class is represented by a disproportionately large number of samples relative to the other classes. This can lead to problems during training because the model is likely to focus on the majority class and ignore the minority class. When working with an imbalanced dataset, first try training on the true distribution. If the model is able to learn and generalize well on the true distribution, the problem is likely not severe and no further action may be required.
  7. **Shuffle the data**: Shuffle the data to reduce bias and ensure the model is not learning any order-based information, unless that is the objective of your training

## 2. What are some common problems in collecting data for training a model?

- **Answer**
  1. **Collecting the data**: Be aware of the biases that your data collection technique introduces, such as serving bias, and apply appropriate countermeasures, such as bandit approaches. Improper sampling may result in the dataset becoming narrow or homogeneous, leading to poor generalization and limited diversity in predictions
  2. **Cleaning the data**: Techniques used to clean the data, such as outlier and duplicate detection, and missing value imputation, may affect the relevance of the dataset – particularly if the data are not missing at random – and may negatively impact the training of the model
  3. **Labeling the data**: Beware of noise that this step introduces, such as ambiguous annotation instructions, or misalignment between annotators. Consider employing techniques like weighted voting or annotation validation, to reduce annotation error.
  4. **Splitting the data**: Ensure proper separation of data to prevent contamination between the training, validation, and test sets. For instance, consider splitting the data by dates where the training data includes all data up to a date and the test data includes data from the following date. 
  5. **Preprocessing the data**: Improper normalization may affect the utility of certain features. For instance, normalization of counting features where the vast majority of occurrences are zero may be detrimental to model training.
  6. **Checking for balance**: The distribution of classes in the data can affect the model's performance. Models may converge slowly, or be biased towards the majority class and have difficulty recognizing minority classes. Techniques like oversampling and undersampling can be used to address this, but issues may remain, such as poorly calibrated probabilities from model outputs. One solution is to upweight the downsampled class to improve calibration


## 3. How to determine whether collected data is suitable for modeling?

- **Answer**
  - **Quantity** is important because it directly impacts the model's performance. Having a large dataset allows the model to learn more patterns and generalize better. In general, a dataset should have an order of magnitude more data than the number of parameters in the model. Simple models trained on large datasets often outperform complex models trained on small datasets. Google has had success training simple linear regression models on large datasets ( The Size and Quality of a Data Set | Machine Learning, 2022).
  - **Quality** is equally important because unreliable data can negatively impact the model's performance. There are several reasons why a dataset might be unreliable:
    - Missing values in either the features or labels. This can be due to a variety of factors, such as human error in data collection or corrupted data.
    - Rows in the dataset may be duplicated, which can skew the results and lead to overfitting.
    - Feature values may be incorrect, which can occur if there is a disparity between the training feature distribution and the prediction feature distribution. Reuse code between the training pipeline and serving pipeline where possible
    - Labels may be incorrect, which can happen if there are mislabeled data or errors in the data collection process.
  - Choosing an appropriate **sampling strategy** may be important for constructing high-quality datasets. The sampling strategy that you choose should align with the specific goals and metrics of your model. For instance, if your objective is to build a ranking model, then random sampling may not be sufficient. Instead, you may need to sample data in a way that reflects the ranking metrics that you want to evaluate, such as at the query level

## 4. What are several ways to handle label imbalance in a dataset?

- **Answer**
  1. **Resampling**
     - Oversampling the minority class: Duplicating samples from the minority class until the classes are balanced.
     - Downsampling the majority class: Removing samples from the majority class until the classes are balanced, which can lead to faster model convergence and reduced storage requirements. Additionally, it may be beneficial to _upweight the majority class after downsampling_, as this can help to maintain model calibration throughout training.
  2. **Synthetic data generation** 
     - Creating new synthetic samples of the minority class to balance the dataset. This can be done using techniques like Synthetic Minority Over-sampling Technique (SMOTE)
  3. **Cost-sensitive learning**: 
     - Assigning different weights to samples from different classes to account for the imbalance in the loss function used to train the model.
  4. **Ensemble methods**
     - Using multiple models with different sampling strategies or using an ensemble of models trained on different subsets of the data with boosting or bagging.
  5. **Precision-oriented modeling**:
      - Some models are better suited for evaluation metrics sensitive to imbalanced datasets, such as precision or precision@k. For example, in decision trees and their variants – like random forest, boosted trees, etc. – one can prune paths with high entropy or with sparse samples. The trees can also be broken down into decision lists. In certain Natural Language Understanding (NLU) scenarios like domain-constrained intent classification, even rule-based models can perform effectively. Alternatively, models that generate probability-based outputs, such as logistic regression, can be thresholded to attain the desired performance outcomes.

## 5. How to deal when labels are missing?

- **Answer**
  - **Annotate the data**: This is the most direct approach for providing accurate labels, however, can be expensive and time consuming.
  - **Remove missing labels**: This approach is suitable for datasets with small amounts of missing data, but it may result in loss of information.
  - **Impute missing labels**: Replace missing values with estimated values based on the available data, such as mean or mode. For example, in a highly unbalanced dataset (e.g., web clicks) the majority class may be imputed. However, the imputed values may also introduce bias into the dataset, leading to a decrease in performance.
  - Predict missing labels with modeling (aka. **“Induction”**): This involves training a model (self-training), or multiple models (co-training), on the available data and then using them to predict missing labels. The downside is that the trained models may reinforce poor predictions.
  - Predict missing labels with separation (aka. **“Transduction”**): Instead of building a model only on labeled data, consider both labeled and unlabeled data to predict missing labels. This can be achieved several ways: (a) clustering algorithms with partial supervision, (b) graph-based methods like label propagation algorithms (LPA), and (c) manifold learning which encourages neighboring points in a lower-dimensional space to have similar predictions.