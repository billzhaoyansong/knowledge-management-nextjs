# ML Fundamentals - Featuers

- [ML Fundamentals - Featuers](#ml-fundamentals---featuers)
  - [1. Can you describe different types of input features and their intuitions?](#1-can-you-describe-different-types-of-input-features-and-their-intuitions)
  - [2. What is feature selection / importance?](#2-what-is-feature-selection--importance)
  - [3. How to perform feature selection?](#3-how-to-perform-feature-selection)
  - [4. How would you deal with missing feature values?](#4-how-would-you-deal-with-missing-feature-values)

## 1. Can you describe different types of input features and their intuitions?

- **Answer**:
  - **Numerical features**: Features expressed by continuous or discrete numerical values, such as amounts, counts, rates, and durations.
  - **Categorical features**: Features expressed by categories, such as keywords, device, demographic, language, location, IDs, etc. Categorical features can be represented as one-hot, multi-hot, or weighted (e.g., frequencies).
  - **Multi-modal examples**:
    - _**Binary features**_: Features expressed by yes/no or true/false values, such as membership (“is-a”), traits (“has-a”), and thresholds (“greater-than”, “less-than”). These can be represented as either numerical or categorical features.
    - _**Text features**_: Text data are frequently transformed into embeddings, which are numerical vectors, or represented as one-hot or multi-hot categorical features.
    - _**Image features**_: Image data are preprocessed into numerical arrays, such as raw pixel values (e.g., RGB or HSV) or modified versions of the images created by applying various filters like thresholding, blurring, or equalization.
    - _**Audio features**_: Sounds are preprocessed into numerical representations such as frequency components of the audio signal, or Mel-Frequency Cepstral Coefficients (MFCCs).
    - _**Sequential features**_: These are features representing time-series data, such as stock prices. Sequential features can be numerical or categorical.
    - _**Video features**_: Video features can be represented as sequential image features. Another approach, optical flow features track pixel intensities across video frames.

## 2. What is feature selection / importance?

- **Answer**:
  - **Feature selection** is the process of identifying a subset of the most relevant and useful features (predictive variables) from a larger set of features to be used in model building.
  - One goal of feature selection is to improve the model's predictive performance and reduce the complexity and overfitting of the model by removing irrelevant, redundant, or noisy features. Another goal is to reduce the footprint of the model and streamline the feature hydration process, thereby improving training and serving efficiency.
  - **Feature importance** refers to a numerical value or score that indicates the contribution of each feature to the prediction performance of the model. It is used to rank and select the most important features for a given task.

## 3. How to perform feature selection?

- **Answer**:
  - **Three approaches:**
    - **Filter**: Features are selected based on **statistical tests** such as chi-squared test, ANOVA, mutual information, information gain, correlation coefficient, and others.
    - **Intrinsic**: Features are selected as part of the modeling process. For instance, the Random Forest method (Palczewska et al., 2013) trains models on subsets of features where the expected fraction of samples contributed by a split is an estimate of the relative importance of the feature. An alternative approach is to incorporate L1 regularization during the modeling process, which reduces weights of less informative features to zero.
    - **Wrapper**: A subset of features is selected based on their performance in a model. This usually involves training a model multiple times with different subsets of features. One method is to perturb individual feature values (such as randomly shuffling them) and evaluating the effect on the model's performance. Another method involves omitting features one by one, or in groups. The procedure can be performed bottom-up (Sequential Forward Selection) or top-down (Recursive Feature Elimination), or using non-greedy techniques such as simulated annealing.
  - When deciding on the best approach, there are certain tradeoffs to consider. For example, Filter methods are straightforward and inexpensive to compute, but they assume that statistical significance equates to improved predictive power. On the other hand, Wrapper methods are computationally intensive, but more directly assess the model's performance. To gain a better understanding of the relative importance of features, ML practitioners often employ a variety of approaches. 
  - Approaches can also be combined, such as Filter and Wrapper. If several features are highly correlated, dropping an individual feature may not impact model performance even if it has a significant contribution to model performance, so highly correlated features should be dropped together.

## 4. How would you deal with missing feature values?

- **Answer**:
  - before imputing, or estimating, inspect missing feature values
    - is the training data complete? It’s possible that the data became corrupted in some way, and the problem could be resolved by improving the data collection process rather than through imputation.
    - What kind of model are you training? Decision trees and ensembles, such as XGBoost, can handle missing values automatically without need for imputation.
    - Is the data missing at random? Many imputation techniques are only appropriate for scenarios where data are missing at random. Read more about this topic in (Puri et al., 2021). In some cases the data is intentionally missing, and missing values can be simply filled in: for instance, user interaction features (e.g., clicks) often omit values where the count is zero
    - How many values are missing? Visualize the severity of the missing values. For small amounts of missing data, values can be imputed. However, if a significant portion of data is missing at random, it may be better to delete the feature or fix the data collection process.
    - What is the distribution for features with missing values? If the distribution is non-Gaussian, such as bimodal, using mean or median imputation will result in poor performance. Studies have also shown that single imputation techniques may underestimate the variance of the data (Alade et al., 2020)
    - Are there highly correlated features? In the presence of collinearity with other features, missing features may be deleted entirely. Alternatively, correlation between features can make imputation easier if one or more highly correlated features is present, for instance using tree imputation. You can employ statistical tests such as pointwise mutual information or Pearson's R, or PCA to find underlying correlations.
  - Several ways to handle missing feature values
    - **Deletion**: Simply remove the records or instances that contain missing values. This method is useful when there is very little missing data, but it can lead to loss of information if there is a large amount of missing data. Another option is to delete the feature(s) itself.
    - **Mean/Median/Mode**: Replace missing values with the mean, median or mode of the feature. This method is simple and fast but often underestimates variance of the data (Zhang, 2016), produces bad estimates for non-Gaussian distributions (e.g., bimodal distributions), and introduces bias if the data is not missing at random. That said, it is often the de facto method used in ML toolsets, such as BigQuery ML (Automatic Feature Preprocessing | BigQuery, 2023)
    - **Interpolation**: In time-series data, missing feature values can be interpolated using adjacent values. For instance, stock prices are not available during weekends. Linear interpolation averages the values from Friday and Monday. An alternative approach is to use the value on Friday (technique called LOCF) or the value on Monday (called NOCB)
    - **kNN** (aka. “Hot Deck”): Replace missing values with values from the k nearest neighbors. This may be resource intensive as kNN requires storing the entire dataset in memory.
    - **Regression**: Train a regression model to predict the missing values based on the other values of other features in the dataset. This approach can also be combined with kNN, for example, the Euclidean distance can be used as a weight. One downside is that regression analysis may underfit the distribution.
    - **Trees**: Decision trees and ensembles are a method of training non-linear models for imputation. The idea is to use trees to identify features that are highly correlated with the missing value and use those features to predict the missing value. Trees are fit on the available features and the prediction is the imputed value.
    - **Multiple imputation**: Multiple imputation involves generating several complete datasets by randomly filling in missing values from an estimated distribution. Each dataset is then used to train a separate model. During inference, the predictions from each model are pooled (e.g., by taking a weighted average) to produce a final estimate. One popular technique is Multiple Imputation with Chained Equations (MICE) (Errickson, n.d.).