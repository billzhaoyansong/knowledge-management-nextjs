# ML Fundamentals - Modeling

- [ML Fundamentals - Modeling](#ml-fundamentals---modeling)
  - [1. What are some common modeling algorithms? List 3 and compare them](#1-what-are-some-common-modeling-algorithms-list-3-and-compare-them)
  - [2. What’s the loss function? How is it optimized?](#2-whats-the-loss-function-how-is-it-optimized)
  - [3.  Is there a faster way to optimize the algorithm? What are the tradeoffs?](#3--is-there-a-faster-way-to-optimize-the-algorithm-what-are-the-tradeoffs)
  - [4. What are some ways to optimize gradient descent?](#4-what-are-some-ways-to-optimize-gradient-descent)
  - [5. What’s a reasonable procedure for hyperparameter tuning?](#5-whats-a-reasonable-procedure-for-hyperparameter-tuning)
  - [6. What is overfitting? How can you tell if your model is overfitting?](#6-what-is-overfitting-how-can-you-tell-if-your-model-is-overfitting)

## 1. What are some common modeling algorithms? List 3 and compare them

- **Answer**:
  - **Logistic regression**:
    - Popular classification algorithm that models the probability of an event occurring based on one or more input variables (the “features”). In binary logistic regression, the features are combined linearly and then transformed using the logistic function, also called the sigmoid function, to model the probability of the event. In multinomial logistic regression (also called softmax regression), the softmax function is used in place of the logistic function.
    - The weights in the logistic function are estimated from the data by maximizing the likelihood of observing the dependent variable given the inputs and weights. This is equivalent to minimizing the negative log-likelihood, which is expressed as the Log Loss, or Cross-Entropy loss. 
    - Log Loss can be minimized using various optimization algorithms such as gradient descent, stochastic gradient descent, or quasi-Newton methods (e.g., BFGS) (Minka, 2003). During training, the model is repeatedly presented with training examples, and the model weights are updated in the direction of the negative gradient of the loss function with respect to the weights. This process is repeated until the loss converges to a minimum, according to a tolerance threshold.
    - Logistic regression can be regularized by adding a regularization term to the loss function. The most commonly used regularization techniques are L1 and L2, which add a penalty term to the loss function that encourages small weights.
    - Pros
      - Simple and efficient algorithm that can be trained quickly even on large datasets.
      - Outputs probability estimates, which can be useful in certain applications such as risk assessment or fraud detection.
      - Can be easily interpreted, and the weight vector can be used to identify the most important features for the classification task.
    - Cons
      - Assumes a linear relationship between the input variables and the target variable, which may not always hold true in practice.
      - Prone to overfitting if the number of features is much larger than the number of training examples
  - **Deep neural networks (DNN)**
    - Deep neural networks (DNNs) are a class of modeling algorithms that consists of layers of interconnected neurons (also known as “units”).
    - A variety of loss functions are utilized in DNNs (see ML Fundamentals – Evaluation for a list), popular ones being MSE, Cross-Entropy, and Log-Cosh. DNNs are trained using a process called backpropagation, which involves iteratively updating the weights of the neurons to minimize the difference between the predicted and true outputs. Backpropagation works by computing the gradient of the loss function with respect to the weights of the neurons, and using this gradient to update the weights using a variant of gradient descent, such as stochastic gradient descent (SGD).
    - DNNs can be regularized to prevent overfitting, by adding L1 or L2 regularization, by using dropout, or with early stopping. Max-norm and batch normalization are other popular techniques.
    - Pros:
      - Learns complex and non-linear relationships between inputs and outputs, and along with XGBoost, achieves state-of-the-art performance on many tasks.
      - Scales well to high-dimensional and large-scale datasets, making them well-suited for big data applications.
      - Versatility: models come in all shapes and sizes, feature interactions can be modeled explicitly (e.g., with crossing), support for a wide range of features and label types, pre-trained models can be easily transferred to new tasks, and so on.
    - Cons:
      - Computationally expensive to train complex models.
      - Requires large amounts of data to train effectively.
      - Complex models are prone to overfitting on small to medium datasets.
      - Can be difficult to interpret and understand.

## 2. What’s the loss function? How is it optimized?

- **Answer**:
  - Logistic regression is trained using a maximum likelihood estimation approach, which involves finding values of model coefficients (also known as weights) that maximize the likelihood of the observed data given the input variables and weights. The **likelihood function is a product of the probabilities of each observation**.
  - _**Maximizing the likelihood is equivalent to minimizing the negative log-likelihood**_, which is known as the _**Log Loss (or Cross-Entropy)**_ function
  - log Loss in logistic regression can be optimized using gradient-based optimization algorithms such as stochastic gradient descent (SGD) or its variants (e.g. mini-batch gradient descent).
  - In each iteration of the optimization algorithm, the gradient of the loss is calculated based on a single random sample (for SGD) or a small batch of samples (in mini-batch gradient descent).

## 3.  Is there a faster way to optimize the algorithm? What are the tradeoffs?

- **Answer**:
  - Second-order optimization methods are optimization algorithms that use information from the second derivative (also known as the Hessian matrix) of the objective function to optimize it. These methods are more efficient than first-order optimization methods (such as gradient descent), as they have a **better estimate of the curvature of the objective function**, leading to faster convergence and better solutions. Some examples of second-order optimization methods are Newton's method, BFGS and L-BFGS (Tan & Lim, 2019)
  - Downsides:
    - Computational cost: Second-order methods require computation of the Hessian matrix, which can be expensive and time-consuming for large models, though quasi-Newton methods aim to reduce the computational burden.
    - Memory requirements: Storing the Hessian matrix can be memory-intensive for high-dimensional datasets, although there are approaches to approximate the matrix (e.g., L-BGFS).
    - Saddle points: Some second-order methods such as Newton's method are attracted to saddle points, though this can be mitigated with regularization.

## 4. What are some ways to optimize gradient descent?

- **Answer**:
  - **SGD** suffers from high variance in gradience estimation, due to the small sample size, which may lead to overshooting.
  - **Mini-batch gradient descent** reduces the variance of the gradient estimation by computing weight update on a small batch of samples (usually between 32 and 256), resulting in smoother convergence. Drawbacks for SGD and Mini-batch:
    - Learning rates that are high lead to large fluctuations in weight updates or even to divergence.
    - Learning rates that are low result in slow convergence.
    - Learning rates that are fixed or adjusted on a schedule (e.g., annealing) fail to adapt to the model, dataset, or contour of the loss function.
    - Learning rates applied uniformly to all weights don’t account for feature sparsity.
  - **Momentum**: Adds a fraction of the weight update from the previous time step to the current weight update. Can be viewed as adapting to the slope of the loss function. Encourages faster convergence but may also wander in the wrong direction.
  - **Nesterov Momentum**: Version of Momentum that computes the gradient after applying the momentum update to the weights. It can be seen as a correction to the momentum, to increase the accuracy of weight updates without sacrificing the speed
  - **AdaGrad**: Adapts weight updates according to feature sparsity. Features that activate frequently perform smaller weight updates than features that are infrequent. This is achieved by accumulating squared gradients per feature in the denominator of the learning rate, so learning rate decreases the more often features activate. The downside is that the learning rate may be reduced too much before the algorithm converges.
  - **AdaDelta (and RMSProp)**: Addresses the diminishing learning rates of AdaGrad by allowing the learning rates to speed up again. This is achieved by keeping an exponentially decaying moving average of squared gradients, rather than the entire history of the squared gradient. More memory efficient than AdaGrad. AdaDelta and RMSProp are commonly used by ML practitioners in the industry settings.
  - **Adam**: Can be seen as adding momentum to Adadelta/RMSProp. In addition to accumulating an exponentially decaying average of squared gradients in the denominator, Adam also stores an exponentially decaying average of gradients in the numerator, which acts as the momentum term.

## 5. What’s a reasonable procedure for hyperparameter tuning?

- **Answer**:
  - Methods:
    - In **grid search**, a predefined set of hyperparameters are evaluated exhaustively,
    while in random search, hyperparameters are selected randomly from a predefined search space. These approaches can be inefficient, especially for
    models with a large number of hyperparameters or ones that take a long time to train.
    - **Bayesian optimization** leverages a probabilistic model to approximate the relationship between hyperparameters and the model performance, using an   acquisition function that guides the search by balancing exploration (sampling
    new hyperparameters) and exploitation (evaluating promising hyperparameters).
    - In a genetic algorithm, models represent combinations of hyperparameters (“genes”). The process involves selecting the fittest models from each generation in an iterative manner. The surviving models undergo modification (“mutation”) or recombination (“crossover”) of their hyperparameters.
    - Bayesian optimization and genetic algorithms can be effective in finding hyperparameter values for complex models and large datasets
  - For **logistic regression**, commonly tuned hyperparameters include
    - Optimization algorithm (aka. “solver”)
    - Learning rate
    - Regularization (aka. “penalty”): L1 and L2
  - some **additional hyperparameters for DNNs**:
    - Number of hidden layers (aka. “depth”)
    - Neurons (units) per layer
    - Embedding size e.g., for text and categorical input features
    - Activation functions
    - Batch size
    - Dropout

## 6. What is overfitting? How can you tell if your model is overfitting?

