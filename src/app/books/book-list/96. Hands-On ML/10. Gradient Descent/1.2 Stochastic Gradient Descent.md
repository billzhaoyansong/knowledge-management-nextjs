# 11.3 Stochastic Gradient Descent

- philosophy:
  - stochastic gradient descent picks a random instance in the training set at every step and computes the gradients
  - it iterates by rounds of $m$ iterations; each round is called an _epoch_
  - When using stochastic GD, the training instances must be __independent and identically distributed (IID)__
    - solution: to shuffle the instances during training (e.g., pick each instance randomly, or shuffle the training set at the beginning of each epoch)
- due to its stochastic (i.e., random) nature, this algorithm is much less regular than batch gradient descentï¼šcost function will bounce up and down, decreasing only on average
  - good part: when the cost function is very irregular, this can actually help the algorithm jump out of local minima
  - bad part: algorithm can never settle at the minimum
    - One solution to this dilemma is to gradually reduce the learning rate (akin to simulated annealing)
    - the function that determines the learning rate at each iteration is called the __learning schedule__

```python
n_epochs = 50
t0, t1 = 5, 50  # learning schedule hyperparameters

def learning_schedule(t):
    return t0 / (t + t1)

np.random.seed(42)
theta = np.random.randn(2, 1)  # random initialization

for epoch in range(n_epochs):
    for iteration in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index : random_index + 1]
        yi = y[random_index : random_index + 1]
        gradients = 2 * xi.T @ (xi @ theta - yi)  # for SGD, do not divide by m
        eta = learning_schedule(epoch * m + iteration)
        theta = theta - eta * gradients
```

- linear regression with stochastic GD (defaults to optimizing the MSE cost function)

    ```python
    from sklearn.linear_model import SGDRegressor

    sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01,
                        n_iter_no_change=100, random_state=42)
    sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets

    print(sgd_reg.intercept_, sgd_reg.coef_)
    ```

---
