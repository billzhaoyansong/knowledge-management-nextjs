# 12.2 Learning Curves to Decide Regression Degree

- learning curves are used to **check the complexity of your model**: either the model is too simple or too complex
- learning curves are generated by evaluating the model (Root Mean Square Error,RMSE) at regular intervals during training on both the training set and the validation set

<div class="table-responsive">
<table class="table" width="100%">
<caption>Table: Learning Curves of Linear and 10th-degree Polynomial Regression Models for a Quadratic Dataset</caption>
<thead>
<tr><th></th><th>Linear</th><th>10th-Degree Polynomial</th></tr>
</thead>
<tbody>
<tr>
<th>code</th>
<td>

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, valid_scores = learning_curve(
    LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,
    scoring="neg_root_mean_squared_error")
train_errors = -train_scores.mean(axis=1)
valid_errors = -valid_scores.mean(axis=1)

plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")
[...]  # beautify the figure: add labels, axis, grid, and legend
plt.show()
```

</td>
<td>

```python
from sklearn.pipeline import make_pipeline

polynomial_regression = make_pipeline(
    PolynomialFeatures(degree=10, include_bias=False),
    LinearRegression())

train_sizes, train_scores, valid_scores = learning_curve(
    polynomial_regression, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5,
    scoring="neg_root_mean_squared_error")
[...]  # same as earlier
```

</td>
</tr>

<tr>
<th>
diagram
</th>
<td>
<img width=50% src="books\Hands-On ML\lc-4-linear.png" />
</td>
<td>
<img width=50% src="books\Hands-On ML\lc-4-10th-poly.png" />
</td>
</tr>

<tr>
<th>explanation</th>
<td>

- This model is underfitting
  - Both curves have reached a plateau; they are close and fairly high
- first let’s look at the training error
  - When there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts at zero.
  - as new instances are added to the training set, it becomes impossible for the model to fit the training data perfectly
- look at the validation error
  - When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite large.
  - as the model is shown more training examples, it learns, and thus the validation error slowly goes down.  
- need to use a better model or come up with better features.

</td>

<td>

- This model is overfitting
  - The error on the training data is much lower than before.
  - There is a gap between the curves. This means that the model performs significantly better on the training data than on the validation data, which is the _hallmark_ of an overfitting model.
- ways to improve an overfitting model:
  - feed it more training data until the validation error reaches the training error
  - **regularization**

</td>
</tr>
</tbody>
</table>
</div>

- Bias/Variance Tradeoff
  - a model’s generalization error = Bias + Variance + Irreducible Error
    - Bias:  wrong assumptions, such as assuming that the data is linear when it is actually quadratic
    - Variance: excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance and thus overfit the training data.
    - Irreducible Error: noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers)
  - Increasing a model’s complexity will typically increase its variance and reduce its bias.
  - Conversely, reducing a model’s complexity increases its bias and reduces its variance. This is why it is called a trade-off.
