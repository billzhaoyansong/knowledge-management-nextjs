# 6.2 Model Evaluation

- a great way to evaluate ML model is to use _k-fold cross-validation_, e.g.
  1. randomly splits the training set into 10 nonoverlapping subsets called folds,
  2. it trains and evaluates the decision tree model 10 times
  3. picking a different fold for evaluation every time and using the other 9 folds for training.
- note: _Scikit-Learnâ€™s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the RMSE_

    ```python
    from sklearn.model_selection import cross_val_score

    # cut the whole training set into 10 folds
    # 9 folds are used as the new training set
    # the residual 1 fold is used as the validation set
    tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)

    # peek the errors on the validation set
    pd.Series(tree_rmses).describe()
    ```

- one thing can be done here is to compare __the RMSE of validation set__ computed just now, and __the RMSE of the whole training set__ computed in the Model Training section
  - a much lower error in the RMSE of the whole training set means (1) training set overfitting; (2) data mismatch

