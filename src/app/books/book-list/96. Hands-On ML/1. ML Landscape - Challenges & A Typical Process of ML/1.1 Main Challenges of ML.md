# 2.1 Main Challenges of ML

## Insufficient Quantity of Training Data

- In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric Brill showed that very different machine learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation⁠6 once they were given enough data
- It should be noted, however, that small and medium-sized datasets are still very common, and it is not always easy or cheap to get extra training data⁠—so don’t abandon algorithms just yet.

## Nonrepresentative Training Data

- if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.

## Poor-Quality Data

- If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.
- If some instances are missing a few features (e.g., 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute altogether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it.

## Irrelevant Features

- Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones
- steps of feature engineering
  1. Feature selection (selecting the most useful features to train on among existing features)
  2. Feature extraction (combining existing features to produce a more useful one⁠—as we saw earlier, dimensionality reduction algorithms can help)
  3. Creating new features by gathering new data

## Overfitting the Training Data

- Overfitting happens when the model is too complex relative to the amount and noisiness of the training data.
- possible solutions:
  - simplify the model
    - by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model),
    - by reducing the number of attributes in the training data,
    - or by __regularization__: constraining the model to make it simpler
      - e.g. : linear model we defined earlier has two parameters, $\theta_0$ and $\theta_1$. This gives the learning algorithm two degrees of freedom to adapt the model to the training data: it can tweak both the height ($\theta_0$) and the slope ($\theta_1$) of the line. If we forced $\theta_1 = 0$, the algorithm would have only one degree of freedom and would have a much harder time fitting the data properly: all it could do is move the line up or down to get as close as possible to the training instances, so it would end up around the mean.
  - gather more training data.
  - Reduce the noise in the training data (e.g., fix data errors and remove outliers).

## Underfitting the Training Data

- options for fixing the problem
  - Select a more powerful model, with more parameters.
  - Feed better features to the learning algorithm (feature engineering).
  - Reduce the constraints on the model (for example by reducing the regularization hyperparameter).
