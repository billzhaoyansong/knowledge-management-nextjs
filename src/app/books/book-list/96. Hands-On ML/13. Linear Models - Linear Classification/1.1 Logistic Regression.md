# 14.1 Logistic Regression

## Model

- $\hat{p}=h_{\theta}(\mathbf{x})=\sigma(\bm{\theta}^T\mathbf{x})$
- $\sigma(t)=\frac{1}{1+\exp(-t)}$: sigmoid function
- $\hat{y}=\begin{cases}
0 & \text{if } \hat{p}<0.5 \text{, or } t<0 \\
1 & \text{if } \hat{p}\geq 0.5\text{, or } t\geq0
\end{cases}$

## Cost Function

- the cost function for a single instance
  - $c(\bm{\theta})=\begin{cases}
    -\log(\hat{p}) & \text{if } y=1 \\
    -\log(1-\hat{p}) & \text{if } y=0
  \end{cases}$
    - the logic behind:
      - for a positive instance ($y=1$), the cost will be very large if the model estimates a probability close to 0, while the cost will be close to 0 if the model estimates a probability close to 1
      - for a negative instance ($y=0$), the cost will be very large if the model estimates a probability close to 1, while the cost will be close to 0 if the model estimates a probability close to 0
- the cost function over the whole training set
  - $J(\Theta)=-\frac{1}{m}\sum_{i=1}^m\left[y^{(i)}\log(\hat{p}^{(i)})+(1-y^{(i)})\log(1-\hat{p}^{(i)})\right]$
    - no known closed-form equation to compute the value of $\Theta$
    - but cost function is convex, so gradient descent methods can be applied

## The Code

```python
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

iris = load_iris(as_frame=True)
list(iris)

X = iris.data[["petal width (cm)"]].values
y = iris.target_names[iris.target] == 'virginica'
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

X_new = np.linspace(0, 3, 1000).reshape(-1, 1)  # reshape to get a column vector
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]

plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2,
         label="Not Iris virginica proba")
plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica proba")
plt.plot([decision_boundary, decision_boundary], [0, 1], "k:", linewidth=2,
         label="Decision boundary")
[...] # beautify the figure: add grid, labels, axis, legend, arrows, and samples
plt.show()
```

<img src="books\Hands-On ML\logistic-decision-boundary.png" /> 
