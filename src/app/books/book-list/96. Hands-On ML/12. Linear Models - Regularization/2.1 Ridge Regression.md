# 13.2 Ridge Regression (Tikhonov regularization)

- the cost function for Ridge regression
  - $J(\bm{\theta})=\text{MSE}(\bm{\theta})+\frac{\alpha}{m}\sum_{i=1}^n\theta^2_i$
    - the bias term $θ_0$ is not regularized (the sum starts at $i = 1$, not $0$)
    - if we define $\mathbf{w}=(\theta_1,...,\theta_n)$, then the regularization term $\sum_{i=1}^n\theta^2_i=\Vert\mathbf{w}\Vert_2^2$
    - forces the learning algorithm to not only fit the data but also **keep the model weights as small as possible**.
    - the cost function should only be used during training. the unregularized MSE should be used to evaluate performance after training
    - $\alpha$ controls how much to regularize the model
      - if $\alpha=0$, there will be no regularization effect.
      - if $\alpha$ is very large, all model weights will be close to $0$, generating a line crossing the data's mean
      - effect of $\alpha$ on linear model and polynomial model

    <img src="books\Hands-On ML\ridge-regression-comparison.png" />

      - on the left, _linear regression_ with ridge regularization
      - on the right, _polynomial regression_ with ridge regularization

- Close-form solution
  - $\hat{\bm{\theta}}=\left(\mathbf{X}^T\mathbf{X}+\alpha \mathbf{A}\right)^{-1}\mathbf{X}^T \mathbf{y}$

    ```python
    from sklearn.linear_model import Ridge
    ridge_reg = Ridge(alpha=0.1, solver="cholesky")
    ridge_reg.fit(X, y)
    ridge_reg.predict([[1.5]]) # array([[1.55325833]])
    ```

- SGD

    ```python
    sgd_reg = SGDRegressor(penalty="l2", alpha=0.1 / m, tol=None,
                           max_iter=1000, eta0=0.01, random_state=42)
    sgd_reg.fit(X, y.ravel())  # y.ravel() because fit() expects 1D targets
     sgd_reg.predict([[1.5]]) # array([1.55302613])
    ```

  - Specifying `"l2"` indicates that you want SGD to add a regularization term to the MSE cost function equal to alpha times the square of the $l_2$ norm of the weight vector.
  - This is just like ridge regression, except there’s no division by m in this case; that’s why we passed `alpha=0.1/m`, to get the same result as `Ridge(alpha=0.1)`.
