# 5.1 Feature Scaling and Transformation

## (1) Feature scaling for large-scale attributes

- why feature scaling
  - usually, ML algorithms _don’t_ perform well when the input numerical attributes have very different scales
  - e.g. the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms
- Solutions:
  - __Min-max scaling (normalization)__: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1, or -1 to 1
    - but min-max scaling can be easily affected by outliers

        ```python
        from sklearn.preprocessing import MinMaxScaler

        min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
        housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)
        ```

  - __Standardization $\frac{x-\mu}{\sigma}$__: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1).
    - standardization does not restrict values to a specific range
    - standardization is much less affected by outliers than min-max scaling

        ```python
        from sklearn.preprocessing import StandardScaler

        std_scaler = StandardScaler()
        housing_num_std_scaled = std_scaler.fit_transform(housing_num)
        ```

## (2) Transformation for distribution with long tails

- When a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), before you scale the feature, you should first transform it to shrink the heavy tail
- solutions:
  - __replacing the feature with its logarithm__, or

    ```python
    from sklearn.preprocessing import FunctionTransformer

    log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
    log_pop = log_transformer.transform(housing[["population"]])
    ```

  - __Bucketizing__: chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to
    - e.g. replace each value with its percentile
    - optional: divide by the number of buckets to force the values to the 0–1 range.

## (3) Transformation for multimodal distributions

- multimodal distribution: with two or more clear peaks, called _modes_
- solutions:
  - __bucketizing__
    - identify modes, and decide on number of buckets (e.g. 3 modes, 3-5 buckets)
    - encode each bucket as a category with `OneHotEncoder`
  - __Gaussian RBF (radial basis function)__: add a feature for each of the modes
    - _there is no inverse function for RBF kernel_

    ```python
    from sklearn.metrics.pairwise import rbf_kernel
    from sklearn.preprocessing import FunctionTransformer

    # 1d rbf transformer
    rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[35.]], gamma=0.1))
    age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])

    # 2d rbf transformer
    # sf: San Fransisco
    sf_coords = 37.7749, -122.41
    sf_transformer = FunctionTransformer(rbf_kernel,
                                        kw_args=dict(Y=[sf_coords], gamma=0.1))
    sf_simil = sf_transformer.transform(housing[["latitude", "longitude"]])
    ```

    <img width="250px" src="/books/Hands-On%20ML/rbf_eg.png"/>

## (4) Inverse transformation for target values

- not only the input features,  target values may also need to be transformed
- for target values, the prediction results must be inversely transformed before presenting to the user
  - e.g. if you do a logrithm transformation on target values, you must compute the exponential value of the prediction results to inverse back to the real value

    ```python
    from sklearn.compose import TransformedTargetRegressor

    # the transformer is defined for the target value
    model = TransformedTargetRegressor(LinearRegression(),
                                    transformer=StandardScaler())
    model.fit(housing[["median_income"]], housing_labels)

    # inverse transformation is done automatically
    predictions = model.predict(some_new_data)
    ```

## (5) Customized transformer without fitting

- e.g. FunctionTransformer to combine features

```python
from sklearn.preprocessing import FunctionTransformer

ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
```

## (6) Customized transformer with fitting

- `fit` is used to learn parameters from attributes
  - e.g. `MinMaxScaler` and `StandardScaler` learn parameters (min/max values, mean/std values)

- examples

  - an example to reproduce `StandardScaler`

    ```python
    from sklearn.base import BaseEstimator, TransformerMixin
    from sklearn.utils.validation import check_array, check_is_fitted

    class StandardScalerClone(BaseEstimator, TransformerMixin):
        def __init__(self, with_mean=True):  # no *args or **kwargs!
            self.with_mean = with_mean

        def fit(self, X, y=None):  # y is required even though we don't use it
            X = check_array(X)  # checks that X is an array with finite float values
            self.mean_ = X.mean(axis=0)
            self.scale_ = X.std(axis=0)
            self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()
            return self  # always return self!

        def transform(self, X):
            check_is_fitted(self)  # looks for learned attributes (with trailing _)
            X = check_array(X)
            assert self.n_features_in_ == X.shape[1]
            if self.with_mean:
                X = X - self.mean_
            return X / self.scale_
    ```

  - another example to compute the distance between the sample and the centers identified by `KMeans`

    ```python
    from sklearn.cluster import KMeans

    class ClusterSimilarity(BaseEstimator, TransformerMixin):
        def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
            self.n_clusters = n_clusters
            self.gamma = gamma
            self.random_state = random_state

        def fit(self, X, y=None, sample_weight=None):
            self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)
            self.kmeans_.fit(X, sample_weight=sample_weight)
            return self  # always return self!

        def transform(self, X):
            return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)

        def get_feature_names_out(self, names=None):
            return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
    ```
